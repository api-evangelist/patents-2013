---

title: Handling NAT in logical L3 routing
abstract: A non-transitory machine readable medium storing a program that configures first and second managed forwarding elements to perform logical L2 switching and L3 routing is described. The program generates a first set of flow entries for configuring the first managed forwarding element to perform (1) a first logical L2 processing for a first logical L2 domain, (2) a logical L3 processing, (3) a network address translation (NAT) processing on packets to be sent to the second managed forwarding element, and (4) a logical ingress L2 processing for a second logical L2 domain on the packets. The program generates a second set of flow entries for configuring the second managed forwarding element to perform a logical egress L2 processing for the second logical L2 domain on the packets.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09350696&OS=09350696&RS=09350696
owner: NICIRA, INC.
number: 09350696
owner_city: Palo Alto
owner_country: US
publication_date: 20130201
---
This application is a continuation application of U.S. patent application Ser. No. 13 589 062 filed on Aug. 17 2012 now published as U.S. Patent Publication 2013 0044636. U.S. patent application Ser. No. 13 589 062 claims the benefit of U.S. Provisional Patent Application 61 524 754 filed Aug. 17 2011 U.S. Provisional Patent Application 61 643 339 filed May 6 2012 U.S. Provisional Patent Application 61 654 121 filed Jun. 1 2012 and U.S. Provisional Patent Application 61 666 876 filed Jul. 1 2012. This application claims the benefit of U.S. Provisional Patent Application 61 643 339 filed May 6 2012 U.S. Provisional Patent Application 61 654 121 filed Jun. 1 2012 and U.S. Provisional Patent Application 61 666 876 filed Jul. 1 2012. U.S. patent application Ser. No. 13 589 062 now published as U.S. Patent Publication 2013 0044636 and U.S. Provisional Patent Applications 61 524 754 61 643 339 61 654 121 and 61 666 876 are incorporated herein by reference.

Many current enterprises have large and sophisticated networks comprising switches hubs routers servers workstations and other networked devices which support a variety of connections applications and systems. The increased sophistication of computer networking including virtual machine migration dynamic workloads multi tenancy and customer specific quality of service and security configurations require a better paradigm for network control. Networks have traditionally been managed through low level configuration of individual components. Network configurations often depend on the underlying network for example blocking a user s access with an access control list ACL entry requires knowing the user s current IP address. More complicated tasks require more extensive network knowledge forcing guest users port 80 traffic to traverse an HTTP proxy requires knowing the current network topology and the location of each guest. This process is of increased difficulty where the network switching elements are shared across multiple users.

In response there is a growing movement towards a new network control paradigm called Software Defined Networking SDN . In the SDN paradigm a network controller running on one or more servers in a network controls maintains and implements control logic that governs the forwarding behavior of shared network switching elements on a per user basis. Making network management decisions often requires knowledge of the network state. To facilitate management decision making the network controller creates and maintains a view of the network state and provides an application programming interface upon which management applications may access a view of the network state.

Some of the primary goals of maintaining large networks including both datacenters and enterprise networks are scalability mobility and multi tenancy. Many approaches taken to address one of these goals results in hampering at least one of the others. For instance one can easily provide network mobility for virtual machines within an L2 domain but L2 domains cannot scale to large sizes. Furthermore retaining user isolation greatly complicates mobility. As such improved solutions that can satisfy the scalability mobility and multi tenancy goals are needed.

Some embodiments in some cases model logical routing as an act of interconnecting two or more logical datapath LDP sets operating in L2 domains by a logical router that implements a logical datapath set LDPS operating in an L3 domain. A packet traversing from a logical L2 domain to another will take the following four steps in some embodiments. These four steps are described below in terms of the logical processing operations that the network control system implements. However it is to be understood that these operations are performed by the managed switching elements of the network based on the physical control plane data that is produced by the network control system.

First the packet will be processed through an L2 table pipeline of the originating logical L2 domain. The pipeline will conclude with the destination media access control MAC address being forwarded to a logical port attached to a logical port of a logical router.

Second the packet will be processed though a logical router s L3 datapath again by sending it through this router s L3 table pipeline. The L2 lookup stage common in physical routers is skipped in the router s L3 datapath in some embodiments as the logical router will only receive packets requiring routing.

In some embodiments the L3 forwarding decision will use the prefix forwarding information base FIB entries that are provisioned by the logical control plane of the logical router. In some embodiments a control application is used to receive the logical control plane data and to convert this data to logical forwarding plane data that is then supplied to the network control system. For the L3 forwarding decision some embodiments use the prefix FIB entries to implement longest prefix matching.

As a result the L3 router will forward the packet to the logical port that is connected to the destination L2 LDPS. Before forwarding the packet further to that LDPS the L3 router will change the originating MAC address to one that is defined in its domain as well as resolve the destination IP address to a destination MAC address. The resolution is executed by the last IP output stage of the L3 data pipeline in some embodiments. The same pipeline will decrement TTL and update the checksum and respond with ICMP if TTL goes to zero .

It should be noted that some embodiments rewrite the MAC address before feeding the processed packet to the next LDPS because without this rewriting a different forwarding decision could result at the next LDPS. It should also be noted that even though traditional routers execute the resolution of the destination IP address using Address Resolution Protocol ARP some embodiments do not employ ARP for this purpose in the L3 logical router because as long as the next hop is a logical L2 datapath this resolution remains internal to the virtualization application.

Third the packet will be processed through an L2 table pipeline of the destination logical L2 domain. The destination L2 table pipeline determines the logical egress port along which it should send the packet. In case of an unknown MAC address this pipeline would resolve the MAC address location by relying on some distributed lookup mechanism. In some embodiments the managed switching elements rely on a MAC learning algorithm e.g. they flood the unknown packets. In these or other embodiments the MAC address location information can also be obtained by other mechanisms for instance out of band. If such a mechanism is available in some embodiments the last logical L2 table pipeline uses this mechanism to obtain the MAC address location.

Fourth the packet gets sent to the logical port attached to the physical port representing the logical port attachment. At this stage if the port is point to point media e.g. virtual network interface VIF there s nothing left to do but to send the packet to the port. However if the last LDPS was an L3 router and hence the attachment is a physical L3 subnet the attachment point in some embodiments resolves the destination IP address by using ARP before sending the packet out. In that case the source MAC address would be egress specific and not the logical MAC interface address in case of a VIF. In other embodiments resolving the destination IP address by using ARP is performed during the second step by the L3 logical router.

In the example above there s only a single logical router interconnecting logical L2 datapaths but nothing limits the topologies. One of ordinary skill in the art will recognize that more LDP sets can be interconnected for richer topologies.

In some embodiments the control application allows an L3 specific logical state to be defined in terms of one or more tables that specify a logical L3 pipeline. The corresponding logical control plane managing the LDPS pipeline can either rely on static route configuration or peer with other LDP sets over a standard routing protocol.

In some embodiments the virtualization application defines the physical realization of the above described four step L2 L3 packet processing into physical control plane data which when translated into physical forwarding data by the managed switching elements effectuates a sequence of logical pipeline executions that are all or predominantly performed at the first hop managed edge switching element. In order to maintain the locality of the physical traffic the first hop executes the series of pipelines with all state required and directly sends the traffic towards the ultimate egress location in the physical network. When short cut tunnels are used the virtualization application interconnects logical L2 datapaths with logical L3 datapaths by extending the short cut tunnel mesh beyond a single LDPS to a union of ports of all the interconnected LDP sets. When everything is executed at the first hop the first hop elements typically have access to all the states of the logical network through which the packet traverses.

The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly to understand all the embodiments described by this document a full review of the Summary Detailed Description and the Drawings is needed. Moreover the claimed subject matters are not to be limited by the illustrative details in the Summary Detailed Description and the Drawing but rather are to be defined by the appended claims because the claimed subject matters can be embodied in other specific forms without departing from the spirit of the subject matters.

Some embodiments of the invention provide a network control system that allows logical datapath LDP sets e.g. logical networks to be implemented by switching elements of a physical network. To implement LDP sets the network control system of some embodiments generates physical control plane data from logical forwarding plane data. The physical control plane data is then pushed to the managed switching elements where it is typically converted into physical forwarding plane data that allows the managed switching elements to perform their forwarding decisions. Based on the physical forwarding data the managed switching elements can process data packets in accordance with the logical processing rules specified within the physical control plane data.

A single logical datapath set provides switching fabric to interconnect a number of logical ports which can be either attached to physical or virtual endpoints. In some embodiments the creation and use of such LDP sets and logical ports provides a logical service model that corresponds to a virtual local area network VLAN . This model in some embodiments limits the operations of the network control system to defining only logical L2 switching capabilities. However other embodiments extend the operations of the network control system to both the logical L2 switching capabilities and the logical L3 switching capabilities.

The network control system of some embodiments supports the following logical L3 switching capabilities.

The design for each of these L3 features will be described below. Implementation wise the features are largely orthogonal so one of ordinary skill will realize that these features do not all have to be offered by a network control system of some embodiments. Before describing the features further several assumptions should be mentioned. These assumptions are as follows.

Some of the embodiments described below are implemented in a novel distributed network control system that is formed by one or more controllers also called controller instances below for managing one or more shared forwarding elements. The shared forwarding elements in some embodiments can include virtual or physical network switches software switches e.g. Open vSwitch routers and or other switching devices as well as any other network elements such as load balancers etc. that establish connections between these switches routers and or other switching devices. Such forwarding elements e.g. physical switches or routers are also referred to below as switching elements. In contrast to an off the shelf switch a software forwarding element is a switch that in some embodiments is formed by storing its switching table s and logic in the memory of a standalone device e.g. a standalone computer while in other embodiments it is a switch that is formed by storing its switching table s and logic in the memory of a device e.g. a computer that also executes a hypervisor and one or more virtual machines on top of that hypervisor.

In some embodiments the controller instances allow the system to accept logical datapath sets from users and to configure the switching elements to implement these logical datapath sets. In some embodiments one type of controller instance is a device e.g. a general purpose computer that executes one or more modules that transform the user input from a logical control plane to a logical forwarding plane and then transform the logical forwarding plane data to physical control plane data. These modules in some embodiments include a control module and a virtualization module. A control module allows a user to specify and populate logical datapath set while a virtualization module implements the specified logical datapath set by mapping the logical datapath set onto the physical switching infrastructure. In some embodiments the control and virtualization applications are two separate applications while in other embodiments they are part of the same application.

From the logical forwarding plane data for a particular logical datapath set the virtualization module of some embodiments generates universal physical control plane UPCP data that is generic for any managed switching element that implements the logical datapath set. In some embodiments this virtualization module is part of a controller instance that is a master controller for the particular logical datapath set. This controller is referred to as the logical controller.

In some embodiments the UPCP data is then converted to customized physical control plane CPCP data for each particular managed switching element by a controller instance that is a master physical controller instance for the particular managed switching element or by a chassis controller for the particular managed switching element as further described in U.S. patent application Ser. No. 13 589 077 filed Aug. 17 2012 now issued as U.S. Pat. No. 9 178 833 which is incorporated herein by reference. When the chassis controller generates the CPCP data the chassis controller obtains the UPCP data from the virtualization module of the logical controller through the physical controller.

Irrespective of whether the physical controller or chassis controller generate the CPCP data the CPCP data for a particular managed switching element needs to be propagated to the managed switching element. In some embodiments the CPCP data is propagated through a network information base NIB data structure which in some embodiments is an object oriented data structure. Several examples of using the NIB data structure are described in U.S. patent application Ser. No. 13 177 529 now issued as U.S. Pat. No. 8 743 889 and 13 177 533 now issued as U.S. Pat. No. 8 817 620 which are incorporated herein by reference. As described in these applications the NIB data structure is also used in some embodiments to may serve as a communication medium between different controller instances and to store data regarding the logical datapath sets e.g. logical switching elements and or the managed switching elements that implement these logical datapath sets.

However other embodiments do not use the NIB data structure to propagate CPCP data from the physical controllers or chassis controllers to the managed switching elements to communicate between controller instances and to store data regarding the logical datapath sets and or managed switching elements. For instance in some embodiments the physical controllers and or chassis controllers communicate with the managed switching elements through OpenFlow entries and updates over the configuration protocol. Also in some embodiments the controller instances use one or more direct communication channels e.g. RPC calls to exchange data. In addition in some embodiments the controller instances e.g. the control and virtualization modules of these instances express the logical and or physical data in terms of records that are written into the relational database data structure. In some embodiments this relational database data structure are part of the input and output tables of a table mapping engine called nLog that is used to implement one or more modules of the controller instances.

Some embodiments in some cases model logical routing as an act of interconnecting two or more LDP sets operating in L2 domains by a logical router that implements a LDPS operating in an L3 domain. A packet traversing from a logical L2 domain to another will take the following four steps in some embodiments. These four steps are described below in terms of the logical processing operations that the network control system implements. However it is to be understood that these operations are performed by the managed switching elements of the network based on the physical control plane data that is produced by the network control system.

First the packet will be processed through an L2 table pipeline of the originating logical L2 domain. The pipeline will conclude with the destination media access control MAC address being forwarded to a logical port attached to a logical port of a logical router.

Second the packet will be processed though a logical router s L3 datapath again by sending it through this router s L3 table pipeline. The L2 lookup stage common in physical routers is skipped in the router s L3 datapath in some embodiments as the logical router will only receive packets requiring routing.

In some embodiments the L3 forwarding decision will use the prefix forwarding information base FIB entries that are provisioned by the logical control plane of the logical router. In some embodiments a control application is used to receive the logical control plane data and to convert this data to logical forwarding plane data that is then supplied to the network control system. For the L3 forwarding decision some embodiments use the prefix FIB entries to implement longest prefix matching.

As a result the L3 router will forward the packet to the logical port that is connected to the destination L2 LDPS. Before forwarding the packet further to that LDPS the L3 router will change the originating MAC address to one that is defined in its domain as well as resolve the destination IP address to a destination MAC address. The resolution is executed by the last IP output stage of the L3 data pipeline in some embodiments. The same pipeline will decrement TTL and update the checksum and respond with ICMP if TTL goes to zero .

It should be noted that some embodiments rewrite the MAC address before feeding the processed packet to the next LDPS because without this rewriting a different forwarding decision could result at the next LDPS. It should also be noted that even though traditional routers execute the resolution of the destination IP address using ARP some embodiments do not employ ARP for this purpose in the L3 logical router because as long as the next hop is a logical L2 datapath this resolution remains internal to the virtualization application.

Third the packet will be processed through an L2 table pipeline of the destination logical L2 domain. The destination L2 table pipeline determines the logical egress port along which it should send the packet. In case of an unknown MAC address this pipeline would resolve the MAC address location by relying on some distributed lookup mechanism. In some embodiments the managed switching elements rely on a MAC learning algorithm e.g. they flood the unknown packets. In these or other embodiments the MAC address location information can also be obtained by other mechanisms for instance out of band. If such a mechanism is available in some embodiments the last logical L2 table pipeline uses this mechanism to obtain the MAC address location.

Fourth the packet gets sent to the logical port attached to the physical port representing the logical port attachment. At this stage if the port is point to point media e.g. virtual network interface VIF there s nothing left to do but to send the packet to the port. However if the last LDPS was an L3 router and hence the attachment is a physical L3 subnet the attachment point in some embodiments resolves the destination IP address by using ARP before sending the packet out. In that case the source MAC address would be egress specific and not the logical MAC interface address in case of a VIF. In other embodiments resolving the destination IP address by using ARP is performed during the second step by the L3 logical router.

In the example above there s only a single logical router interconnecting logical L2 datapaths but nothing limits the topologies. One of ordinary skill in the art will recognize that more LDP sets can be interconnected for richer topologies.

In some embodiments the control application allows an L3 specific logical state to be defined in terms of one or more tables that specify a logical L3 pipeline. The corresponding logical control plane managing the LDPS pipeline can either rely on static route configuration or peer with other LDP sets over a standard routing protocol.

In some embodiments the virtualization application defines the physical realization of the above described four step L2 L3 packet processing into physical control plane data which when translated into physical forwarding data by the managed switching elements effectuates a sequence of logical pipeline executions that are all or predominantly performed at the first hop managed edge switching element. In order to maintain the locality of the physical traffic the first hop executes the series of pipelines with all state required and directly sends the traffic towards the ultimate egress location in the physical network. When short cut tunnels are used the virtualization application interconnects logical L2 datapaths with logical L3 datapaths by extending the short cut tunnel mesh beyond a single LDPS to a union of ports of all the interconnected LDP sets.

When everything is executed at the first hop the first hop elements typically have access to all the states of the logical network through which the packet traverses. The dissemination and its scaling implications of the state for the execution of the logical pipelines at the first hop switching element is described further below.

The logical switch is a logical switch or a logical switching element described in U.S. patent application Ser. No. 13 177 535. The logical switch is implemented across several managed switching elements not shown . The logical switch routes network traffic between the machines at L2 layer 2 . That is the logical switch makes switching decisions to route network data at the data link layer between the machines based on one or more forwarding tables not shown that the logical switch has. The logical switch along with several other logical switches not shown routes the network traffic for the logical network . The logical switch is another logical switch. The logical switch routes the traffic between machines for the logical network .

A logical router in some embodiments routes traffic at L3 layer 3 network layer between different logical networks. Specifically the logical router routes network traffic between two or more logical switches based on a set of routing tables. In some embodiments a logical router is implemented in a single managed switching element while in other embodiments a logical router is implemented in several different managed switching elements in a distributed manner. A logical router of these different embodiments will be described in detail further below. The logical router routes the network traffic at the L3 between the logical networks and . Specifically the logical router routes the network traffic between the two logical switches and .

The machines are machines that are capable of exchanging data packets. For instance each machine has a network interface controller NIC so that applications that execute on the machine can exchange data between them through the logical switches and and the logical router .

The logical networks and are different in that the machines in each network use different L3 addresses. For instance the logical networks and are different IP subnets for two different departments of a company.

In operation the logical switches and and the logical router function like switches and routers. For instance the logical switch routes data packets originating from one of the machines and heading to another of the machines . When the logical switch in the logical network receives a data packet that is destined for one of the machines in the logical network the logical switch sends the packet to the logical router . The logical router then routes the packet based on the information included in the header of the packet to the logical switch . The logical switch then routes the packet to one of the machines . Data packets originating from one of the machines are routed by the logical switches and and the logical router in a similar manner.

The logical router is similar to the logical router described above by reference to in that the logical router routes data packets between the logical switches and . The logical switches and are similar to the logical switches and . The logical switches and each forward the traffic at L2 for a logical network.

When the logical switch receives a packet the logical switch performs stage L2 processing of the logical processing pipeline in order to forward the packet in one logical network. When the packet is destined for another logical network the logical switch forwards the packet to the logical router . The logical router then performs stage L3 processing of the logical processing pipeline on the packet in order to route the data at L3. The logical router sends this packet to another logical router not shown or if the logical router is coupled to the logical switch the logical router sends the packet to the logical switch that would send the packet directly to the destination machine of the packet. The logical switch which directly sends the packet to the packet s destination performs stage L2 processing of the logical processing pipeline in order to forward the packet to the packet s destination.

In some embodiments logical switches and logical routers are implemented by a set of managed switching elements not shown . These managed switching elements of some embodiments implement the logical switches and logical routers by performing a logical processing pipeline such as the logical processing pipeline . The managed switching elements of some embodiments perform the logical processing pipelines based on flow entries in the managed switching elements. The flow entries not shown in the managed switching elements are configured by the network control system of some embodiments. More details of the logical processing pipeline will be described further below.

The next three figures conceptually illustrates several implementations of logical switches and logical routers of some embodiments. illustrates two different implementations of centralized L3 routing while illustrates a distributed L3 routing.

The L3 router implements the logical router . The L3 router routes packets between different logical networks that include logical switches and . The L3 router routes the packets according to L3 entries that specify the manner in which the packets should be routed at L3. For instance the L3 entries of some embodiments are entries e.g. routes in routing tables that specify that a packet that has a destination IP address that falls in a particular range of IP addresses should be sent out through a particular logical port of the logical router . In some embodiments the logical ports of the logical router are mapped to the ports of the L3 router and the logical router generates the L3 entries based on the mappings. Mapping ports of a logical router to an L3 router that implements the logical router will be described further below.

The managed switching elements of some embodiments implement logical switches in a distributed manner. That is a logical switch in these embodiments may be implemented across one or more of the managed switching elements . For instance the logical switch may be implemented across the managed switching elements and and the logical switch may be implemented across the managed switching elements and . The six VMs logically coupled to the logical switches and are coupled to the managed switching elements as shown.

The managed switching elements of some embodiments each forwards the packets according to L2 flow entries that specify the manner in which the packets should be forwarded at L2. For instance the L2 flow entries may specify that a packet that has a particular destination MAC address should be sent out through a particular logical port of the logical switch. Each of the managed switching elements has a set of L2 flow entries Flow entries for switching elements are not depicted for simplicity . The L2 flow entries for each managed switching elements are configured in the managed switching element by the controller cluster. Configuring managed switching elements by configuring L2 flows entries for the managed switching elements will be described in detail further below.

The managed switching element of some embodiments is a second level managed switching element. A second level managed switching element is a managed non edge switching element which in contrast to a managed edge switching element does not send and receive packets directly to and from the machines. A second level managed switching element facilitates packet exchanges between non edge managed switching elements and edge managed switching elements. A pool node and an extender which are described in U.S. patent application Ser. No. 13 177 535 are also second level managed switching elements. The managed switching element of some embodiments functions as an extender. That is the managed switching element communicatively bridges remote managed networks not shown that are separated by one or more other networks not shown .

The managed switching element of some embodiments is communicatively coupled to the L3 router . When there are packets that need to be routed at L3 the managed switching elements send the packets to the managed switching element so that the L3 router routes the packets at L3. More details about a centralized logical router that is implemented in an L3 router will be described further below by reference to .

The network architecture is similar to the network architecture except that the network architecture does not include the L3 router . The managed switching element implements the logical router . That is the managed switching element routes packets between different logical networks that include logical switches and . The managed switching element of some embodiments routes the packets according to L3 entries that specify the manner in which the packets should be routed at L3. However in contrast to the L3 entries of some embodiments the L3 entries are not entries for routing tables. Rather the L3 entries are flow entries. As described in U.S. patent application Ser. No. 13 177 535 a flow entry includes a qualifier and an action while the entries in routing tables are just lookup tables for finding the next hops for the packets. Also the L3 flow entries may specify the manner in which to generate entries in the routing tables not shown .

In addition to implementing a centralized logical router the managed switching element of some embodiments implements one or more logical switches that are implemented across several managed switching elements. The managed switching element therefore has its own set of L2 flow entries not depicted . In the architecture the managed switching elements and together implement the logical switches and in a distributed manner.

The managed switching element of some embodiments thus implements both a centralized logical router and logical switches. In other embodiments implementation of a centralized logical router and logical switches may be separated into two or more managed switching elements. For instance one managed switching element not shown may implement a centralized logical router using flow entries and another managed switching element not shown may implement logical switches based on flow entries in a distributed manner. More details about a centralized logical router that is implemented in a managed switching element based on flow entries will be described further below by reference to .

The managed switching elements implement a logical router and several logical switches for several different logical networks. Each of the managed switching elements of some embodiments is an edge switching element. That is the managed switching element has one or more machines that are coupled to the managed switching element. The machines that are coupled to the managed switching elements are also logically coupled to the logical switches. The machines that are coupled to a managed switching element may or may not be logically coupled to the same logical switch.

Each of the managed switching elements implements at least one logical router and at least one logical switch that will route and forward packets to and from the machines coupled to the managed switching element. In other words when the managed switching element receives a packet from the machines coupled to the managed switching element the managed switching element makes both logical forwarding decisions and logical routing decisions. Each of the managed switching elements makes the logical forwarding and routing decisions according to the L2 entries and L3 entries in the logical flow entries . The logical flow entries include a set of L2 flow entries and a set of L3 flow entries . More details about a distributed logical router will be described further below by reference to .

The managed switching element is an edge switching element that directly receives the packets from a machine coupled to the edge switching element. The managed switching element receives packets from the source machine . When the managed switching element receives a packet from the source machine the managed switching element performs a portion of the L2 processing on the packet in order to logically forward the packet.

There may be one or more managed switching elements not shown between the managed switching element and the managed switching element . These managed switching elements have network constructs e.g. PIFs VIFs etc. to which the logical constructs e.g. logical ports of the logical switch not shown in are mapped.

When the packet is headed to the destination machine which is in another logical network the packet is forwarded to the managed switching element . The managed switching element then performs the rest of the L2 processing and sends the packet to an L3 router which implements a centralized logical router not shown .

Similar to L3 router described above by reference to the L3 router is a hardware router or a software router of which the ports are mapped to the ports of a logical router. The L3 router performs the L3 processing on the packet in order to logically route the packet. That is the L3 router sends the packet to another logical router not shown or to the managed switching element .

The managed switching element is a second level managed switching element that functions as an extender in some embodiments. The managed switching element receives a packet from the L3 router and starts performing the L2 processing of the logical processing pipeline . There may be one of more managed switching elements not shown between the managed switching element and the managed switching element . These managed switching elements have network constructs to which the logical constructs of the logical switch not shown in are mapped.

The managed switching element in the example receives the packet from the managed switching element . The managed switching element performs the rest of the L2 processing on the packet in order to logically forward the packet. In this example the managed switching element is also the switching element that directly sends the packet to the destination machine . However there may be one or more managed switching elements not shown between the managed switching element and the destination machine . These managed switching elements have network constructs to which the logical constructs of the logical switch not shown in are mapped.

Although the L2 processing and the L2 processing are performed in a distributed manner in this example the L2 processing and the L2 processing do not have to be performed in a distributed manner. For instance the managed switching element may perform the entire L2 processing and the managed switching element may perform the entire L2 processing . In such case the managed switching element would just relay the packets between the L3 router and the managed switching elements and .

The L2 processing in some embodiments includes eight stages for processing a packet through the logical switch not shown in in a logical network not shown that is implemented across the managed switching elements and . In some embodiments the managed switching element that receives the packet performs a portion of the L2 processing when the managed switching element receives the packet. The managed switching element then performs the rest of the L2 processing .

In some embodiments a packet includes a header and a payload. The header includes in some embodiments a set of fields that contains information used for routing the packet through a network. Logical switches and logical routers may determine switching routing decisions based on the information contained in the header fields and may in some cases modify some or all of the header fields.

In the stage of the L2 processing ingress context mapping is performed on the packet to determine the logical context of the packet. In some embodiments the stage is performed when the logical switch receives the packet e.g. the packet is initially received by the managed switching element . A logical context in some embodiments represents the state of the packet with respect to the logical switch. The logical context may for example specify the logical switch to which the packet belongs the logical port of the logical switch through which the packet was received the logical port of the logical switch through which the packet is to be transmitted the stage of the logical forwarding plane of the logical switch the packet is at etc.

Some embodiments determine the logical context of a packet based on the source MAC address of the packet i.e. the machine from which the packet was sent . Some embodiments perform the logical context lookup based on the source MAC address of the packet and the inport i.e. ingress port of the packet i.e. the port of the managed switching element through which the packet was received . Other embodiments may use other fields in the packet s header e.g. MPLS header VLAN id etc. for determining the logical context of the packet.

After the first stage is performed some embodiments store the information that represents the logical context in one or more fields of the packet s header. These fields may also be referred to as a logical context tag or a logical context ID. Furthermore the logical context tag may coincide with one or more known header fields e.g. the VLAN id field in some embodiments. As such these embodiments do not utilize the known header field or its accompanying features in the manner that the header field is defined to be used. Alternatively some embodiments store the information that represents the logical context as metadata that is associated with instead of stored in the packet itself and passed along with the packet.

In some embodiments the second stage is defined for the logical switch . In some such embodiments the stage operates on the packet s logical context to determine ingress access control of the packet with respect to the logical switch. For example an ingress ACL is applied to the packet to control the packet s access to the logical switch when the logical switch receives the packet. Based on the ingress ACL defined for the logical switch the packet may be further processed e.g. by the stage or the packet may be dropped for example.

In the third stage of the L2 processing an L2 forwarding is performed on the packet in the context of the logical switch. In some embodiments the third stage operates on the packet s logical context to process and forward the packet with respect to the logical switch . For instance some embodiments define a L2 forwarding table or L2 forwarding entries for processing the packet at layer 2.

Moreover when the packet s destination is in another logical network i.e. when the packet s destination logical network is different than the logical network whose traffic is processed by the logical switch the logical switch sends the packet to the logical router which will then perform the L3 processing in order to route the packet to the destination logical network. Thus at the third stage the managed switching element of some embodiments determines that the packet should be forwarded to the logical router through a logical port not shown of the logical switch that is associated with the logical router . In other embodiments the managed switching element does not necessarily determine whether the packet should be forwarded to the logical router . Rather the packet would have an address of a port of the logical router as a destination address and the managed switching element forwards this packet through the logical port of the logical switch according to the destination address.

At the fourth stage egress context mapping is performed to identify a physical result that corresponds to the result of the logical forwarding of the packet. For example the logical processing of the packet may specify that the packet is to be sent out of one or more logical ports e.g. a logical egress port of the logical switch . As such the egress context mapping operation identifies a physical port s of one or more of the managed switching elements including the managed switching elements and that corresponds to the particular logical port of the logical switch . The managed switching element determines that the physical port e.g. a VIF to which the logical port determined at the previous stage is mapped is a port not shown of the managed switching element .

The fifth stage of the L2 processing performs a physical mapping based on the egress context mapping performed at the fourth stage . In some embodiments the physical mapping determines operations for sending the packet towards the physical port that was determined in the fourth stage . For example the physical mapping of some embodiments determines one or more queues not shown associated with one or more ports of the set of ports not shown of the managed switching element that is performing the L2 processing through which to send the packet in order for the packet to reach the physical port s determined in the fifth stage . This way the managed switching elements can forward the packet along the correct path in the network for the packet to reach the determined physical port s .

As shown the sixth stage of the L2 processing is performed by the managed switching element . The sixth stage is similar to the first stage . The stage is performed when the managed switching element receives the packet. At the stage the managed switching element looks up the logical context of the packet and determines that L2 egress access control is left to be performed.

The seventh stage of some embodiments is defined for the logical switch . The seventh stage of some such embodiments operates on the packet s logical context to determine egress access control of the packet with respect to the logical switch. For instance an egress ACL may be applied to the packet to control the packet s access out of the logical switch after logical forwarding has been performed on the packet. Based on the egress ACL defined for the logical switch the packet may be further processed e.g. sent out of a logical port of the logical switch or sent to a dispatch port for further processing or the packet may be dropped for example.

The eighth stage is similar to the fifth stage . At the eighth stage the managed switching element determines a specific physical port not shown of the managed switching element to which the logical egress port of the logical switch is mapped.

The L3 processing includes six stages for processing a packet through the logical switch not shown in that is implemented by the L3 router . As mentioned above L3 processing involves performing a set of logical routing lookups for determining where to route the packet through a layer 3 network.

The first stage performs a logical ingress ACL lookup for determining access control when the logical router receives the packet i.e. when the L3 router which implements the logical router receives the packet . The next stage performs network address translation NAT on the packet. In particular the stage performs destination NAT DNAT to revert the destination address of the packet back to the real address of the destination machine that is hidden from the source machine of the packet. This stage is performed when DNAT is enabled.

The next stage performs a logical L3 routing for determining one or more logical ports to send the packet through the layer 3 network based on the L3 addresses e.g. destination IP address of the packet and routing tables e.g. containing L3 entries . Since the logical router is implemented by the L3 router the routing tables are configured in the L3 router .

At the fourth stage the L3 router of some embodiments also performs source NAT SNAT on the packet. For instance the L3 router replaces the source IP address of the packet with a different IP address in order to hide the source IP address when the source NAT is enabled.

The fifth stage performs logical L3 egress ACL lookups for determining access control before the logical router routes the packet out of the logical router through the port determined in the stage . The L3 egress ACL lookups are performed based on the L3 addresses e.g. source and destination IP addresses of the packet.

The sixth stage performs address resolution in order to translate the destination L3 address e.g. a destination IP address into a destination L2 address e.g. a destination MAC address . In some embodiments the L3 router uses a standard address resolution e.g. by sending out ARP requests or looking up ARP cache to find the destination L2 address that corresponds to the destination IP address.

When the logical router is not coupled to the destination logical network the logical switch sends the packet to another logical router network towards the destination logical network. When the logical router is coupled to the destination logical network the logical switch routes the packet to the destination logical network i.e. the logical switch that forwards the packet for the destination logical network .

The L2 processing in some embodiments includes eight stages for processing the packet through the logical switch in another logical network not shown in that is implemented across the managed switching elements and . In some embodiments the managed switching element in the managed network that receives the packet performs the L2 processing when the managed switching element receives the packet from the managed switching element . The stages are similar to the stage respectively except that the stage are performed by the logical switch i.e. by the managed switching elements and that implement the logical switch . That is the stages are performed to forward the packet received from the L3 router to the destination through the managed switching elements and .

In this example the logical switch forwards data packets between the logical router VM 1 and VM 2. The logical switch forwards data packets between the logical router VM 3 and VM 4. As mentioned above the logical router routes data packets between the logical switches and and possibly other logical routers and switches not shown . The logical switches and and the logical router are logically coupled through logical ports not shown and exchange packets through the logical ports. These logical ports are mapped to physical ports of the L3 router and the managed switching elements and .

In some embodiments each of the logical switches and is implemented across the managed switching elements and and possibly other managed switching elements not shown . In some embodiments the logical router is implemented in the L3 router which is communicatively coupled to the managed switching element .

In this example the managed switching elements and are software switching elements running in hosts and respectively. The managed switching elements and have flow entries which implement the logical switches and . Using these flow entries the managed switching elements and route network data e.g. packets between network elements in the network that are coupled to the managed switching elements and . For instance the managed switching element routes network data between VMs 1 and 3 and the second level managed switching element . Similarly the managed switching element routes network data between VMs 2 and 4 and the second level managed switching element . As shown the managed switching elements and each have three ports depicted as numbered squares through which to exchange data packets with the network elements that are coupled to the managed switching elements and .

The managed switching element is similar to the managed switching element described above by reference to in that the managed switching element is a second level managed switching element that functions as an extender. The managed switching element runs in the same host as the L3 router which in this example is a software router.

In some embodiments tunnels are established by the network control system not shown to facilitate communication between the network elements. For instance the managed switching element is coupled to the managed switching element which runs in the host through a tunnel that terminates at port 2 of the managed switching element as shown. Similarly the managed switching element is coupled to the managed switching element through a tunnel that terminates at port 1 of the managed switching element .

Different types of tunneling protocols are supported in different embodiments. Examples of tunneling protocols include control and provisioning of wireless access points CAPWAP generic route encapsulation GRE GRE Internet Protocol Security IPsec among other types of tunneling protocols.

In this example each of the hosts and includes a managed switching element and several VMs as shown. VMs 1 4 are virtual machines that are each assigned a set of network addresses e.g. a MAC address for L2 an IP address for L3 etc. and can send and receive network data to and from other network elements. The VMs are managed by hypervisors not shown running on the hosts and .

Several example data exchanges through the network architecture will now be described. When VM 1 that is coupled to the logical switch sends a packet to VM 2 that is also coupled to the same logical switch the packet is first sent to the managed switching element . The managed switching element then performs the L2 processing on the packet because the managed switching element is the edge switching element that receives the packet from VM 1. The result of the L2 processing on this packet would indicate that the packet should be sent to the managed switching element to get to VM 2 through port 4 of the managed switching element . Because VMs 1 and 2 are in the same logical network and therefore L3 routing for the packet is not necessary no L3 processing needs to be performed on this packet. The packet is then sent to the managed switching element via the second level managed switching element which is bridging between the managed switching element and the managed switching element . The packet reaches VM 2 through port 4 of the managed switching element .

When VM 1 that is coupled to the logical switch sends a packet to VM 3 that is coupled to the logical switch the packet is first sent to the managed switching element . The managed switching element performs a portion of L2 processing on the packet. However because the packet is sent from one logical network to another i.e. the logical L3 destination address of the packet is for another logical network an L3 processing needs to be performed on this packet.

The managed switching element sends the packet to the second level managed switching element so that the managed switching element performs the rest of the L2 processing on the packet to forward the packet to the L3 router . The result of L3 processing performed at the L3 router would indicate that the packet should be sent back to the managed switching element . The managed switching element then performs a portion of another L2 processing and forwards the packet received from the L3 router back to the managed switching element . The managed switching element performs the L2 processing on the packet received from the managed switching element and the result of this L2 processing would indicate that the packet should be sent to VM 3 through port 5 of the managed switching element .

When VM 1 that is coupled to the logical switch sends a packet to VM 4 that is coupled to the logical switch the packet is first sent to the managed switching element . The managed switching element performs the L2 processing on the packet. However because the packet is sent from one logical network to another an L3 processing needs to be performed.

The managed switching element sends the packet to the L3 router via the managed switching element so that the L3 router performs the L3 processing on the packet. The result of the L3 processing performed at the L3 router would indicate that the packet should be sent to the managed switching element . The managed switching element then performs a portion of L2 processing on the packet received from the managed switching element and the result of this L2 processing would indicate that the packet should be sent to VM 4 through the managed switching element . The managed switching element performs the rest of the L2 processing to determine that the packet should be sent to VM 4 through port 5 of the managed switching element .

The network architecture is similar to the network architecture except that the network architecture additionally includes the managed switching element which runs in the host . The managed switching element of some embodiments is a second level managed switching element that functions as a pool node.

In some embodiments tunnels are established by the network control system not shown to facilitate communication between the network elements. For instance the managed switching element in this example is coupled to the managed switching element which runs in the host through a tunnel that terminates at port 1 of the managed switching element as shown. Similarly the managed switching element is coupled to the managed switching element through a tunnel that terminates at port 2 of the managed switching elements . Also the managed switching elements and are coupled through a tunnel as shown.

The logical router and the logical switches and are implemented in the L3 router and the managed switching elements and as described by reference to above except that the second level managed switching element is involved in the data packet exchange. That is the managed switching elements and exchange packets through the managed switching element .

The controller cluster is a set of network controllers or controller instances that manage the network elements including the managed switching element . The managed switching element in this example is a software switching element implemented in the host that includes a user space and a kernel . The managed switching element includes a control daemon running in the user space and controller patch and a bridge running in the kernel . The user space and the kernel in some embodiments is of an operating system for the host while in other embodiments the user space and the kernel is of a virtual machine that is running on the host .

In some embodiments the controller cluster communicates with a control daemon e.g. by using OpenFlow protocol or another communication protocol which in some embodiments is an application running in the background of the user space . The control daemon communicates with the controller cluster in order to process and route packets that the managed switching element receives. Specifically the control daemon in some embodiments receives configuration information from the controller cluster and configures the controller patch . For example the control daemon receives commands from the controller cluster regarding operations for processing and routing packets that the managed switching element receives.

The control daemon also receives configuration information for the controller patch to set up ports not shown connecting to the logical router not shown implemented in the namespace such that the logical router populates the routing tables and other tables with appropriate entries.

The controller patch is a module that runs in the kernel . In some embodiments the control daemon configures the controller patch . When configured the controller patch contains rules e.g. flow entries regarding processing and forwarding the packets to receive. The controller patch of some embodiments also creates a set of ports e.g. VIFs to exchange packets with the namespace .

The controller patch receives packets from a network stack of the kernel or from the bridge . The controller patch determines which namespace to which to send the packets based on the rules regarding processing and routing the packets. The controller patch also receives packets from the namespace and sends the packets to the network stack or the bridge based on the rules. More details about architecture of a managed switching element are described in U.S. patent application Ser. No. 13 177 535.

The namespace e.g. Linux namespace is a container created in the host . The namespace can implement network stacks network devices network addresses routing tables network address translation tables network caches etc. not all of these are shown in . The namespace thus can implement a logical router when the namespace is configured to handle packets with logical source or destination addresses. The namespace can be configured to handle such packets for example by configuring the routing tables of the namespace. In some embodiments the namespace populates the routing tables as the namespace connects to the managed switching element and exchanges packets i.e. dynamic routing . In other embodiments the controller cluster may directly configure the routing tables by populating the routing tables with routes.

Moreover the namespace in some embodiments also performs network address translation NAT on the packets that the namespaces route. For instance when the namespace changes the source network address of the received packet into another network address i.e. performs source NAT .

The bridge routes network data between the network stack and network hosts external to the host i.e. network data received through the NIC . As shown the bridge routes network data between the network stack and the NIC and between the controller patch and the NIC . The bridge of some embodiments performs standard L2 packet learning and routing.

The network stack can receive packets from network hosts external to the managed switching element through the NIC . The network stack then sends the packets to the controller patch . In some cases the packets are received from network hosts external to the managed switching element through tunnels. In some embodiments the tunnels terminate at the network stack . Thus when the network stack receives a packet through a tunnel the network stack unwraps the tunnel header i.e. decapsulates the payload and sends the unwrapped packet to the controller patch .

An example operation of the managed switching element and the namespace will now be described. In this example tunnels are established between the managed switching element and the managed switching elements and not shown in that are external to the host . That is the managed switching elements and are connected through the tunnels as illustrated in . The tunnels terminate at the network stack .

The managed switching element sends a packet sent by VM1 to VM 4 to the managed switching element . The packet is received by the NIC and then is sent to the bridge . Based on the information in the packet header the bridge determines that the packet is sent over the established tunnel and sends the packet to the network stack . The network stack unwraps the tunnel header and sends the unwrapped packet to the controller patch .

According to the rules that the controller patch has the controller patch sends the packet to the namespace because the packet is sent from one logical network to another logical network. For instance the rules may say a packet with certain destination MAC address should be sent to the namespace . In some cases the controller patch removes logical context from the packet before sending the packet to the namespace. The namespace then performs an L3 processing on the packet to route the packet between the two logical networks.

By performing the L3 processing the namespace determines that the packet should be sent to the controller patch because the destination network layer address should go to a logical switch that belongs to the destination logical network. The controller patch receives the packet and sends the packet through the network stack the bridge and the NIC over the tunnel to the managed switching element that implements the logical switch that belongs to the destination logical network.

As described above some embodiments implement the L3 router in the namespace . Other embodiments however may implement the L3 router in a VM that runs on the host .

The logical switches and and the logical router are logically coupled through logical ports. As shown a logical port X of the logical switch is coupled to the logical port 1 of the logical router . Similarly a logical port Y of the logical switch is coupled to the logical port 2 of the logical router . The logical switches and exchange data packets with the logical router through these logical ports. Also in this example the logical switch associates the logical port X with a MAC address 01 01 01 01 01 01 which is a MAC address of the logical port 1 of the logical router . When the logical switch receives a packet that needs an L3 processing the logical switch sends the packet out to the logical router through port X. Similarly the logical switch associates the logical port Y with a MAC address 01 01 01 01 01 02 which is a MAC address of the logical port 2 of the logical router . When the logical switch receives a packet that needs an L3 processing the logical switch sends the packet out to the logical router through port Y.

In this example the controller cluster not shown in configures the managed switching element such that port 1 of the managed switching element is associated with the same MAC address 01 01 01 01 01 01 that is associated with port X of the logical switch . Accordingly when the managed switching element receives a packet that has this MAC address as destination MAC address the managed switching element sends the packet out to the L3 router configured in the namespace through the port 1 of the managed switching element . As such port X of the logical switch is mapped to port 1 of the managed switching element .

Similarly port 2 of the managed switching element is associated with the same MAC address 01 01 01 01 01 02 that is associated with port Y of the logical switch . Accordingly when the managed switching element receives a packet that has this MAC address as destination MAC address the managed switching element sends the packet out to the L3 router through the port 2 of the managed switching element . As such port Y of the logical switch is mapped to port 2 of the managed switching element .

In this example the logical router has logical ports 1 and 2 and other logical ports not shown . Port 1 of the logical router is associated with an IP address 1.1.1.1 24 which represents a subnet behind port 1. That is when the logical router receives a packet to route and the packet has a destination IP address e.g. 1.1.1.10 the logical router sends this packet towards the destination logical network e.g. a logical subnet through port 1.

Similarly port 2 of the logical router in this example is associated with an IP address 1.1.2.1 24 which represents a subnet behind port 2. The logical router sends a packet with a destination IP address e.g. 1.1.2.10 to the destination logical network through port 2.

In this example the L3 router implements the logical router by populating the L3 router s routing tables not shown with routes. In some embodiments the L3 router populates its routing tables when the managed switching element establishes connection with the L3 router and send a packet. For instance when the L3 router receives an initial packet from the managed switching element the L3 router finds out that packets that have the initial packet s source address as destination addresses should be sent to the managed switching element . The L3 router may also perform a standard address resolution e.g. by sending out ARP requests to find out where to send the initial packet. The L3 router will store these routes in the routing tables and look up these tables when making routing decisions for the packets that the L3 router receives subsequently. Other L3 routers not shown may populate their routing tables in a similar manner.

In other embodiments the controller cluster configures the routing table of the L3 router such that port 1 of the L3 router is associated with the same IP address that is associated with port 1 of the logical router . Similarly port 2 of the L3 router is associated with the same IP address that is associated with port 2 of the logical router . In a similar manner another logical switch not shown may be implemented in another L3 router not shown of the managed switching element. In some of these embodiments the control cluster may employ one or more routing protocols to configure the L3 router.

When VM 1 that is coupled to the logical switch sends a packet to VM 4 that is coupled to the logical switch the packet is first sent to the managed switching element through port 4 of the managed switching element . The managed switching element performs an L2 processing on the packet.

As shown in the top half of the managed switching element includes a forwarding table that includes rules e.g. flow entries for processing and forwarding the packet . When the managed switching element receives the packet from VM 1 through port 4 of the managed switching element the managed switching element begins processing the packet based on the forwarding tables of the managed switching element . In this example the packet has a destination IP address of 1.1.2.10 which is the IP address of VM 4. The packet s source IP address is 1.1.1.10. The packet also has VM 1 s MAC address as a source MAC address and the MAC address of the logical port 1 i.e. 01 01 01 01 01 01 of the logical router as a destination MAC addresses.

The managed switching element identifies a record indicated by an encircled 1 referred to as record 1 in the forwarding tables that implements the context mapping of the stage . The record 1 identifies the packet s logical context based on the inport which is the port 4 through which the packet is received from VM 1. In addition the record 1 specifies that the managed switching element store the logical context of the packet in a set of fields e.g. a VLAN id field of the packet s header in some embodiments. In other embodiments the managed switching element stores the logical context i.e. the logical switch to which the packet belongs as well as the logical ingress port of that logical switch in a register or meta field of the switch rather than in the packet. The record 1 also specifies the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . A dispatch port is described in U.S. patent application Ser. No. 13 177 535.

Based on the logical context and or other fields stored in the packet s header the managed switching element identifies a record indicated by an encircled 2 referred to as record 2 in the forwarding tables that implements the ingress ACL of the stage . In this example the record 2 allows the packet to be further processed i.e. the packet can get through the ingress port of the logical switch and thus specifies the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . In addition the record 2 specifies that the managed switching element store the logical context i.e. the packet has been processed by the second stage of the processing pipeline of the packet in the set of fields of the packet s header.

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 3 referred to as record 3 in the forwarding tables that implements the logical L2 forwarding of the stage . The record 3 specifies that a packet with the MAC address of the logical port 1 of the logical router as a destination MAC address is to be sent to the logical port X of the logical switch .

The record 3 also specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . Also the record 3 specifies that the managed switching element store the logical context i.e. the packet has been processed by the third stage of the processing pipeline in the set of fields of the packet s header.

Based on the logical context and or other fields stored in the packet s header the managed switching element identifies a record indicated by an encircled 4 referred to as record 4 in the forwarding tables that implements the context mapping of the stage . In this example the record 4 identifies port 1 of the managed switching element to which port 1 of the L3 router is coupled as the port that corresponds to the logical port X of the logical switch to which the packet is to be forwarded. The record 4 additionally specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Based on the logical context and or other fields stored in the packet s header the managed switching element then identifies a record indicated by an encircled 5 referred to as record 5 in the forwarding tables that implements the physical mapping of the stage . The record 5 specifies that the packet is to be sent through port 1 of the managed switching element in order for the packet to reach the managed switching element . In this case the managed switching element is to send the packet out of the port 1 of managed switching element that is coupled to the managed switching element .

As shown in the bottom half of the managed switching element includes a forwarding table that includes rules e.g. flow entries for processing and routing the packet . When the managed switching element receives the packet from the managed switching element the managed switching element begins processing the packet based on the forwarding tables of the managed switching element . The managed switching element identifies a record indicated by an encircled 1 referred to as record 1 in the forwarding tables that implements the context mapping of the stage . The record 1 identifies the packet s logical context based on the logical context that is stored in the packet s header. The logical context specifies that the packet has been processed by the second and third stages and which were performed by the managed switching element . As such the record 1 specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 2 referred to as record 2 in the forwarding tables that implements the egress ACL of the stage . In this example the record 2 allows the packet to be further processed e.g. the packet can get out of the logical switch through port X of the logical switch and thus specifies the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . In addition the record 2 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline of the packet in the set of fields of the packet s header.

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 3 referred to as record 3 in the forwarding tables that implements the physical mapping of the stage . The record 3 specifies the port of the managed switching element through which the packet is to be sent in order for the packet to reach the L3 router . In this case the managed switching element is to send the packet out of port 1 of managed switching element that is coupled to the port 1 of the L3 router . In some embodiments the managed switching element removes the logical context from the packet before sending the packet to the L3 router .

As shown in the top half of the L3 router includes an ingress ACL table a routing table and an egress ACL table that includes entries for processing and routing the packet . When the L3 router receives the packet from the managed switching element the L3 router begins processing the packet based on these tables of the L3 router . The L3 router identifies an entry indicated by an encircled 1 referred to as entry 1 in the ingress ACL table that implements L3 ingress ACL by specifying that the L3 router should accept the packet based on the information in the header of the packet . The L3 router then identifies an entry indicated by an encircled 2 referred to as entry 2 in the routing table that implements L3 routing by specifying that the packet with its destination IP address i.e. 1.1.2.10 should be sent to the logical switch through port 2 of the logical router . The L3 router then identifies an entry indicated by an encircled 3 referred to as entry 3 in the egress ACL table that implements L3 egress ACL by specifying that the L3 router can send the packet out through port 2 of the logical router based on the information in the header of the packet . Also the L3 router rewrites the source MAC address for the packet to the MAC address of port 2 of the L3 router i.e. 01 01 01 01 01 02 .

The L3 router then performs an address resolution to translate the destination IP address into the destination MAC address. In this example the L3 router looks up an ARP cache to find the destination MAC address to which the destination IP address is mapped. The L3 router may send out ARP requests if the ARP cache does not have a corresponding MAC address for the destination IP address. The destination IP address would be resolved to the MAC address of VM 4. The L3 router then rewrites the destination MAC of the packet using the MAC address to which the destination IP address is resolved. The L3 router would send the packet to the logical switch through the logical port 2 of the L3 router based on the new destination MAC address.

As shown in the bottom half of the managed switching element includes a forwarding table that includes rules e.g. flow entries for processing and forwarding the packet . When the managed switching element receives the packet from the L3 router through port 2 of the managed switching element the managed switching element begins processing the packet based on the forwarding tables of the managed switching element . The managed switching element identifies a record indicated by an encircled 4 referred to as record 4 in the forwarding tables that implements the context mapping of the stage . The record 4 identifies the packet s logical context based on the inport which is the port 2 through which the packet is received from the L3 router . In addition the record 4 specifies that the managed switching element store the logical context of the packet in a set of fields e.g. a VLAN id field of the packet s header. The record 4 also specifies the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Based on the logical context and or other fields stored in the packet s header the managed switching element identifies a record indicated by an encircled 5 referred to as record 5 in the forwarding tables that implements the ingress ACL of the stage . In this example the record 5 allows the packet to be further processed and thus specifies the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . In addition the record 5 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline of the packet in the set of fields of the packet s header.

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 6 referred to as record 6 in the forwarding tables that implements the logical L2 forwarding of the stage . The record 6 specifies that a packet with the MAC address of VM 4 as the destination MAC address should be forwarded through the logical port not shown of the logical switch .

The record 6 also specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . Also the record 6 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline in the set of fields of the packet s header.

Based on the logical context and or other fields stored in the packet s header the managed switching element identifies a record indicated by an encircled 7 referred to as record 7 in the forwarding tables that implements the context mapping of the stage . In this example the record 7 identifies port 5 of the managed switching element to which VM 4 is coupled as the port that corresponds to the logical port determined at stage of the logical switch to which the packet is to be forwarded. The record 7 additionally specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Based on the logical context and or other fields stored in the packet s header the managed switching element then identifies a record indicated by an encircled 8 referred to as record 8 in the forwarding tables that implements the physical mapping of the stage . The record 8 specifies a port not shown of the managed switching element through which the packet is to be sent in order for the packet to reach the managed switching element . In this case the managed switching element is to send the packet out of the port of managed switching element that is coupled to the managed switching element .

As shown in the managed switching element includes a forwarding table that includes rules e.g. flow entries for processing and routing the packet . When the managed switching element receives the packet from the managed switching element the managed switching element begins processing the packet based on the forwarding tables of the managed switching element . The managed switching element identifies a record indicated by an encircled 4 referred to as record 4 in the forwarding tables that implements the context mapping of the stage . The record 4 identifies the packet s logical context based on the logical context that is stored in the packet s header. The logical context specifies that the packet has been processed by the stages and which were performed by the managed switching element . As such the record 4 specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 5 referred to as record 5 in the forwarding tables that implements the egress ACL of the stage . In this example the record 5 allows the packet to be further processed and thus specifies the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . In addition the record 5 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline of the packet in the set of fields of the packet s header.

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 6 referred to as record 6 in the forwarding tables that implements the physical mapping of the stage . The record 6 specifies the port 5 of the managed switching element through which the packet is to be sent in order for the packet to reach VM 4. In this case the managed switching element is to send the packet out of port 5 of managed switching element that is coupled to VM 4. In some embodiments the managed switching element removes the logical context from the packet before sending the packet to VM 4.

The process begins by receiving at a packet from a source machine. The process then performs at a portion of L2 processing. As the process performs the L2 processing the process determines at whether the packet needs to be sent to a second level managed switching element for further processing of the packet. In some embodiments the process makes this determination based on the destination L2 address of the packet. The process looks at the destination L2 address and sends out the packet through a port that is associated with the destination L2 address. For instance when the packet s destination L2 address is an L2 address of an L3 router the process sends the packet out of a port that is associated with the managed switching element that is associated with an L3 router. When the packet s destination L2 address is an L2 address of the destination machine the process sends the packet to the managed switching element that is directly connected to the destination machine or to the managed switching element that is closer in the route to the destination machine.

When the process determines at that the packet needs to be sent to a second level managed switching element the process sends at the packet to a second level managed switching element that is communicatively coupled to an L3 router that implements the logical router. Otherwise the process sends at the packet to the destination machine or to another managed switching element. The process then ends.

As shown the managed switching element is coupled to two L3 routers 1 and 2. The flow entries that the managed switching element contains are shown on the right side of the figure. The flow entries indicate that the traffic that is addressed to go from one L3 router to another L3 router should directly go to the other L3 router.

Also this figure illustrates that the additional router can be provisioned in the host in order to provide additional routing resources when more managed switching elements are provisioned and rely on the existing L3 router to route additional network traffic.

The process begins by receiving at a packet from a first L3 router. The process then determines at whether the packet is addressed to a second L3 router that is implemented in the same host in which the first L3 router is implemented. The process determines this by examining the information in the header of the packet e.g. destination MAC address .

When the process determines at that the packets are headed to the second L3 router the process sends the packet to the second L3 router. Otherwise the process sends the packet toward the destination of the packet e.g. another managed switching element or a destination machine . The process then ends.

The managed switching element is similar to the managed switching element described above by reference to in that the managed switching element is also an edge switching element that directly receives the packets from a machine coupled to the edge switching element. The managed switching element receives packets from the source machine . When the managed switching element receives a packet from the source machine the managed switching element performs a portion of the L2 processing on the packet in order to logically forward the packet. When the packet is headed to the destination machine which is in another logical network the packet is forwarded to the managed switching element

There may be one or more managed switching elements not shown between the managed switching element and the managed switching element . These managed switching elements have network constructs e.g. PIFs VIFs etc. to which the logical constructs e.g. logical ports of the logical switch not shown in are mapped.

The managed switching element is a second level managed switching element that functions as an extender in some embodiments. The managed switching element performs the rest of the L2 processing and also performs the L3 processing . The managed switching element also performs a portion of the L2 processing of the logical processing pipeline . The managed switching element then sends the packet to the managed switching element .

There may be one of more managed switching elements not shown between the managed switching element and the managed switching element . These managed switching elements have network constructs to which the logical constructs of the logical switch not shown in are mapped.

The managed switching element in the example receives the packet from the managed switching element . The managed switching element performs the rest of the L2 processing on the packet in order to logically forward the packet. In this example the managed switching element is also the switching element that directly sends the packet to the destination machine . However there may be one or more managed switching elements not shown between the managed switching element and the destination machine . These managed switching elements have network constructs to which the logical constructs of the logical switch not shown in are mapped.

Although the L2 processing and the L2 processing are performed in a distributed manner in this example the L2 processing and the L2 processing do not have to be performed in a distributed manner. For instance the managed switching element may perform the entire L2 processing and the managed switching element may perform the entire L2 processing . In such case the managed switching element would perform only the L3 processing of the logical processing pipeline .

The L2 processing in some embodiments includes seven stages for processing a packet through the logical switch not shown in in a logical network not shown that is implemented across the managed switching elements and . In some embodiments the managed switching element that receives the packet performs a portion of the L2 processing when the managed switching element receives the packet. The managed switching element then performs the rest of the L2 processing .

The first five stages are similar to the first five stages described above by reference to . In the stage of the L2 processing ingress context mapping is performed on the packet to determine the logical context of the packet. In some embodiments the stage is performed when the logical switch receives the packet e.g. the packet is initially received by the managed switching element . After the first stage is performed some embodiments store the information that represents the logical context in one or more fields of the packet s header.

In some embodiments the second stage is defined for the logical switch . In some such embodiments the stage operates on the packet s logical context to determine ingress access control of the packet with respect to the logical switch. For example an ingress ACL is applied to the packet to control the packet s access to the logical switch when the logical switch receives the packet. Based on the ingress ACL defined for the logical switch the packet may be further processed e.g. by the stage or the packet may be dropped for example.

In the third stage of the L2 processing an L2 forwarding is performed on the packet in the context of the logical switch. In some embodiments the third stage operates on the packet s logical context to process and forward the packet with respect to the logical switch . For instance some embodiments define an L2 forwarding table or L2 forwarding entries for processing the packet at layer 2. Moreover when the packet s destination is in another logical network i.e. when the packet s destination logical network is different than the logical network whose traffic is processed by the logical switch the logical switch sends the packet to the logical router which will then perform the L3 processing in order to route the packet to the destination logical network. Thus at the third stage the managed switching element determines that the packet should be forwarded to the logical router through a logical port not shown of the logical switch that is associated with the logical router .

At the fourth stage egress context mapping is performed to identify a physical result that corresponds to the result of the logical forwarding of the packet. For example the logical processing of the packet may specify that the packet is to be sent out of one or more logical ports e.g. a logical egress port of the logical switch . As such the egress context mapping operation identifies a physical port s of one or more of the managed switching elements including the managed switching elements and that corresponds to the particular logical port of the logical switch . The managed switching element determines that the physical port e.g. a VIF to which the logical port determined at the previous stage is mapped is a port not shown of the managed switching element .

The fifth stage of the L2 processing performs a physical mapping based on the egress context mapping performed at the fourth stage . In some embodiments the physical mapping determines operations for sending the packet towards the physical port that was determined in the fourth stage . For example the physical mapping of some embodiments determines one or more queues not shown associated with one or more ports of the set of ports not shown of the managed switching element that is performing the L2 processing through which to send the packet in order for the packet to reach the physical port s determined in the fourth stage . This way the managed switching elements can forward the packet along the correct path in the network for the packet to reach the determined physical port s .

As shown the sixth stage of the L2 processing is performed by the managed switching element . The sixth stage is similar to the first stage . The stage is performed when the managed switching element receives the packet. At the stage the managed switching element looks up the logical context of the packet and determines that L2 egress access control is left to be performed.

The seventh stage of some embodiments is defined for the logical switch . The seventh stage of some such embodiments operates on the packet s logical context to determine egress access control of the packet with respect to the logical switch . For instance an egress ACL may be applied to the packet to control the packet s access out of the logical switch after logical forwarding has been performed on the packet. Based on the egress ACL defined for the logical switch the packet may be further processed e.g. sent out of a logical port of the logical switch or sent to a dispatch port for further processing or the packet may be dropped for example.

The L3 processing includes six stages for processing a packet through the logical switch not shown in that is implemented in the managed switching element based on the L3 flow entries of the managed switching element . As mentioned above L3 processing involves performing a set of logical routing lookups for determining where to route the packet through a layer 3 network.

The first stage performs a logical ingress ACL lookup for determining access control when the logical router receives the packet i.e. when the managed switching element which implements the logical router receives the packet . The next stage performs DNAT to revert the destination address of the packet back to the real address of the destination machine that is hidden from the source machine of the packet. This stage is performed when DNAT is enabled.

The next stage performs a logical L3 routing for determining one or more logical ports to which send the packet through the layer 3 network based on the L3 addresses e.g. destination IP address of the packet and routing tables e.g. containing L3 entries . Since the logical router is implemented by the managed switching element the L3 flow entries are configured in the managed switching element .

At the fourth stage the managed switching element of some embodiments also performs SNAT on the packet. For instance the managed switching element replaces the source IP address of the packet with a different IP address in order to hide the source IP address when the source NAT is enabled. Also as will be described further below the managed switching element may use a NAT daemon to receive flow entries for translating network addresses. A NAT daemon will be described further below by reference to .

The fifth stage performs logical L3 egress ACL lookups for determining access control before the logical router routes the packet out of the logical router through the port determined in the stage . The L3 egress ACL lookups are performed based on the L3 addresses e.g. source and destination IP addresses of the packet.

The sixth stage performs address resolution in order to translate the destination L3 address e.g. a destination IP address into a destination L2 address e.g. a destination MAC address . In some embodiments the managed switching element uses a standard address resolution e.g. by sending out ARP requests or looking up ARP cache to find the destination L2 address that corresponds to the destination IP address. Also as will be described further below the managed switching element of some embodiments may use an L3 daemon to receive flow entries for resolving L3 addresses into L2 addresses. An L3 daemon will be described further below by reference to .

When the logical router is not coupled to the destination logical network the logical switch sends the packet to another logical router network towards the destination logical network. When the logical router is coupled to the destination logical network the logical switch routes the packet to the destination logical network i.e. the logical switch that forwards the packet for the destination logical network .

The L2 processing in some embodiments includes seven stages for processing the packet through the logical switch in another logical network not shown in that is implemented across the managed switching elements and not shown . The stages are similar to the stage respectively except that the stage are performed by the logical switch i.e. by the managed switching elements and that implement the logical switch .

In this example the logical switch forwards data packets between the logical router VM 1 and VM 2. The logical switch forwards data packets between the logical router VM 3 and VM 4. As mentioned above the logical router routes data packets between the logical switches and and possibly other logical routers and switches not shown . The logical switches and and the logical router are logically coupled through logical ports not shown and exchange packets through the logical ports. These logical ports are mapped to physical ports of the L3 router and the managed switching elements and .

In some embodiments each of the logical switches and is implemented across the managed switching elements and and possibly other managed switching elements not shown . In some embodiments the logical router is implemented in the L3 router which is communicatively coupled to the managed switching element .

In this example the managed switching elements and are software switching elements running in hosts and respectively. The managed switching elements and have flow entries which implement the logical switches and . Using these flow entries the managed switching elements and forward network data e.g. packets between network elements in the network that are coupled to the managed switching elements and . For instance the managed switching element routes network data between VMs 1 and 3 and the second level managed switching element . Similarly the managed switching element routes network data between VMs 2 and 4 and the second level managed switching element . As shown the managed switching elements and each have three ports depicted as numbered squares through which to exchange data packets with the network elements that are coupled to the managed switching elements and .

The managed switching element is similar to the managed switching element described above by reference to in that the managed switching element is a second level managed switching element that functions as an extender. The managed switching element also implements the logical router based on the flow entries. Using these flow entries the managed switching element route packets at L3. In this example the logical router implemented in the managed switching element routes packets between the logical switch that is implemented across the managed switching elements and and the logical switch implemented across the managed switching element and .

In this example the managed switching element is coupled to the managed switching element which runs in the host through a tunnel that terminates at port 2 of the managed switching element as shown. Similarly the managed switching element is coupled to the managed switching element through a tunnel that terminates at port 1 of the managed switching elements .

In this example each of the hosts and includes a managed switching element and several VMs as shown. The VMs 1 4 are virtual machines that are each assigned a set of network addresses e.g. a MAC address for L2 an IP address for L3 etc. and can send and receive network data to and from other network elements. The VMs are managed by hypervisors not shown running on the hosts and .

Several example data exchanges through the network architecture will now be described. When VM 1 that is coupled to the logical switch sends a packet to VM 2 that is also coupled to the same logical switch the packet is first sent to the managed switching element . The managed switching element then performs the L2 processing on the packet because the managed switching element is the edge switching element that receives the packet from VM 1. The result of the L2 processing on this packet would indicate that the packet should be sent to the managed switching element to get to VM 2 through port 4 of the managed switching element . Because VMs 1 and 2 are in the same logical network and therefore L3 routing for the packet is not necessary no L3 processing needs to be performed on this packet. The packet is then sent to the managed switching element via the second level managed switching element which is bridging between the managed switching element and the managed switching element . The packet reaches VM 2 through port 4 of the managed switching element .

When VM 1 that is coupled to the logical switch sends a packet to VM 3 that is coupled to the logical switch the packet is first sent to the managed switching element . The managed switching element performs a portion of L2 processing on the packet. However because the packet is sent from one logical network to another i.e. the logical L3 destination address of the packet is for another logical network an L3 processing needs to be performed on this packet.

The managed switching element sends the packet to the second level managed switching element so that the managed switching element performs the rest of the L2 processing and the L3 processing on the packet. The managed switching element then performs a portion of another L2 processing and forwards the packet to the managed switching element . The managed switching element performs the L2 processing on the packet received from the managed switching element and the result of this L2 processing would indicate that the packet should be sent to VM 3 through port 5 of the managed switching element .

When VM 1 that is coupled to the logical switch sends a packet to VM 4 that is coupled to the logical switch the packet is first sent to the managed switching element . The managed switching element performs the L2 processing on the packet. However because the packet is sent from one logical network to another an L3 processing needs to be performed.

The managed switching element sends the packet to the managed switching element so that the managed switching element performs the rest of L2 processing and the L3 processing on the packet. The result of the L3 processing performed at the managed switching element would indicate that the packet should be sent to the managed switching element . The managed switching element then performs a portion of L2 processing on the packet and the result of this L2 processing would indicate that the packet should be sent to VM 4 through the managed switching element . The managed switching element performs the rest of the L2 processing to determine that the packet should be sent to VM 4 through port 5 of the managed switching element .

The network architecture is similar to the network architecture except that the network architecture additionally includes the managed switching element which runs in the host . The managed switching element of some embodiments is a second level managed switching element that functions as a pool node.

In some embodiments tunnels are established by the network control system not shown to facilitate communication between the network elements. For instance the managed switching element in this example is coupled to the managed switching element which runs in the host through a tunnel that terminates at port 1 of the managed switching element as shown. Similarly the managed switching element is coupled to the managed switching element through a tunnel that terminates at port 2 of the managed switching elements . Also the managed switching elements and are coupled through a tunnel as shown.

The logical router and the logical switches and are implemented in the managed switching elements and as described by reference to above except that the second level managed switching element is involved in the data packet exchange. That is the managed switching elements and exchange packets through the managed switching element . The managed switching elements and exchange packets through the managed switching element . The managed switching elements and exchange packets through the managed switching element .

The controller cluster is a set of network controllers or controller instances that manage the network elements including the managed switching element . The managed switching element in this example is a software switching element implemented in the host that includes a user space and a kernel . The managed switching element includes a control daemon running in the user space and a controller patch and a bridge running in the kernel . Also running in the user space is a NAT daemon which will be described further below. The user space and the kernel in some embodiments are of an operating system for the host while in other embodiments the user space and the kernel are of a virtual machine that is running on the host .

In some embodiments the controller cluster communicates with a control daemon e.g. by using OpenFlow protocol or some other communication protocol which in some embodiments is an application running in the background of the user space . The control daemon communicates with the controller cluster in order to process and route packets that the managed switching element receives. Specifically the control daemon in some embodiments receives configuration information from the controller cluster and configures the controller patch . For example the control daemon receives commands from the controller cluster regarding operations for processing and routing packets at L2 and L3 that the managed switching element receives.

The controller patch is a module that runs in the kernel . In some embodiments the control daemon configures the controller patch . When configured the controller patch contains rules e.g. flow entries regarding processing forwarding and routing the packets to receive. The controller patch implements both logical switches and a logical router.

In some embodiments the controller patch uses the NAT daemon for network address translation. As will be described further below the NAT daemon generates flow entries regarding network address translation and sends back the flow entries to the managed switching element to use. A NAT daemon will be described further below.

The controller patch receives packets from a network stack of the kernel or from the bridge . The bridge routes network data between the network stack and network hosts external to the host i.e. network data received through the NIC . As shown the bridge routes network data between the network stack and the NIC and between the network stack and the NIC . The bridge of some embodiments performs standard L2 packet learning and routing.

The network stack can receive packets from network hosts external to the managed switching element through the NIC . The network stack then sends the packets to the controller patch . In some cases the packets are received from network hosts external to the managed switching element through tunnels. In some embodiments the tunnels terminate at the network stack . Thus when the network stack receives a packet through a tunnel the network stack unwraps the tunnel header i.e. decapsulates the payload and sends the unwrapped packet to the controller patch .

An example operation of the managed switching element will now be described. In this example tunnels are established between the managed switching element and the managed switching elements and not shown in that are external to the host . That is the managed switching elements and are connected through the tunnels as illustrated in . The tunnels terminate at the network stack .

The managed switching element sends a packet sent by VM1 to VM 4 to the managed switching element . The packet is received by the NIC and then is sent to the bridge . Based on the information in the packet header the bridge determines that the packet is sent over the established tunnel and sends the packet to the network stack . The network stack unwraps the tunnel header and sends the unwrapped packet to the controller patch .

According to the flow entries that the controller patch has the controller patch performs L3 processing to route the packet because the packet is sent from one logical network to another logical network. By performing the L3 processing and some L2 processing the managed switching element determines that the packet should be sent to the managed switching element because the destination network layer address should go to a logical switch that belongs to the destination logical network. The controller patch sends the packet through the network stack the bridge and the NIC over the tunnel to the managed switching element that implements the logical switch that belongs to the destination logical network.

The logical switches and and the logical router are logically coupled through logical ports. This particular configuration of the logical switches and is the same as the configuration illustrated in an example described above by reference to .

In the example of the controller cluster not shown in configures the managed switching element by supplying flow entries to the managed switching element such that the managed switching element implements the logical router based on the flow entries.

As shown in the bottom half of the managed switching element includes L2 entries and and L3 entries . These entries are flow entries that the controller cluster not shown supplies to the managed switching element . Although these entries are depicted as three separate tables the tables do not necessarily have to be separate tables. That is a single table may include all these flow entries.

When the managed switching element receives a packet from the managed switching element that is sent from VM 1 towards VM 4 the managed switching element begins processing the packet based on the flow entries of the managed switching element . The managed switching element identifies a record indicated by an encircled 1 referred to as record 1 in the forwarding tables that implements the context mapping of the stage . The record 1 identifies the packet s logical context based on the logical context that is stored in the packet s header. The logical context specifies that the packet has been processed by the portion of logical processing i.e. L2 ingress ACL L2 forwarding performed by the managed switching element . As such the record 1 specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 2 referred to as record 2 in the forwarding tables that implements the egress ACL of the stage . In this example the record 2 allows the packet to be further processed e.g. the packet can get out of the logical switch through port X of the logical switch and thus specifies the packet be further processed by the flow entries of the managed switching element e.g. by sending the packet to a dispatch port . In addition the record 2 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline of the packet in the set of fields of the packet s header. It is to be noted that all records specify that a managed switching element performing logical processing update the logical context store in the set of fields whenever a managed switching element performs some portion of logical processing based on a record. 

The managed switching element continues processing the packet based on the flow entries. The managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 3 referred to as record 3 in the L3 entries that implements L3 ingress ACL by specifying that the managed switching element should accept the packet through the logical port 1 of the logical router based on the information in the header of the packet .

The managed switching element then identifies a flow entry indicated by an encircled 4 referred to as record 4 in the L3 entries that implements L3 routing by specifying that the packet with its destination IP address e.g. 1.1.2.10 should be allowed to exit out of port 2 of the logical router . Also the record 4 or another record in the routing table not shown indicates that the source MAC address for the packet is to be rewritten to the MAC address of port 2 of the logical router i.e. 01 01 01 01 01 02 . The managed switching element then identifies a flow entry indicated by an encircled 5 referred to as record 5 in the L3 entries that implements L3 egress ACL by specifying that the managed switching element can send the packet out through port 2 of the logical router based on the information e.g. source IP address in the header of the packet .

Based on the logical context and or other fields stored in the packet s header the managed switching element identifies a record indicated by an encircled 6 referred to as record 6 in the L2 entries that implements the ingress ACL of the stage . In this example the record 6 allows the packet to be further processed and thus specifies the packet be further processed by the managed switching element e.g. by sending the packet to a dispatch port . In addition the record 6 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline of the packet in the set of fields of the packet s header.

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 7 referred to as record 7 in the forwarding tables that implements the logical L2 forwarding of the stage . The record 7 specifies that a packet with the MAC address of VM 4 as destination MAC address should be forwarded through a logical port not shown of the logical switch that is connected to VM 4.

The record 7 also specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . Also the record 7 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline in the set of fields of the packet s header.

Based on the logical context and or other fields stored in the packet s header the managed switching element identifies a record indicated by an encircled 8 referred to as record 8 in the forwarding tables that implements the context mapping of the stage . In this example the record 8 identifies port 5 of the managed switching element to which VM 4 is coupled as the port that corresponds to the logical port determined at stage of the logical switch to which the packet is to be forwarded. The record 8 additionally specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Based on the logical context and or other fields stored in the packet s header the managed switching element then identifies a record indicated by an encircled 9 referred to as record 9 in the L2 entries that implements the physical mapping of the stage . The record 9 specifies a port not shown of the managed switching element through which the packet is to be sent in order for the packet to reach the managed switching element . In this case the managed switching element is to send the packet out of that port of managed switching element that is coupled to the managed switching element .

The managed switching element is an edge switching element that directly receives the packets from a machine coupled to the edge switching element. The managed switching element receives packets from the source machine . When the managed switching element receives a packet from the source machine the managed switching element in some embodiments performs the entire logical processing pipeline on the packet in order to logically forward and route the packet.

When a received packet is headed to the destination machine which is in another logical network in this example the managed switching element functions as a logical switch that is in the logical network to which the source machine belongs a logical switch that is in the logical network to which the destination machine belongs and a logical router that routes packets between these two logical switches. Based on the result of performing logical processing pipeline the managed switching element forwards the packet to the managed switching element through which the destination machine receives the packet.

The L2 processing in some embodiments includes four stages for processing a packet through the logical switch not shown in . In the stage ingress context mapping is performed on the packet to determine the logical context of the packet. In some embodiments the stage is performed when the logical switch receives the packet e.g. the packet is initially received by the managed switching element .

In some embodiments the second stage is defined for the logical switch . In some such embodiments the stage operates on the packet s logical context to determine ingress access control of the packet with respect to the logical switch. For example an ingress ACL is applied to the packet to control the packet s access to the logical switch when the logical switch receives the packet. Based on the ingress ACL defined for the logical switch the packet may be further processed e.g. by the stage or the packet may be dropped for example.

In the third stage of the L2 processing an L2 forwarding is performed on the packet in the context of the logical switch. In some embodiments the third stage operates on the packet s logical context to process and forward the packet with respect to the logical switch . For instance some embodiments define an L2 forwarding table or L2 forwarding entries for processing the packet at layer 2.

The fourth stage of some embodiments is defined for the logical switch . The fourth stage of some such embodiments operates on the packet s logical context to determine egress access control of the packet with respect to the logical switch. For instance an egress ACL may be applied to the packet to control the packet s access out of the logical switch after logical forwarding has been performed on the packet. Based on the egress ACL defined for the logical switch the packet may be further processed e.g. sent out of a logical port of the logical switch or sent to a dispatch port for further processing or the packet may be dropped for example.

When the packet s destination is in another logical network i.e. when the packet s destination logical network is different than the logical network whose traffic is processed by the logical switch the logical switch sends the packet to the logical router which then performs the L3 processing at stage in order to route the packet to the destination logical network. The L3 processing includes six stages for processing a packet through the logical router not shown in that is implemented by the managed switching element not shown in . As mentioned above L3 processing involves performing a set of logical routing lookups for determining where to route the packet through a layer 3 network.

The first stage performs a logical ingress ACL lookup for determining access control when the logical router receives the packet i.e. when the managed switching element which implements the logical router receives the packet . In some embodiments the stage operates on the packet s logical context to determine ingress access control of the packet with respect to the logical router . The next stage performs DNAT to revert the destination address of the packet back to the real address of the destination machine that is hidden from the source machine of the packet. This stage is performed when DNAT is enabled.

The next stage performs a logical L3 routing for determining one or more logical ports to send the packet through the layer 3 network based on the L3 addresses e.g. destination IP address of the packet forwarding tables containing L3 flow entries and the packet s logical context.

The fourth stage of some embodiments performs SNAT on the packet. For instance the managed switching element replaces the source IP address of the packet with a different IP address in order to hide the source IP address when the SNAT is enabled. Also as will be described further below the managed switching element may use a NAT daemon to receive flow entries for translating network addresses. A NAT daemon will be described further below by reference to .

The fifth stage performs logical egress ACL lookups for determining access control before the logical router routes the packet out of the logical router through the port determined in the stage . The egress ACL lookups are performed based on the L3 addresses e.g. source and destination IP addresses of the packet. In some embodiments the stage operates on the packet s logical context to determine egress access control of the packet with respect to the logical router .

The sixth stage performs address resolution in order to translate the destination L3 address e.g. a destination IP address into a destination L2 address e.g. a destination MAC address . In some embodiments the managed switching element uses a standard address resolution e.g. by sending out ARP requests or looking up ARP cache to find the destination L2 address that corresponds to the destination IP address. Also as will be described further below the managed switching element of some embodiments may use an L3 daemon to receive flow entries for resolving L3 addresses into L2 addresses. An L3 daemon will be described further below by reference to .

When the logical router is not coupled to the destination logical network the logical switch sends the packet to another logical router network towards the destination logical network. A portion of the logical processing that corresponds to the operation of the other logical router would also be implemented in the managed switching element . When the logical router is coupled to the destination logical network the logical switch routes the packet to the destination logical network i.e. the logical switch that forwards the packet for the destination logical network .

The L2 processing in some embodiments includes five stages for processing the packet through the logical switch not shown in . In some embodiments the first stage is defined for the logical switch . In some such embodiments the stage operates on the packet s logical context to determine ingress access control of the packet with respect to the logical switch . For example an ingress ACL is applied to the packet to control the packet s access to the logical switch when the logical switch receives the packet from the logical router . Based on the ingress ACL defined for the logical switch the packet may be further processed e.g. by the stage or the packet may be dropped for example.

In the second stage of the L2 processing pipeline an L2 forwarding is performed on the packet in the context of the logical switch. In some embodiments the third stage operates on the packet s logical context to process and forward the packet with respect to the logical switch . For instance some embodiments define an L2 forwarding table or L2 forwarding entries for processing the packet at layer 2.

The third stage of some embodiments is defined for the logical switch . The third stage of some such embodiments operates on the packet s logical context to determine egress access control of the packet with respect to the logical switch. For instance an egress ACL may be applied to the packet to control the packet s access out of the logical switch after logical forwarding has been performed on the packet. Based on the egress ACL defined for the logical switch the packet may be further processed e.g. sent out of a logical port of the logical switch or sent to a dispatch port for further processing or the packet may be dropped for example.

In the fourth stage egress context mapping is performed to identify a physical result that corresponds to the result of the logical forwarding of the packet. For example the logical processing of the packet may specify that the packet is to be sent out of one or more logical ports e.g. a logical egress port of the logical switch . As such the egress context mapping operation identifies a physical port s of one or more of the managed switching elements including the managed switching element that corresponds to the particular logical port of the logical switch.

The fifth stage of the L2 processing performs a physical mapping based on the egress context mapping performed at the fourth stage . In some embodiments the physical mapping determines operations for forwarding the packet to the physical port that was determined in the fourth stage . For example the physical mapping of some embodiments determines one or more queues not shown associated with one or more ports of the set of ports not shown of the managed switching element through which to send the packet in order for the packet to reach the physical port s determined in the fourth stage . This way the managed switching elements can route the packet along the correct path in the network for the packet to reach the determined physical port s . Also some embodiments remove the logical context after the fifth stage is completed in order to return the packet to its original state before the logical processing pipeline was performed on the packet.

In this example the logical switch forwards data packets between the logical router VM 1 and VM 2. The logical switch forwards data packets between the logical router VM 3 and VM 4. As mentioned above the logical router routes data packets between the logical switches and and other logical routers and switches not shown . The logical switches and and the logical router are logically coupled through logical ports not shown and exchange data packets through the logical ports. These logical ports are mapped or attached to physical ports of the managed switching elements and .

In some embodiments a logical router is implemented in each managed switching element in the managed network. When the managed switching element receives a packet from a machine that is coupled to the managed switching element the managed switching element performs the logical routing. In other words a managed switching element of these embodiments that is a first hop switching element with respect to a packet performs the L3 processing .

In this example the managed switching elements and are software switching elements running in hosts and respectively. The managed switching elements and have flow entries which implement the logical switches and to forward and route the packets that the managed switching element and receive from VMs 1 4. The flow entries also implement the logical router . Using these flow entries the managed switching elements and can forward and route packets between network elements in the network that are coupled to the managed switching elements and . As shown the managed switching elements and each have three ports e.g. VIFs through which to exchange data packets with the network elements that are coupled to the managed switching elements and . In some cases the data packets in these embodiments will travel through a tunnel that is established between the managed switching elements and e.g. the tunnel that terminates at port 3 of the managed switching element and port 3 of the managed switching element .

In this example each of the hosts and includes a managed switching element and several VMs as shown. The VMs 1 4 are virtual machines that are each assigned a set of network addresses e.g. a MAC address for L2 an IP address for network L3 etc. and can send and receive network data to and from other network elements. The VMs are managed by hypervisors not shown running on the hosts and .

Several example data exchanges through the network architecture will now be described. When VM 1 that is coupled to the logical switch sends a packet to VM 2 that is also coupled to the same logical switch the packet is first sent to the managed switching element . The managed switching element then performs the L2 processing on the packet. The result of L2 processing would indicate that the packet should be sent to the managed switching element over the tunnel established between the managed switching elements and and get to VM 2 through port 4 of the managed switching element . Because VMs 1 and 2 are in the same logical network the managed switching element does not perform the L3 processing and the L2 processing .

When VM 1 that is coupled to the logical switch sends a packet to VM 3 that is coupled to the logical switch the packet is first sent to the managed switching element . The managed switching element performs the L2 processing on the packet. However because the packet is sent from one logical network to another i.e. the logical L3 destination address of the packet is for another logical network the L3 processing needs to be performed. The managed switching element also performs the L2 processing . That is the managed switching element as the first hop switching element that receives the packet performs the entire logical processing pipeline on the packet. The result of performing the logical processing pipeline would indicate that the packet should be sent to VM 3 through port 5 of the managed switching element . Thus the packet did not have to go to another managed switching element although the packet did go through two logical switches and a logical router.

When VM 1 that is coupled to the logical switch sends a packet to VM 4 that is coupled to the logical switch the packet is first sent to the managed switching element . The managed switching element as the first hop switching element for the packet performs the entire logical processing pipeline on the packet. The result of performing the logical processing pipeline on this packet would indicate that the packet should be sent to the managed switching element over the tunnel established between the managed switching elements and and get to VM 4 through port 5 of the managed switching element .

The network architecture is similar to the network architecture except that the network architecture additionally includes the managed switching element . The managed switching element of some embodiments is a second level managed switching element that functions as a pool node.

In some embodiments tunnels are established by the network control system not shown to facilitate communication between the network elements. For instance the managed switching element in this example is coupled to the managed switching element which runs in the host through a tunnel that terminates at port 1 of the managed switching element as shown. Similarly the managed switching element is coupled to the managed switching element through a tunnel that terminates at port 2 of the managed switching elements . In contrast to the example architecture illustrated in above no tunnel is established between the managed switching elements and .

The logical router and the logical switches and are implemented in the managed switching element and the second level managed switching element is involved in the data packet exchange. That is the managed switching elements and exchange packets through the managed switching element .

When VM 1 that is coupled to the logical switch sends a packet to VM 2 that is also coupled to the same logical switch the packet is first sent to the managed switching element through port 4 of the managed switching element because a logical port 1 of the logical switch through which the packet goes into the logical switch is attached or mapped to port 4 of the managed switching element .

The managed switching element then performs the L2 processing on the packet. Specifically the managed switching element first performs a logical context look up to determine the logical context of the packet based on the information included in the header fields of the packet. In this example the source MAC address of the packet is a MAC address of VM 1 and the source IP address of the packet is an IP address of VM 1. The destination MAC address of the packet is a MAC address of VM 2 and destination IP address of the packet is an IP address of VM 2. In this example the logical context specifies that logical switch is the logical switch that is to forward the packet and that logical port 1 of the logical switch is the port through which the packet was received. The logical context also specifies that port 2 of the logical switch is the port through which to send the packet out to VM 2 because port 2 is associated with the MAC address of VM 2.

The managed switching element then performs logical forwarding lookups based on the determined logical context of the packet. The managed switching element determines access control for the packet. For instance the managed switching element determines that the packet does not have network addresses e.g. source destination MAC IP addresses etc. that will cause the logical switch to reject the packet that came through port 1 of the logical switch . The managed switching element also identifies from the logical context that port 2 of the logical switch is the port to send out the packet. Furthermore the managed switching element determines access control for the packet with respect to port 2 of the logical switch . For instance the managed switching element determines that the packet does not have network addresses that will cause the logical switch not to send the packet through the port 2 of the logical switch .

The managed switching element then performs a mapping lookup to determine a physical port to which the logical port 2 of the logical switch is mapped. In this example the managed switching element determines that logical port 2 of the logical switch is mapped to port 4 of the managed switching element . The managed switching element then performs a physical lookup to determine operations for forwarding the packet to the physical port. In this example the managed switching element determines that the packet should be sent to the managed switching element over the tunnel established between the managed switching elements and and get to VM 2 through port 4 of the managed switching element . Because VMs 1 and 2 are in the same logical network the managed switching element does not perform an L3 processing. The managed switching element does not perform any logical processing on the packet but just forwards the packet to VM 2 through port 4 of the managed switching element .

When VM 1 that is coupled to the logical switch sends a packet to VM 3 that is coupled to the logical switch i.e. when VMs 1 and 3 are in different logical networks the packet is first sent to the managed switching element through port 4 of the managed switching element . The managed switching element performs the L2 processing on the packet. Specifically the managed switching element first performs a logical context look up to determine the logical context of the packet based on the information included in the header fields of the packet. In this example the source MAC address of the packet is a MAC address of VM 1 and the source IP address of the packet is an IP address of VM 1. Because the packet is sent from VM 1 to VM 3 that is in a different logical network the packet has a MAC address associated with port X as the destination MAC address i.e. 01 01 01 01 01 01 in this example . The destination IP address of the packet is an IP address of VM 3 e.g. 1.1.2.10 . In this example the logical context specifies that logical switch is the logical switch that is to forward the packet and that logical port 1 of the logical switch is the port through which the packet was received. The logical context also specifies that port X of the logical switch is the port through which to send the packet out to the logical router because port X is associated with the MAC address of port 1 of the logical router .

The managed switching element then determines access control for the packet. For instance the managed switching element determines that the packet does not have network addresses e.g. source destination MAC IP addresses etc. that will cause the logical switch to reject the packet that came through port 1 of the logical switch . The managed switching element also identifies from the logical context that port X of the logical switch is the port to send out the packet. Furthermore the managed switching element determines access control for the packet with respect to port X. For instance the managed switching element determines that the packet does not have network addresses that will cause the logical switch not to send the packet through the port X.

The managed switching element then performs the L3 processing on the packet because the packet s destination IP address 1.1.2.10 is for another logical network i.e. when the packet s destination logical network is different than the logical network whose traffic is processed by the logical switch . The managed switching element determines access control for the packet at L3. For instance the managed switching element determines that the packet does not have network addresses that will cause the logical router to reject the packet that came through logical port 1 of the logical router . The managed switching element also looks up the L3 flow entries and determines that the packet is to be sent to the logical port 2 of the logical router because the destination IP address of the packet 1.1.2.10 belongs to the subnet address of 1.1.2.1 24 that is associated with the logical port 2 of the logical router . Furthermore the managed switching element determines access control for the packet with respect to the logical port 2 of the logical router . For instance the managed switching element determines that the packet does not have network addresses that will cause the logical switch not to send the packet through the logical port 2.

The managed switching element modifies the logical context of the packet or the packet itself while performing the L3 processing . For instance the managed switching element modifies the logical source MAC address of the packet to be the MAC address of the logical port 2 of the logical router i.e. 01 01 01 01 01 02 in this example . The managed switching element also modifies the destination MAC address of the packet to be a MAC address of VM 3.

The managed switching element then performs the L2 processing . Specifically the managed switching element determines access control for the packet. For instance the managed switching element determines that the packet does not have network addresses e.g. source destination MAC IP addresses etc. that will cause the logical switch to reject the packet that came through port Y of the logical switch . The managed switching element then determines that port 1 of the logical switch is the port through which to send the packet out to the destination VM 3. Furthermore the managed switching element determines access control for the packet with respect to port 1 of the logical switch . For instance the managed switching element determines that the packet does not have network addresses that will cause the logical switch not to send the packet through the port 1 of the logical switch .

The managed switching element then performs a mapping lookup to determine a physical port to which the logical port 1 of the logical switch is mapped. In this example the managed switching element determines that logical port 1 of the logical switch is mapped to port 5 of the managed switching element . The managed switching element then performs a physical lookup to determine operations for forwarding the packet to the physical port. In this example the managed switching element determines that the packet should be sent to VM 3 through port 5 of the managed switching element . The managed switching element in this example removes the logical context from the packet before sending out the packet to VM 3. Thus the packet did not have to go to another managed switching element although the packet did go through two logical switches and a logical router.

When VM 1 that is coupled to the logical switch sends a packet to VM 4 that is coupled to the logical switch the packet is sent to VM 4 in a similar manner in which the packet sent from VM 1 to VM 3 is sent to VM 3 except that the packet heading to VM 4 is sent from the managed switching element to the managed switching element over the tunnel established between the managed switching elements and and gets to VM 4 through port 5 of the managed switching element .

As shown in the bottom half of the managed switching element includes L2 entries and and L3 entries . These entries are flow entries that a controller cluster not shown supplies to the managed switching element . Although these entries are depicted as three separate tables the tables do not necessarily have to be separate tables. That is a single table may include all these flow entries.

When VM 1 that is coupled to the logical switch sends a packet to VM 4 that is coupled to the logical switch the packet is first sent to the managed switching element through port 4 of the managed switching element . The managed switching element performs an L2 processing on the packet based on the forwarding tables of the managed switching element . In this example the packet has a destination IP address of 1.1.2.10 which is the IP address of VM 4. The packet s source IP address is 1.1.1.10. The packet also has VM 1 s MAC address as a source MAC address and the MAC address of the logical port 1 e.g. 01 01 01 01 01 01 of the logical router as a destination MAC address.

The managed switching element identifies a record indicated by an encircled 1 referred to as record 1 in the forwarding tables that implements the context mapping of the stage . The record 1 identifies the packet s logical context based on the inport which is the port 4 through which the packet is received from VM 1. In addition the record 1 specifies that the managed switching element store the logical context of the packet in a set of fields e.g. a VLAN id field of the packet s header. The record 1 also specifies the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . A dispatch port is described in U.S. patent application Ser. No. 13 177 535.

Based on the logical context and or other fields stored in the packet s header the managed switching element identifies a record indicated by an encircled 2 referred to as record 2 in the forwarding tables that implements the ingress ACL of the stage . In this example the record 2 allows the packet to be further processed i.e. the packet can get through the ingress port of the logical switch and thus specifies the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . In addition the record 2 specifies that the managed switching element store the logical context i.e. the packet has been processed by the second stage of the processing pipeline of the packet in the set of fields of the packet s header.

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 3 referred to as record 3 in the forwarding tables that implements the logical L2 forwarding of the stage . The record 3 specifies that a packet with the MAC address of the logical port 1 of the logical router as a destination MAC address is to be sent to the logical port X of the logical switch .

The record 3 also specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . Also the record 3 specifies that the managed switching element store the logical context i.e. the packet has been processed by the third stage of the processing pipeline in the set of fields of the packet s header.

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 4 referred to as record 4 in the forwarding tables that implements the egress ACL of the stage . In this example the record 4 allows the packet to be further processed e.g. the packet can get out of the logical switch through port X of the logical switch and thus specifies the packet be further processed by the flow entries of the managed switching element e.g. by sending the packet to a dispatch port . In addition the record 4 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline of the packet in the set of fields of the packet s header. It is to be noted that all records specify that a managed switching element update the logical context store in the set of fields whenever the managed switching element performs some portion of logical processing based on a record. 

The managed switching element continues processing the packet based on the flow entries. The managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 5 referred to as record 5 in the L3 entries that implements L3 ingress ACL by specifying that the managed switching element should accept the packet through the logical port 1 of the logical router based on the information in the header of the packet .

The managed switching element then identifies a flow entry indicated by an encircled 6 referred to as record 6 in the L3 entries that implements L3 routing by specifying that the packet with its destination IP address e.g. 1.1.2.10 should exit out of port 2 of the logical router . Also the record 6 or another record in the routing table not shown indicates that the source MAC address for the packet is to be rewritten to the MAC address of port 2 of the logical router i.e. 01 01 01 01 01 02 .

The managed switching element then identifies a flow entry indicated by an encircled 7 referred to as record 7 in the L3 entries that implements L3 egress ACL by specifying that the managed switching element allow the packet to exit out through port 2 of the logical router based on the information e.g. source IP address in the header of the packet .

Based on the logical context and or other fields stored in the packet s header the managed switching element identifies a record indicated by an encircled 8 referred to as record 8 in the L2 entries that implements the ingress ACL of the stage . In this example the record 8 specifies the packet be further processed by the managed switching element e.g. by sending the packet to a dispatch port . In addition the record 8 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline of the packet in the set of fields of the packet s header.

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 9 referred to as record 9 in the L2 entries that implements the logical L2 forwarding of the stage . The record 9 specifies that a packet with the MAC address of VM 4 as the destination MAC address should be forwarded through a logical port not shown of the logical switch that is connected to VM 4.

The record 9 also specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . Also the record 9 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline in the set of fields of the packet s header.

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 10 referred to as record 10 in the forwarding tables that implements the egress ACL of the stage . In this example the record 10 allows the packet to exit through a logical port not shown that connects to VM 4 and thus specifies the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . In addition the record 10 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline of the packet in the set of fields of the packet s header.

Based on the logical context and or other fields stored in the packet s header the managed switching element identifies a record indicated by an encircled 11 referred to as record 11 in the L2 entries that implements the context mapping of the stage . In this example the record 11 identifies port 5 of the managed switching element to which VM 4 is coupled as the port that corresponds to the logical port determined at stage of the logical switch to which the packet is to be forwarded. The record 11 additionally specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Based on the logical context and or other fields stored in the packet s header the managed switching element then identifies a record indicated by an encircled 12 referred to as record 12 in the L2 entries that implements the physical mapping of the stage . The record 12 specifies port 3 of the managed switching element as a port through which the packet is to be sent in order for the packet to reach the managed switching element . In this case the managed switching element is to send the packet out of port 3 of managed switching element that is coupled to the managed switching element .

As shown in the managed switching element includes a forwarding table that includes rules e.g. flow entries for processing and routing the packet . When the managed switching element receives the packet from the managed switching element the managed switching element begins processing the packet based on the forwarding tables of the managed switching element . The managed switching element identifies a record indicated by an encircled 1 referred to as record 1 in the forwarding tables that implements the context mapping. The record 1 identifies the packet s logical context based on the logical context that is stored in the packet s header. The logical context specifies that the packet has been processed by the entire logical processing which were performed by the managed switching element . As such the record 4 specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 2 referred to as record 2 in the forwarding tables that implements the physical mapping. The record 2 specifies the port 5 of the managed switching element through which the packet is to be sent in order for the packet to reach VM 4. In this case the managed switching element is to send the packet out of port 5 of managed switching element that is coupled to VM 4. In some embodiments the managed switching element removes the logical context from the packet before sending the packet to VM 4.

The flow entries and are flow entries that each has a qualifier and an action. The text illustrated as flow entries and may not be the actual format. Rather the text is just a conceptual illustration of a qualifier and an action pair. In some embodiments flow entries have priorities and a managed switching element takes the action of the flow entry with the highest priority when qualifiers for more than one flow entry are satisfied.

The host in some embodiments is a machine operated by an operating system e.g. Windows and Linux that is capable of running a set of software applications. The managed switching element of some embodiments is a software switching element e.g. Open vSwitch that executes in the host . As mentioned above a controller cluster not shown configures a managed switching element by supplying flow entries that specify the functionality of the managed switching element. The managed switching element of some embodiments does not itself generate flow entries.

The managed switching element of some embodiments runs all or part of the logical processing pipeline described above. In particular the managed switching element is a managed switching element e.g. the managed switching elements or that performs the L3 processing to route packets received from the machines if necessary based on flow entries in the forwarding table . In some embodiments the managed switching element is an edge switching element that receives a packet from a machine not shown that is coupled to the managed switching element. In some such embodiments one or more virtual machines not shown are running in the host and are coupled to the managed switching elements . In other embodiments the managed switching element is a second level managed switching element.

When the managed switching element is configured to perform network address translation NAT the managed switching element of some embodiments uses the NAT daemon for performing NAT on packets. In some embodiments the managed switching element does not maintain a lookup table for finding an address to which to translate from a given address. Instead the managed switching element asks the NAT daemon for addresses.

The NAT daemon of some embodiments is a software application running on the host . The NAT daemon maintains the table which includes pairings of addresses where each pair includes two addresses to be translated into each other. When the managed switching element asks for an address to which to translate from a given address the NAT daemon looks up the table to find the address into which the given address should be translated.

The managed switching element and the NAT daemon of different embodiments use different techniques to ask for and supply addresses. For instance the managed switching element of some embodiments sends a packet which has an original address but does not have a translated address to the NAT daemon. The NAT daemon of these embodiments translates the original address into a translated address. The NAT daemon sends the packet back to the managed switching element which will perform logical forwarding and or routing to send the packet towards the destination machine. In some embodiments the managed switching element initially sends metadata along with the packet that contains an original address to resolve to the NAT daemon . This metadata includes information e.g. register values logical pipeline state etc. that the managed switching element uses to resume performing the logical processing pipeline when the managed switching element receives the packet back from the NAT daemon .

In other embodiments the managed switching element of some embodiments requests addresses by sending a flow template which is a flow entry that does not have actual values for the addresses to the NAT daemon . The NAT daemon finds out the addresses to fill in the flow template by looking up the table . The NAT daemon then sends the flow template that is filled in with actual addresses back to the managed switching element by putting the filled in flow template into the forwarding table . In some embodiments the NAT daemon assigns a priority value to the filled in flow template that is higher than the priority value of the flow template that is not filled in. Moreover when the NAT daemon fails to find a translated address the NAT daemon would specify in the flow template to drop the packet.

An example operation of the managed switching element and the NAT daemon will now be described in terms of three different stages encircled 1 3 . In this example the managed switching element is a managed edge switching element that receives a packet to forward and route from a machine not shown . The managed switching element receives a packet and performs the L3 processing based on the flow entries in the forwarding table .

While performing the L3 processing on the packet the managed switching element at stage 1 identifies the flow entry and performs the action specified in the flow entry . As shown the flow entry indicates that a flow template having an IP address 1.1.1.10 to be translated to X should be sent to the NAT daemon . In this example the flow entry has a priority value of N which is a number in some embodiments.

At stage 2 the NAT daemon receives the flow template and finds out that 1.1.1.10 is to be translated into 2.1.1.10 by looking up the NAT table . The NAT daemon fills out the flow template and inserts the filled in template now the flow entry into the forwarding table . In this example the NAT daemon assigns a priority of N 1 to the filled in template.

At stage 3 the managed switching element uses the flow entry to change the address for the packet. Also for the packets that the managed switching element subsequently processes the managed switching element uses flow entry over the flow entry when a packet has the source IP address of 1.1.1.10.

In some embodiments the NAT daemon and the managed switching element run in a same virtual machine that is running on the host or in different virtual machines running on the host . The NAT daemon and the managed switching element may also run in separate hosts.

The process begins by determining at whether the packet needs network address translation NAT . In some embodiments the process determines whether the packet needs NAT based on flow entry. The flow entry of which the qualifier matches the information stored in the packet s header or logical context specifies that the packet needs NAT. As mentioned above NAT could be SNAT or DNAT. The flow entry would also specify which NAT is to be performed on the packet.

When the process determines at that the packet does not need NAT the process ends. Otherwise the process determines at whether the process needs to request for an address into which to translate a packet s address e.g. source IP address from a NAT daemon. In some embodiments the process determines whether the process needs to ask the NAT daemon based on the flow entry. For instance the flow entry may specify that the address into which to translate the packet s address should be obtained by requesting for the address from the NAT daemon. In some embodiments the process determines that the NAT daemon should provide the translated address when the flow entry is a flow template that has an empty field for the translated address or some other value in the field for indicating the translated address should be obtained from the NAT daemon.

When the process determines at that the process does not need to request for an address from the NAT daemon the process obtains at the translated address from the flow entry. For instance the flow entry would provide the translated address. The process then proceeds to which will be described further below. When the process determines at that the process needs to request for an address from the NAT daemon the process at requests for and obtains the translated address from the NAT daemon. In some embodiments the process requests for the translated address by sending a flow template to the NAT daemon. The NAT daemon would fill the flow template with the translated address and will place that filled in flow template in the forwarding table not shown that the process uses.

Next the process modifies at the packet with the translated address. In some embodiments the process modifies an address field in the header of the packet. Alternatively or conjunctively the process modifies the logical context to replace the packet s address with the translated address. The process then ends.

It is to be noted that the MAC addresses IP addresses and other network addresses used above and below in this application are examples for illustrative purpose and may not have the values in the allowable ranges unless specified otherwise.

Logical networks interfacing external networks need to interact with a next hop router. The virtualization applications of different embodiments use different models to interface a logical L3 network with external networks through a next hop router.

First in a fixed attachment model the physical infrastructure interacts with a set of managed integration elements that will receive all the ingress traffic for a given IP prefix and will send all the egress traffic back to the physical network. In this model logical abstraction can be a single logical uplink port for the logical L3 router per a given set of managed integration elements. In some embodiments there could be more than a single integration cluster. The logical control plane that is provided by the control application is responsible for routing outbound egress traffic towards the uplink s . In some embodiments examples of managed integration elements include second level managed switching elements that function as extenders which are described in U.S. patent application Ser. No. 13 177 535. The examples of managed integration elements also include the managed switching element described above by reference to .

Second in a distributed attachment model the virtualization application distributes the attachment throughout managed edge switching elements that it connects. To do so the managed edge switching elements have to integrate to the physical routing infrastructure. In other words each managed edge switching element has to be able to communicate with the physical routing infrastructure outside of the group of managed switching elements. In some embodiments these switching elements use the IGP protocol or other routing protocol to communicate with the physical switching elements e.g. the physical routers that send packets into the logical network implemented by the managed switching elements and receive packets from the logical network. Using this protocol the managed edge switching elements of some embodiments can advertise host routes 32 to attract direct ingress traffic to its proper location. While in some embodiments there is no centralized traffic hotspot as the ingress and egress traffic is completely distributed over the managed switching elements the logical abstraction is still a single logical uplink port for the logical L3 router and the logical control plane is responsible for routing traffic to the uplink. Nothing prevents having more than a single uplink port exposed for the logical control plane if that is beneficial for the control plane. However the number of uplink ports does not have to match with the number of attachment points in this model.

Third in a control plane driven model the logical control plane is responsible for integrating with the external network. Control plane is exposed with one to one routing integration for every attachment point in the physical network there s a logical port. Logical control plane has the responsibility to peer with next hop routers at the routing protocol level.

The three models all hit different design trade offs fixed attachment model implies non optimal physical traffic routes but require less integration with the physical infrastructure. Of the distributed models the fully distributed model scales best in some embodiments as the logical control plane is not responsible for all the peering traffic which in the extreme could be thousands of peering sessions. However the control plane driven model gives the maximal control for the logical control plane. The maximal control requires policy routing though as the egress port has to depend on the ingress port if optimal physical routes are desired.

Stateful packet operations place NAT on a logical L3 datapath for the routed traffic. In the logical pipeline network address translation is done in an extra NAT stage before or after the actual standard L3 pipeline. In other words network address translation hits the packet before or after the routing. In some embodiments NAT configuration is done via flow templates that create the actual address translation entries. Flow templates will be further described below.

Placing the NAT functionality is one feature that deviates from the approach of performing all or most of the logical packet processing in first hop. The basic model of executing most or all of the operations at the first hop places the processing of packets flowing in opposite directions at different first hop switching elements in some embodiments for a given transport level flow the packets in one direction would be sent through the logical pipeline at one end and the packets in the reverse direction would be sent through the pipeline at the other end. Unfortunately the per flow NAT state can be fairly rich especially if NAT supports higher level application protocols and the state has to be shared between the directions for a given transport flow.

Hence some embodiments let the first hop switching elements of the logical port receive the opening packet of the transport flow to execute the logical pipelines to both directions. For example if VM A opens a TCP connection to VM B then the edge switching element connected to the hypervisor which may run on the same machine as the hypervisor of VM A becomes responsible for sending the packets through the logical pipelines to both directions. This allows for purely distributed NAT functionality as well as having multiple NATs in the logical network topology. The first hop switching element will execute all the necessary NAT translations regardless how many there are and the network address translation just becomes an extra step in the LDPS pipelines the packet traverses within that switching element .

However placing the feeding of the packets sent in the reverse direction through the logical pipelines requires additional measures otherwise the first hop switching element for the reverse packets will execute the processing without having the NAT state locally available . For this purpose some embodiments allow the first packet sent from the source edge switching element of VM A above to the destination edge switching element of VM B above to establish a special hint state that makes the destination switching element send the reverse packets of that transport flow directly to the source switching element without processing. The source switching element will then execute the pipelines in the reverse direction and reverse the NAT operations using the local NAT state for the reverse packets. Some embodiments use the flow templates which are described below to establish this reverse hint state at the destination switching element so that the controller does not need to be involved per flow operations.

The next two figures illustrate placing NAT functionality and the hint state. conceptually illustrates that a first hop switching element of some embodiments performs the entire logical processing pipeline including the NAT operation . is identical with except that the logical processing pipeline includes the NAT operation depicted in the L3 processing to indicate that the NAT operation is performed.

A managed switching element of some embodiments that implements a logical router performs a NAT operation on a packet after the packet is routed by the logical router. For instance when VM 1 that is coupled to the logical switch sends a packet to VM 4 that is coupled to the logical switch the managed switching element translates the source IP address e.g. 1.1.1.10 of the packet into a different IP address e.g. 3.1.1.10 before sending the packet out to the managed switching element . The managed switching element performs the NAT operation based on a set of NAT rules e.g. flow entries configured in the managed switching element by the controller cluster not shown that manages the managed switching element .

The packet that VM 4 receives has the translated IP address 3.1.1.10 as the packet s source IP address. A return packet from VM 4 to VM 1 will have this translated address as the packet s destination IP address. Thus the translated IP address has to be translated back to VM l s IP address in order for this packet to reach VM 1. However the managed switching element of some embodiments would not perform the NAT operation to recover VM1 s IP address for the returning packet because the NAT rules for performing NAT operations are only in the managed switching element and are not in the managed switching element . In this manner the NAT rules and the state do not have to be shared by all potential managed edge switching elements.

The rule in some embodiments is a flow entry in the forwarding table that is configured by a controller cluster not shown that manages the managed network switching element . The rule specifies or hints that when the managed switching element receives a packet originating from the managed switching element the managed switching element should not perform a logical processing pipeline on the returning packets to the managed switching element .

When the managed switching element receives from the managed switching element a packet on which the managed switching element has performed a NAT operation the managed switching element finds the rule based on the information included in the packet s header e.g. logical context . Also the managed switching element in some embodiments modifies one or more other flow entries to indicate that no logical processing pipeline should be performed on packets from the destination machine e.g. VM 4 of the received packet that are headed to the source machine e.g. VM 1 .

The managed switching element then forwards this packet to the destination machine e.g. VM 4. When the managed switching element receives a returning packet from VM 4 that is headed to VM 1 the managed switching element will not perform a logical processing pipeline on this packet. That is the managed switching element will not perform logical forwarding at L2 or logical routing at L3. The managed switching element will simply indicate in the logical context for this packet that no logical processing has been performed on the packet.

When the managed switching element receives this packet from the managed switching element the managed switching element performs the logical processing pipeline . Specifically the managed switching element first performs a logical context look up to determine the logical context of the packet based on the information included in the header fields of the packet. In this example the source MAC address of the packet is a MAC address of VM 4 and the source IP address of the packet is an IP address of VM 4. Because the packet is sent from VM 4 to VM 1 that is in a different logical network the packet has a MAC address associated with port Y of the logical switch as the destination MAC address i.e. 01 01 01 01 01 02 in this example . The destination IP address of the packet is the NAT ed IP address of VM 1 i.e. 3.1.1.10 .

The managed switching element then determines access control for the packet with respect to the logical switch . For instance the managed switching element determines that the packet does not have network addresses e.g. source destination MAC IP addresses etc. that will cause the logical switch to reject the packet that came through port 2 of the logical switch . The managed switching element also identifies from the logical context that port Y of the logical switch is the port to send out the packet. Furthermore the managed switching element determines access control for the packet with respect to port Y. For instance the managed switching element determines that the packet does not have network addresses that will cause the logical switch not to send the packet through the port Y.

Next the managed switching element performs the NAT operation on the packet to translate the destination IP address back to the IP address of VM 1. That is the managed switching element in this example replaces 3.1.1.10 with 1.1.1.10 based on the NAT rules. The managed switching element then performs an L3 processing on the packet because the packet s destination IP address now 1.1.1.10 is for another logical network. The managed switching element determines ingress access control for the packet at L3 with respect to port 2 of the logical router . The managed switching element also looks up the flow entries and determines that the packet is to be sent to the logical port 1 of the logical router because the destination IP address of the packet 1.1.1.10 belongs to the subnet address of 1.1.1.1 24 that is associated with the logical port 1 of the logical router . Furthermore the managed switching element determines egress access control for the packet with respect to the logical port 1 of the logical router . The managed switching element also modifies the destination MAC address of the packet to be a MAC address of VM 1.

The managed switching element then performs the L2 processing . In this example the source MAC address of the packet is now a MAC address of logical port 1 of the logical router and the source IP address of the packet is still the IP address of VM 4. The destination IP address of the packet is the IP address of VM 1 i.e. 1.1.1.10 . In this example the logical context specifies that logical switch is the logical switch that is to forward the packet and that logical port X of the logical switch is the port through which the packet was received. The logical context also specifies that port 1 of the logical switch is the port through which to send the packet out to the destination VM 1 because port 1 is associated with the MAC address of VM 1.

The managed switching element then performs logical forwarding lookups based on the logical context of the packet including determining ingress and egress access control with respect to port X and port 1 of the logical switch respectively. The managed switching element performs a mapping lookup to determine a physical port to which the logical port 1 of the logical switch is mapped. In this example the managed switching element determines that logical port 1 of the logical switch is mapped to port 4 of the managed switching element . The managed switching element then performs a physical lookup to determine operations for forwarding the packet to the physical port. In this example the managed switching element determines that the packet should be sent to VM 1 through port 4 of the managed switching element .

The process begins by receiving at a packet from a source machine. The process then determines at whether the packet is headed to a destination machine whose address is NAT ed. In some embodiments the process determines whether the packet is headed to such destination machine by looking up flow entries that match the information included in the header of the packet e.g. destination IP address . One or more flow entries specify that no logical processing e.g. logical forwarding at L2 or logical routing at L3 should be performed on this packet when the packet is addressed to a destination machine whose address is NAT ed. Other flow entries specify that logical processing should be performed when the packet is addressed to a destination machine whose address is not NAT ed.

When the process determines at that the packet is headed to a destination machine whose address is NAT ed the process proceeds to which will be described further below. When the process determines at that the packet is headed to a destination machine whose address is not NAT ed the process performs logical processing on the packet e.g. logical forwarding at L2 and or logical routing at L3 .

The process then sends at the packet to the next hop managed switching element in route to the destination machine. The process then ends.

Note above the controllers are not involved in the per packet operations. The logical control plane only provisions the FIB rules identifying what should be network address translated. All per flow state is established by the datapath Open vSwitch .

The embodiments described above utilize Source NAT ing. However some embodiments use Destination NAT ing DNAT ing along the same lines. In the case of DNAT ing all the processing can be done at the source managed edge switching element.

Moreover in the case of placing the NAT functionality between the external and logical network the operations are no different from the one described above. In this case for the flows incoming from the external network the NAT state will be held at the extender which in this case would be the first hop managed edge switching element for both directions. On the other hand for transport flows initiated towards the external network the state will be held at the managed edge switching element attached to the originating host VM.

With this purely distributed approach for the network address translation VM mobility support requires migrating the established NAT state with the VM to the new hypervisor. Without migrating the NAT state the transport connections will break. For such conditions some embodiments are designed to expect the NAT to respond with TCP reset to packets sent to closed non existing TCP flows. More advanced implementations will integrate with the VM management system facilitating the migration of the NAT state together with the VM in this case the transport connections do not have to break.

As shown the host in this example is a source host from which a VM is migrating to the host . In the host a NAT daemon and a managed switching element are running. The NAT daemon is similar to the NAT daemon described above by reference to . The NAT daemon maintains the NAT table which includes mappings of original and translated addresses. The managed switching element uses the NAT daemon to obtain translated address. The managed switching element in some embodiments sends flow templates to the NAT daemon to send original addresses and to obtain translated addresses as described above.

The hypervisor creates and manages VMs running in the host . In some embodiments the hypervisor notifies the managed switching element and or the NAT daemon of a migration of a VM running in the host out of the host before the VM migrates to another host. The managed switching element and or the NAT daemon gets such notifications by registering for callbacks in the event of a VM migration in some embodiments.

In some such embodiments the managed switching element asks the NAT daemon to fetch the NAT state e.g. address mapping for the VM and protocol information etc. associated with the migrating VM and to provide the NAT state to the hypervisor . In some embodiments the NAT daemon provides the NAT state associated with the migrating VM to the hypervisor when the NAT daemon is directly notified of the migration by the hypervisor . The hypervisor then migrates the NAT state to the destination host along with the migrating VM.

In some embodiments the NAT daemon sends the NAT state associated with the migrating VM directly to the NAT daemon running in the destination host. In these embodiments the NAT daemon and or the managed switching element notifies the hypervisor of the completion of the migration of the NAT state so that the hypervisor can start migrating the VM to the destination host.

In some embodiments the managed switching element also provides the flow entries related to the migrating VM to the hypervisor or to the managed switching element running in the destination host. When the hypervisor is provided with the flow entries the hypervisor sends the flow entries to the flow table of the managed switching element running in the destination host. The migration of flow entries to the destination host is optional since the NAT state alone will enable the managed switching element running in the destination host to obtain translated addresses for the migrating VM.

An example operation of the source host will now be described. When the hypervisor is to migrate VM e.g. per user input or inputs from a control cluster the hypervisor notifies the managed switching element . The managed switching element in this example then asks the NAT daemon to fetch the NAT state associated with VM and send the fetched state to the hypervisor .

The hypervisor then migrates the VM to the destination host by moving the data of the VM. In some embodiments the hypervisor is capable of live migration by capturing the running state of the VM and sending the state to the VM . The hypervisor also moves the fetched NAT state to the NAT table of the host so that the managed switching element running in the host can obtain translated addresses from the NAT daemon for VM just migrated into the host .

Because the hypervisor of some embodiments does not notify the managed switching element or the NAT daemon of a migration of a VM to a destination host the NAT state associated with the migrating VM is sent to the destination host after the hypervisor starts or completes migrating a VM to the destination host. In particular the managed switching element in some embodiments would detect migration of VM by e.g. detecting the MAC address of that is new to the managed switching element . The managed switching element notifies the control cluster the addition of VM therefore a new port of the managed switching element for the VM .

The control cluster is similar to the control clusters and described above. Upon receiving the notification from the managed switching element of the addition of VM the control cluster asks the hypervisor running in the source host to fetch the NAT state associated with the migrated VM and update the NAT table with the fetched NAT state. In some embodiments the control cluster additionally asks to fetch flow entries associated with the migrated VM and put those flow entries in the flow table of the destination host .

In some embodiments the control cluster may directly ask the managed switching element and or the NAT daemon to send the NAT state and or flow entries to the NAT daemon and or the managed switching element so that the NAT table and or are updated with the NAT state and or flow entries associated with the migrated VM .

An example operation of the source host the destination host and the control cluster will now be described. When the hypervisor is to migrate VM e.g. per user input or inputs from a control cluster the hypervisor migrates the VM by moving the configuration data or the running state of the VM to the host . The VM now running in the host sends a packet to the managed switching element . The managed switching element in this example detects the migration of VM to the host by recognizing that the source MAC address of the packet is new to the managed switching element . The managed switching element in this example then notifies the control cluster of the addition of VM or a creation of a new port for the VM .

The control cluster then asks the hypervisor to fetch the NAT state associated with VM and to send the NAT state to the destination host . The managed switching element running in the destination host can obtain translated addresses from the NAT daemon for VM that has just migrated into the host .

Some embodiments implement load balancing as an extra step in the L3 pipeline. For instance some embodiments implement a logical bundle based load balancing step followed by a destination network address translation. In some embodiments the logical router that provides the load balance service hosts the virtual IP address and hence will respond to the ARP requests sent to the virtual IP address VIP . With this the virtual IP will remain functional even if the traffic is sent to the VIP from the same L2 domain in which the cluster members exist.

The managed switching element of some embodiments is a second level managed switching element functioning as an extender. The managed switching element in some such embodiments is similar to the managed switching elements and described above in that the managed switching element implements a logical router not shown based on flow entries not shown or is running in the same host on which an L3 router that implements the logical router is running. In addition the managed switching element performs DNAT and load balancing to translate a destination address into another address and balance the load among different machines e.g. VMs that provide the same service e.g. a web service .

The managed switching elements implement logical switches not shown to which VMs are connected. The VMs and in this example provide the same service. That is the VMs and in some embodiments collectively act as a server that provides the same service. However the VMs and are separate VMs that have different IP addresses. The managed switching element or the L3 router not shown used by the managed switching element perform a load balancing to distribute workload among the VMs and .

In some embodiments load balancing is achieved by translating the destination address of the packets requesting the service into different addresses of the VMs providing the service. In particular the managed switching element or the L3 router not shown used by the managed switching element translates the destination addresses of the request packets into addresses of the several VMs and such that no particular VM of the VMs gets too much more workload than the other VMs do. More details about finding the current workload of the service providing VMs will be described further below.

In some embodiments the managed switching element or the L3 router perform an L3 routing after performing DNAT and load balancing of the logical processing pipeline. Therefore the managed switching element or the L3 router route the packets to different managed switching elements based on the translated destination addresses in these embodiments. The managed switching elements and are edge switching elements and thus send and receive packets to and from the VMs and directly. In other embodiments the managed switching element or the L3 router performs the L3 routing before performing DNAT and load balancing of the logical processing pipeline.

An example operation of the managed switching element will now be described. The managed switching element receives a packet requesting a service collectively provided by the VMs and . This packet comes from one of VM specifically from an application that uses a particular protocol. The packet in this example includes a protocol number that identifies the particular protocol. The packet also includes an IP address that represents the server providing the service as destination IP address. The details of performing source L2 processing on this packet are omitted for simplicity of description because it is similar to the source L2 processing examples described above and below.

After the source L2 processing is performed to route the packet to the managed switching element for performing an L3 processing that includes L3 routing . In this example the managed switching element performs the DNAT and load balancing on the packet. That is the managed switching element translates the destination IP address of the packet into an IP address of one of the VMs that provides the service. In this example the managed switching element selects one of VMs that has the least workload among the VMs . The managed switching element performs L3 routing on the packet i.e. routes the packet based on the new destination IP address.

The managed switching element receives the packet because the destination IP address is of one of the VMs and this destination IP is resolved into the MAC address of the VM. The managed switching element forwards the packet to the VM. This VM will return packets to the application that originally requested the service. These returning packets will reach the managed switching element and the managed switching element will perform NATs and identify that the application is the destination of these packets.

The managed switching element of some embodiments is similar to the managed switching elements described above by reference to in that the managed switching element implements the entire logical processing pipeline. That is the managed switching element implements the logical router and logical switches. In addition the managed switching element performs DNAT and load balancing to translate a destination address into another address and balance the load among different machines e.g. VMs that provide the same service e.g. a web service .

As mentioned above the managed switching element implements logical switches not shown to which VMs and are connected. The managed switching element also performs a load balancing to distribute workload among the VMs and . In particular the managed switching element translates the destination addresses of the request packets into addresses of the several VMs and such that no particular VM of the VMs gets too much more workload than the other VMs do. More details about finding current workload of the service providing VMs will be described further below.

In some embodiments the managed switching element performs an L3 routing after performing DNAT and load balancing of the logical processing pipeline. Therefore the managed switching element routes the packets to different managed switching elements based on the translated destination addresses. The managed switching elements and are edge switching elements and thus send and receive packets to and from the VMs and directly. In other embodiments the managed switching element performs the L3 routing before performing DNAT and load balancing of the logical processing pipeline.

The operation of the managed switching element would be similar to the example operation described above by reference to except that the managed switching element performs the entire logical processing pipeline including the DNAT and load balancing .

The managed switching element of some embodiments is similar to the managed switching elements described above by reference to in that the managed switching element performs the source L2 processing and the L3 processing of the logical processing pipeline. That is the managed switching element implements the logical router and a logical switch that is connected to a source machine. In addition the managed switching element performs DNAT and load balancing to translate destination address into another address and balance the load among different machines e.g. VMs that provide the same service e.g. a web service .

As mentioned above the managed switching element implements a logical switch not shown to which one or more of VMs are connected. The managed switching element also performs a load balancing to distribute workload among the VMs and . In particular the managed switching element translates the destination addresses of the request packets into addresses of the several VMs and such that no particular VM of the VMs gets too much more workload than the other VMs do. More details about finding the current workload of the service providing VMs will be described further below.

In some embodiments the managed switching element performs an L3 routing after performing DNAT and load balancing of the logical processing pipeline. Therefore the managed switching element routes the packets to different managed switching elements based on the translated destination addresses. The managed switching elements and are edge switching elements and thus send and receive packets to and from the VMs and directly. In other embodiments the managed switching element performs the L3 routing before performing DNAT and load balancing of the logical processing pipeline.

The operation of the managed switching element would be similar to the example operation described above by reference to except that different managed switching elements perform different portions of the logical processing pipeline.

The flow entries and each has a qualifier and an action. The text illustrated as flow entries and may not be in an actual format. Rather the text is just a conceptual illustration of a qualifier and an action pair. The host in some embodiments is a machine operated by an operating system e.g. Windows and Linux that is capable of running a set of software applications. The managed switching element of some embodiment is a software switching element e.g. Open vSwitch that executes in the host . As mentioned above a controller cluster not shown configures a managed switching element by supplying flow entries that specify the functionality of the managed switching element. The managed switching element of some embodiments does not itself generate flow entries.

The managed switching element of some embodiments runs all or part of the logical processing pipeline described above by reference to . In particular the managed switching element performs the L3 processing to route packets received from the machines if necessary based on flow entries in the forwarding table . In some embodiments the managed switching element is an edge switching element that receives a packet from a machine not shown that is coupled to the managed switching element. In some such embodiments one or more virtual machines not shown are running in the host and are coupled to the managed switching elements .

When the managed switching element is configured to perform load balancing the managed switching element of some embodiments uses the load balancing daemon for performing load balancing on packets. The load balancing daemon is similar to the NAT daemon in that the load balancing daemon provides a translated destination address e.g. a destination IP address . In addition the load balancing daemon selects a destination into which to translate the original destination address based on the current load of the machines the IP addresses of which are included in the table .

The load balancing daemon of some embodiments is a software application running on the host . The load balancing daemon maintains the connection table which includes pairings of connection identifiers and available addresses of the machines that provide the service. Though not depicted the connection table of some embodiments may also include the current workload quantified for a machine associated with an address. In some embodiments the load balancing daemon periodically communicates with the VMs providing the service to get the updated state of the VMs including the current workload on the VMs.

When the managed switching element asks for an address to select based on connection identifiers the load balancing daemon in some embodiments looks up the table to find the address into which the given destination address should be translated. In some embodiments the load balancing daemon runs a scheduling method to identify a server VM in order to balance the load among the server VMs. Such a scheduling algorithm considers the current load on the machine associated with the address. More details and examples of load balancing methods are described in the U.S. Provisional Patent Application 61 560 279 which is incorporated herein by reference.

The connection identifiers uniquely identify a connection between the requester of the service i.e. the origin or source of the packet and the machine that ends up providing the requested service so that the packets returning from the machine can be accurately relayed back to the requester. The source IP addresses of these returning packets will be translated back to an IP address referred to as virtual IP address that represents a server providing the service. The mapping between these connection identifiers will also be used for the packets that are subsequently sent from the source. In some embodiments the connection identifiers include a source port a destination port a source IP address a destination IP address a protocol identifier etc. The source port is a port from which the packet was sent e.g. a TCP port . The destination port is a port to which the packet is to be sent. The protocol identifier identifies the type of protocol e.g. TCP UDP etc. used for formatting the packet.

The managed switching element and the load balancing daemon of different embodiments use different techniques to ask for and supply addresses. For instance the managed switching element of some embodiments sends a packet which has an original address but does not have a translated address to the load balancing daemon. The load balancing daemon of these embodiments translates the original address into a translated address. The load balancing daemon sends the packet back to the managed switching element which will perform logical forwarding and or routing to send the packet towards the destination machine. In some embodiments the managed switching element initially sends metadata along the with packet that contains an original address to resolve to the load balancing daemon . This metadata includes information e.g. register values logical pipeline state etc. that the managed switching element uses to resume performing the logical processing pipeline when the managed switching element receives the packet back from the load balancing daemon .

In other embodiments the managed switching element of some embodiments requests an address by sending a flow template which is a flow entry that does not have actual values for the addresses to the load balancing daemon . The load balancing daemon finds out the addresses to fill in the flow template by looking up the table . The load balancing daemon then sends the flow template that is filled in with actual addresses back to the managed switching element by putting the filled in flow template into the forwarding table . In some embodiments the load balancing daemon assigns to the filled in flow template a priority value that is higher than the priority value of the flow template that is not filled in. Moreover when the load balancing daemon fails to find a translated address the load balancing daemon would specify in the flow template to drop the packet.

An example operation of the managed switching element and the load balancing daemon will now be described in terms of three different stages 1 3 encircled 1 3 . In this example the managed switching element is a managed edge switching element that receives a packet to forward and route from a machine not shown . In particular the packet in this example is a request for a service. The packet has an IP address that represents a server that provides the requested service.

The managed switching element receives this packet and performs the L3 processing based on the flow entries in the forwarding table . While performing the L3 processing on the packet the managed switching element at stage 1 identifies the flow entry and performs the action specified in the flow entry . As shown the flow entry indicates that a flow template having connection identifiers should be sent to the load balancing daemon to have the load balancing daemon to provide a new destination IP address. In this example the flow entry has a priority value of N which is a number in some embodiments.

At stage 2 the load balancing daemon receives the flow template and finds out that the destination IP address of a packet that has the specified connection IDs is to be translated into 2.1.1.10 by looking up the connection table and by running a scheduling algorithm. The load balancing daemon fills out the flow template and inserts the filled in template now the flow entry into the forwarding table . In this example the load balancing daemon assigns a priority of N 1 to the filled in template.

At stage 3 the managed switching element uses the flow entry to change the destination IP address for the packet. Also for the packets that the managed switching element subsequently processes the managed switching element uses flow entry over the flow entry when a packet has the specified connection identifiers.

In some embodiments the load balancing daemon and the managed switching element run in a same virtual machine that is running on the host or in different virtual machines running on the host . The load balancing daemon and the managed switching element may also run in separate hosts.

The virtualization application in some embodiments defines forwarding rules that route DHCP requests to a DHCP daemon that is running in a shared host. Using a shared host for this functionality avoids the extra cost of running a DHCP daemon per customer.

As shown in the left half of the figure the logical network includes a logical router and two logical switches and . VMs and are connected to the logical switch . That is VMs and send and receive packets forwarded by the logical switch . VM is connected to the logical switch . The logical router routes packets between the logical switches and . The logical router is also connected to a DHCP Daemon which provides DHCP service to the VMs in the logical network which are VMs of the user A.

The logical network for the user B includes a logical router and two logical switches and . VMs and are connected to the logical switch . VM is connected to the logical switch . The logical router routes packets between the logical switches and . The logical router is also connected to a DHCP Daemon which provides DHCP service to the VMs in the logical network which are VMs of the user B.

In the logical implementation shown in the left half of the figure each logical network for a user has its own DHCP daemon. In some embodiments the DHCP daemons and may be physically implemented as separate DHCP daemons running in different hosts or VMs. That is each user would have a separate DHCP daemon for the user s machines only.

In other embodiments the DHCP daemons for different users may be physically implemented as a single DHCP daemon that provides DHCP service to VMs of different users. That is different users share the same DHCP daemon. The DHCP daemon is a shared DHCP daemon that serves VMs of both users A and B. As shown in the right half of figure the managed switching elements that implement the logical routers and and the logical switches and for users A and B use the single DHCP daemon . Therefore VMs of the users A and B use the DHCP daemon to dynamically obtain an address e.g. an IP address .

The DHCP daemon of different embodiments may run in different hosts. For instance the DHCP daemon of some embodiments runs in the same host not shown in which one of the managed switching elements is running. In other embodiments the DHCP daemon does not run in a host on which a managed switching element is running and instead runs in a separate host that is accessible by the managed switching elements.

As shown the central DHCP daemon runs in a host in which a managed switching element also runs. The managed switching element of some embodiments is a second level managed switching element functioning as a pool node for managed switching elements and . The central DHCP daemon provides DHCP services to different VMs and of different users. In some embodiments the central DHCP daemon distributes the available addresses e.g. IP addresses in batches of addresses to different local DHCP daemons including local DHCP daemons and in order to offload the DHCP service to these local DHCP daemons. The central DHCP daemon provides more addresses to a local DHCP daemon when the local DHCP daemon runs out of available address to assign in its own batch of addresses.

The local DHCP daemon runs in a host in which a managed switching element also runs. The managed switching element is an edge switching element that directly sends and receives packets to and from VMs . The managed switching element implements one or more logical switches and logical routers of different users. That is the VMs may belong to different users. The local DHCP daemon provides DHCP service to VMs using the batch of addresses that the local DHCP daemon obtains from the central DHCP daemon . The local DHCP daemon resorts to the central DHCP daemon when the local DHCP daemon runs out of available addresses to assign in the batch of addresses . In some embodiments the local DHCP daemon communicates with the central DHCP daemon via the managed switching elements and . The managed switching elements and has a tunnel established between them in some embodiments.

Similarly the local DHCP daemon runs in a host in which a managed switching element also runs. The managed switching element is an edge switching element that directly sends and receives packets to and from VMs . The managed switching element implements one or more logical switches and logical routers of different users. The local DHCP daemon provides DHCP service to VMs using the batch of addresses that the local DHCP daemon obtains from the central DHCP daemon . In some embodiments the batch of addresses does not include addresses that are in the batch of addresses that are allocated to the local DHCP daemon running in the host . The local DHCP daemon also resorts to the central DHCP daemon when the local DHCP daemon runs out of available addresses to assign in its own batch of addresses . In some embodiments the local DHCP daemon communicates with the central DHCP daemon via the managed switching elements and . The managed switching elements and have a tunnel established between them in some embodiments.

In the discussion above various L3 services that are provided by the virtualization application of some embodiments were described. To maximize the network control system s flexibility some embodiments interpose service machines that provide similar functionality to those provided by the middleboxes that users use today in the physical networks.

Accordingly the network control system of some embodiments includes at least one middlebox VM that is attached to a LDPS of a logical network. Then the pipeline state of the LDP sets is programmed by the control application that populates the logical control plane so that the relevant packets are forwarded to the logical port of this VM. After the VM has processed the packet the packet is sent back to the logical network so that its forwarding continues through the logical network. In some embodiments the network control system utilizes many such middlebox VMs. The middlebox VMs interposed in this manner may be very stateful and implement features well beyond the L3 services described in this document.

The scalability implications of the logical L3 switching design of some embodiments along three dimensions are addressed below. These three dimensions are 1 logical state 2 physical tunneling state and 3 distributed binding lookups. Most of the logical pipeline processing occurs at the first hop. This implies that all the logical table state of all interconnected LDP sets is disseminated in some embodiments to everywhere in the network where the pipeline execution may take place. In other words the combined logical state of all interconnected LDP sets is disseminated to every managed edge switching element attached to any of these LDP sets in some embodiments. However in some embodiments the meshiness of the logical topology does not increase the dissemination load of the logical state.

To limit the state dissemination some embodiments balance the pipeline execution between the source and destination devices so that the last LDPS pipeline would be executed not at the first hop but at the last hop. However in some cases this may result in not disseminating enough state for every managed switching element to do the logical forwarding decision of the last LDPS without that state the source managed switching elements might not even be able to deliver the packets to the destination managed switching elements. Accordingly some embodiments will constrain the general LDPS model in order to balance the pipeline execution between the source and destination devices.

The logical state itself is not likely to contain more than at most O N entries N is the total number of logical ports in the interconnected LDP sets as the logical control plane is designed in some embodiments to mimic the physical control planes that are used today and the physical control planes are limited by the capabilities of existing hardware switching chipsets. Therefore disseminating the logical state might not be the primary bottleneck of the system but eventually it might become one as the logical control plane design grows.

Some embodiments partition the managed switching elements of a network into cliques interconnected by higher level aggregation switching elements. Instead of implementing partitioning to reduce logical state with an everything on the first hop model some embodiments partition to reduce the tunneling state as discussed below. Examples of cliques are described in the above mentioned U.S. patent application Ser. No. 13 177 535. This application also describes various embodiments that perform all or most of the logical data processing at the first hop managed switching elements.

The physical tunneling state maintained in the whole system is O N where N is the number of logical ports in the interconnected LDP sets total. This is because any managed edge switching element with a logical port has to be able to directly send the traffic to the destination managed edge switching element. Therefore maintaining tunneling state in an efficient manner without imposing O N load to any centralized control element becomes even more important than with pure L2 LDP sets. The aggregation switching elements are used in some embodiments to slice the network into cliques. In some of these embodiments the packet is still logically routed all the way in the source managed edge switching element but instead of tunneling it directly to the destination edge switching element it is sent to a pool node that routes it towards the destination based on the destination MAC address. In essence the last L2 LDPS spans multiple cliques and pool nodes are used to stitch together portions of that L2 domain.

In some embodiments a managed switching element does not keep all the information e.g. flow entries in lookup tables to perform the entire logical processing pipeline . For instance the managed switching element of these embodiments does not maintain the information for determining access control with respect to a logical port of the destination logical network through which to send the packet to the destination machine of the packet.

An example packet flow along the managed switching elements and will now be described. When VM 1 that is coupled to the logical switch sends a packet to VM 4 that is coupled to the logical switch the packet is first sent to the managed switching element . The managed switching element then performs the L2 processing and the L3 processing on the packet.

The managed switching element then performs a portion of the L2 processing . Specifically the managed switching element determines access control for the packet. For instance the managed switching element determines that the packet does not have network addresses e.g. source destination MAC IP addresses etc. that will cause the logical switch to reject the packet that came through port Y of the logical switch . The managed switching element then determines that port 1 of the logical switch is the port through which to send the packet out to the destination VM 4. However the managed switching element does not determine access control for the packet with respect to port 1 of the logical switch because the managed switching element in some embodiments does not have information e.g. flow entries to perform the egress ACL .

The managed switching element then performs a mapping lookup to determine a physical port to which the logical port 1 of the logical switch is mapped. In this example the managed switching element determines that logical port 1 of the logical switch is mapped to port 5 of the managed switching element . The managed switching element then performs a physical lookup to determine operations for forwarding the packet to the physical port. In this example the managed switching element determines that the packet should be sent to VM 4 through port 5 of the managed switching element . The managed switching element in this example modifies the logical context of the packet before sending it out along with the packet to VM 4.

The managed switching element sends the packet to the managed switching element . In some cases the managed switching element sends the packet over the tunnel that is established between the managed switching elements and e.g. the tunnel that terminates at port 3 of the managed switching element and port 3 of the managed switching element . When the tunnel is not available the managed switching elements sends the packet to a pool node not shown so that the packet can reach the managed switching element .

When the managed switching element receives the packet the managed switching element performs the egress ACL on the packet based on the logical context of the packet the logical context would indicate that it is the egress ACL that is left to be performed on the packet . For instance the managed switching element determines that the packet does not have network addresses that will cause the logical switch not to send the packet through the port 1 of the logical switch . The managed switching element then sends the packet to VM 4 through port 5 of the managed switching element as determined by the managed switching element that performed the L2 processing .

As shown in the bottom half of the managed switching element includes L2 entries and and L3 entries . These entries are flow entries that a controller cluster not shown supplies to the managed switching element . Although these entries are depicted as three separate tables the tables do not necessarily have to be separate tables. That is a single table may include all these flow entries.

When VM 1 that is coupled to the logical switch sends a packet to VM 4 that is coupled to the logical switch the packet is first sent to the managed switching element through port 4 of the managed switching element . The managed switching element performs an L2 processing on the packet based on the forwarding tables of the managed switching element . In this example the packet has a destination IP address of 1.1.2.10 which is the IP address of VM 4. The packet s source IP address is 1.1.1.10. The packet also has VM1 s MAC address as a source MAC address and the MAC address of the logical port 1 e.g. 01 01 01 01 01 01 of the logical router as a destination MAC address.

The operation of the managed switching element until the managed switching element identifies an encircled 9 and performs L2 logical processing is similar to the operation of the managed switching element in the example of except that the managed switching element in the example of is performed on packet .

Based on the logical context and or other fields stored in the packet s header the managed switching element then identifies a record indicated by an encircled 10 referred to as record 10 in the L2 entries that implements the context mapping of the stage . In this example the record 10 identifies port 5 of the managed switching element to which VM 4 is coupled as the port that corresponds to the logical port determined at stage of the logical switch to which the packet is to be forwarded. The record 10 additionally specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Based on the logical context and or other fields stored in the packet s header the managed switching element then identifies a record indicated by an encircled 11 referred to as record 11 in the L2 entries that implements the physical mapping of the stage . The record 11 specifies port 3 of the managed switching element as a port through which the packet is to be sent in order for the packet to reach the managed switching element . In this case the managed switching element is to send the packet out of port 3 of managed switching element that is coupled to the managed switching element .

As shown in the managed switching element includes a forwarding table that includes rules e.g. flow entries for processing and routing the packet . When the managed switching element receives the packet from the managed switching element the managed switching element begins processing the packet based on the forwarding tables of the managed switching element . The managed switching element identifies a record indicated by an encircled 1 referred to as record 1 in the forwarding tables that implements the context mapping. The record 1 identifies the packet s logical context based on the logical context that is stored in the packet s header. The logical context specifies that the packet has been processed up to the stage by the managed switching element . As such the record 1 specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 2 referred to as record 2 in the forwarding tables that implements the egress ACL. In this example the record 2 allows the packet to be further processed and thus specifies the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . In addition the record 2 specifies that the managed switching element store the logical context i.e. the packet has been processed for L2 egress ACL of the logical switch of the packet in the set of fields of the packet s header.

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 3 referred to as record 3 in the forwarding tables that implements the physical mapping. The record 3 specifies the port 5 of the managed switching element through which the packet is to be sent in order for the packet to reach VM 4. In this case the managed switching element is to send the packet out of port 5 of managed switching element that is coupled to VM 4. In some embodiments the managed switching element removes the logical context from the packet before sending the packet to VM 4.

In some embodiments a managed switching element does not keep all the information e.g. flow entries in lookup tables to perform the entire logical processing pipeline . For instance the managed switching element of these embodiments does not maintain the information for performing logical forwarding for the destination logical network on the packet.

An example packet flow along the managed switching elements and will now be described. When VM 1 that is coupled to the logical switch sends a packet to VM 4 that is coupled to the logical switch the packet is first sent to the managed switching element . The managed switching element then performs the L2 processing and the L3 processing on the packet.

The managed switching element sends the packet to the managed switching element . In some cases the managed switching element sends the packet over the tunnel that is established between the managed switching elements and e.g. the tunnel that terminates at port 3 of the managed switching element and port 3 of the managed switching element . When the tunnel is not available the managed switching elements sends the packet to a pool node not shown so that the packet can reach the managed switching element .

When the managed switching element receives the packet the managed switching element performs the L2 processing on the packet based on the logical context of the packet the logical context would indicate that it is the entire L2 processing that is left to be performed on the packet . The managed switching element then sends the packet to VM 4 through port 5 of the managed switching element .

As shown in the bottom half of the managed switching element includes L2 entries and L3 entries . These entries are flow entries that a controller cluster not shown supplies to the managed switching element . Although these entries are depicted as two separate tables the tables do not necessarily have to be separate tables. That is a single table may include all these flow entries.

When VM 1 that is coupled to the logical switch sends a packet to VM 4 that is coupled to the logical switch the packet is first sent to the managed switching element through port 4 of the managed switching element . The managed switching element performs an L2 processing on the packet based on the forwarding tables of the managed switching element . In this example the packet has a destination IP address of 1.1.2.10 which is the IP address of VM 4. The packet s source IP address is 1.1.1.10. The packet also has VM 1 s MAC address as a source MAC address and the MAC address of the logical port 1 e.g. 01 01 01 01 01 01 of the logical router as a destination MAC address.

The operation of the managed switching element until the managed switching element identifies an encircled 7 and performs L3 egress ACL with respect to the port 2 of the logical router is similar to the operation of the managed switching element in the example of except that the managed switching element in the example of is performed on packet .

Based on the logical context and or other fields stored in the packet s header the managed switching element then identifies a record indicated by an encircled 8 referred to as record 8 in the L2 entries that implements the physical mapping of the stage . The record 8 specifies that the logical switch is implemented in the managed switching element and the packet should be sent to the managed switching element .

Based on the logical context and or other fields stored in the packet s header the managed switching element then identifies a record indicated by an encircled 9 referred to as record 9 in the L2 entries that implements the physical mapping of the stage . The record 9 specifies port 3 of the managed switching element as a port through which the packet is to be sent in order for the packet to reach the managed switching element . In this case the managed switching element is to send the packet out of port 3 of managed switching element that is coupled to the managed switching element .

As shown in the managed switching element includes a forwarding table that includes rules e.g. flow entries for processing and routing the packet . When the managed switching element receives the packet from the managed switching element the managed switching element begins processing the packet based on the forwarding tables of the managed switching element . The managed switching element identifies a record indicated by an encircled 1 referred to as record 1 in the forwarding tables that implements the context mapping. The record 1 identifies the packet s logical context based on the logical context that is stored in the packet s header. The logical context specifies that the L2 processing and the L3 processing have been performed on the packet by the managed switching element . The record 1 specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Based on the logical context and or other fields stored in the packet s header the managed switching element identifies a record indicated by an encircled 2 referred to as record 2 in the L2 forwarding table that implements the L2 ingress ACL. In this example the record 2 allows the packet to come through the logical port Y of the logical switch not shown and thus specifies the packet be further processed by the managed switching element e.g. by sending the packet to a dispatch port . In addition the record 2 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline of the packet in the set of fields of the packet s header.

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 3 referred to as record 3 in the L2 forwarding table that implements the logical L2 forwarding. The record 3 specifies that a packet with the MAC address of VM 4 as destination MAC address should be forwarded through a logical port 2 of the logical switch that is connected to VM 4.

The record 3 also specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . Also the record 3 specifies that the managed switching element store the logical context i.e. the packet has been processed by the stage of the processing pipeline in the set of fields of the packet

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 4 referred to as record 4 in the forwarding tables that implements the egress ACL. In this example the record 4 allows the packet to be further processed and thus specifies the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port . In addition the record 4 specifies that the managed switching element store the logical context i.e. the packet has been processed for L2 egress ACL of the logical switch of the packet in the set of fields of the packet s header.

Based on the logical context and or other fields stored in the packet s header the managed switching element then identifies a record indicated by an encircled 5 referred to as record 5 in the L2 entries that implements the context mapping. In this example the record 5 identifies port 5 of the managed switching element to which VM 4 is coupled as the port that corresponds to the logical port 2 of the logical switch to which the packet is to be forwarded. The record 5 additionally specifies that the packet be further processed by the forwarding tables e.g. by sending the packet to a dispatch port .

Next the managed switching element identifies based on the logical context and or other fields stored in the packet s header a record indicated by an encircled 6 referred to as record 6 in the forwarding tables that implements the physical mapping. The record 6 specifies the port 5 of the managed switching element through which the packet is to be sent in order for the packet to reach VM 4. In this case the managed switching element is to send the packet out of port 5 of managed switching element that is coupled to VM 4. In some embodiments the managed switching element removes the logical context from the packet before sending the packet to VM 4.

The execution of all the pipelines on the logical path of a packet has implications to the distributed lookups namely ARP and learning. As the lookups can now be executed by any edge switching element having a logical port attached to the logical network the total volume of the lookups is going to exceed the lookups executed on a similar physical topology even though the packet would head towards the same port differing senders cannot share the cached lookup state as the lookups will be initiated on different managed edge switching elements. Hence the problems of flooding are amplified by the logical topology and a unicast mapping based approach for lookups is preferred in practice.

By sending a special lookup packet towards a cloud of mapping servers e.g. pool or root nodes the source edge switching element can do the necessary lookups without resorting to flooding. In some embodiments the mapping server benefits from heavy traffic aggregate locality and hence good cache hit ratios on client side as well as from datapath only implementation resulting in excellent throughput.

The flow entries and each has a qualifier and an action. The text illustrated as flow entries and may not be an actual format. Rather the text is just a conceptual illustration of a qualifier and an action pair. In some embodiments flow entries have priorities and a managed switching element takes the action of the flow entry with the highest priority when qualifiers for more than one flow entry are satisfied.

The host in some embodiments is a machine operated by an operating system e.g. Windows and Linux that is capable of running a set of software applications. The managed switching element of some embodiment is a software switching element e.g. Open vSwitch that executes in the host . As mentioned above a controller cluster not shown configures a managed switching element by supplying flow entries that specify the functionality of the managed switching element. The managed switching element of some embodiments does not itself generate flow entries and ARP requests.

The managed switching element of some embodiments runs all or part of the logical processing pipeline described above. In particular the managed switching element is a managed switching element e.g. the managed switching elements or that performs the L3 processing to route packets received from the machines as necessary based on flow entries in the forwarding table . In some embodiments the managed switching element is an edge switching element that receives a packet from a machine not shown that is coupled to the managed switching element. In some such embodiments one or more virtual machines not shown are running in the host and are coupled to the managed switching elements . In other embodiments the managed switching element is a second level managed switching element.

When the managed switching element receives a packet that is the very first packet being sent to a destination machine that is in another logical network or the packet itself is an ARP request the managed switching element of these embodiments would not yet know the MAC address of the destination machine. In other words the managed switching element would not know the mapping between the next hop IP address and the destination MAC address. In order to resolve the next hop IP address into the destination MAC address the managed switching element of some embodiments requests the destination MAC address of the packet from the L3 daemon .

The L3 daemon of some embodiments is a software application running on the host . The L3 daemon maintains the table which includes mappings of IP and MAC addresses. When the managed switching element asks for a destination MAC address that corresponds to a next hop IP address the L3 daemon looks up the mapping table to find the destination MAC address to which the source IP address is mapped. In some cases the destination MAC address to which the source IP address is mapped is the MAC address of the next hop logical router .

The managed switching element and the L3 daemon of different embodiments uses different techniques to ask for and supply addresses. For instance the managed switching element of some embodiments sends a packet which has a destination IP address but does not have a destination MAC address to the L3 daemon. The L3 daemon of these embodiments resolves the IP address into a destination MAC address. The L3 daemon sends the packet back to the managed switching element which will perform logical forwarding and or routing to send the packet towards the destination machine. In some embodiments the managed switching element initially sends metadata along the packet that contains a destination IP address to resolve to the L3 daemon . This metadata includes information e.g. register values logical pipeline state etc. that the managed switching element uses to resume performing the logical processing pipeline when the managed switching element receives the packet back from the L3 daemon .

In other embodiments the managed switching element requests a destination address by sending a flow template which is a flow entry that does not have actual value for the destination MAC addresses to the L3 daemon . The L3 daemon finds the destination MAC addresses to fill in the flow template by looking up the mapping table . The L3 daemon then sends the flow template that is filled in with actual destination MAC addresses back to the managed switching element by putting the filled in flow template into the forwarding table . In some embodiments the L3 daemon assigns the filled in flow template a priority value that is higher than the priority value of the flow template that is not filled in.

When the mapping table has an entry for the destination IP address and the entry has the destination MAC address mapped to the destination IP address the L3 daemon uses the destination MAC address to write in the packet or fill in the flow template. When there is no such entry the L3 daemon generates an ARP request and broadcasts the ARP packet to other hosts or VMs that run L3 daemons. In particular the L3 daemon of some embodiments only sends the ARP requests to those hosts or VMs to which the next hop logical L3 router may be attached. The L3 daemon receives a response to the ARP packet that contains the destination MAC address from one of the hosts or VMs that received the ARP packet. The L3 daemon maps the destination IP address to the destination MAC address and adds this mapping to the mapping table . In some embodiments the L3 daemon sends a unicast packet periodically to another L3 daemon that responded to the ARP request to check the validity of the destination MAC address. In this manner the L3 daemon keeps the IP and MAC addresses mapping up to date.

In some embodiments when the L3 daemon still fails to find a resolved address after looking up the flow entries and sending ARP requests to other L3 daemon instances the L3 daemon would specify in the flow template to drop the packet or the L3 daemon itself will drop the packet.

When the managed switching element receives an ARP packet from another host or VM the managed switching element of some embodiments does not forward the ARP packet to the machines that are coupled to the managed switching element. The managed switching element in these embodiments sends the ARP packet to the L3 daemon. The L3 daemon maintains in the mapping table mapping between IP addresses and MAC addresses that are locally available e.g. IP addresses and MAC addresses of the machines that are coupled to the managed switching element . When the mapping table has an entry for the IP address of the received ARP packet and the entry has a MAC address of a VM that is coupled to the managed switching element the L3 daemon sends the MAC address in the response to the ARP packet to the host or VM i.e. the L3 daemon of the host or VM from which the ARP packet originates.

An example operation of the managed switching element and the L3 daemon will now be described in terms of three different stages 1 3 encircled 1 3 . In this example the managed switching element is a managed edge switching element that receives a packet to forward and route from a machine not shown . The managed switching element receives a packet and performs the logical processing based on the flow entries in the forwarding table .

When the packet is the very first packet that bears the IP address of the destination machine or the packet is an ARP request from a source machine the managed switching element at stage 1 identifies the flow entry and performs the action specified in the flow entry . As shown the flow entry indicates that a flow template having a destination IP address 1.1.2.10 to be resolved to a destination MAC X should be sent to the L3 daemon . In this example the flow entry has a priority value of N which is a number in some embodiments.

At stage 2 the L3 daemon receives the flow template and finds out that 1.1.2.10 is to be resolved to 01 01 01 01 01 09 by looking up the mapping table . The L3 daemon fills out the flow template and inserts the filled in template now the flow entry into the forwarding table . In this example the L3 daemon assigns a priority of N 1 to the filled in template.

At stage 3 the managed switching element in some embodiments uses the flow entry to set the destination MAC address for the packet. Also for the packets that the managed switching element subsequently processes the managed switching element uses flow entry over the flow entry when a packet has the destination IP address of 1.1.2.10.

In some embodiments the L3 daemon and the managed switching element runs in a same virtual machine that is running on the host or in different virtual machines running on the host . In some embodiments the L3 daemon runs in the user space of a virtual machine. The L3 daemon and the managed switching element may also run in separate hosts.

In some embodiments the managed switching element does not rely on the L3 daemon to resolve addresses. In some such embodiments the control cluster not shown in may statically configure the flow entries such that the flow entries include the mappings between IP addresses to MAC addresses obtained through API calls i.e. inputs or DHCP.

The process begins by determining at whether the packet needs address resolution e.g. resolving a destination IP address to a destination MAC address . In some embodiments the process determines whether the packet needs L3 processing based on flow entry. The flow entry of which the qualifier matches the information stored in the packet s header or logical context specifies that the packet needs address resolution.

When the process determines at that the packet does not need address resolution the process ends. Otherwise the process determines at whether the process needs to request an address into which to resolve a packet s address e.g. destination IP address from an L3 daemon. In some embodiments the process determines whether the process needs to ask the L3 daemon based on the flow entry. For instance the flow entry may specify that the address into which to resolve the packet s address should be obtained by requesting for the resolved address from the L3 daemon. In some embodiments the process determines that the L3 daemon should provide the resolved address when the flow entry is a flow template that has an empty field for the resolved address or some other value in the field for indicating the resolved address should be obtained from the L3 daemon.

When the process determines at that the process does not need to request for an address from the L3 daemon the process obtains at the resolved address from the flow entry. For instance the flow entry would provide the translated address. The process then proceeds to which will be described further below. When the process determines at that the process needs to request for an address from the L3 daemon the process at requests for and obtains the resolved address from the L3 daemon. In some embodiments the process requests for the resolved address by sending a flow template to the L3 daemon. The L3 daemon would fill the flow template with the resolved address and place that filled in flow template in the forwarding table not shown that the process uses.

Next the process modifies the packet with the resolved address. In some embodiments the process modifies an address field in the header of the packet. Alternatively or conjunctively the process modifies the logical context to replace the packet s address with the resolved address. The process then ends.

The hosts and are similar to the host described above by reference to in that each of the hosts and runs an L3 daemon a managed switching element and one or more VMs.

The host runs a map server. The map server of some embodiments maintains a global mapping table that includes all the entries of all mapping tables maintained by L3 daemons running in every host in the network that runs a managed edge switching element. In some embodiments an L3 daemon in the network sends the entries of mapping between locally available IP addresses and MAC addresses mappings. Whenever there is a change to the machines coupled to a managed switching element of a host e.g. when a VM fails or is coupled to or de coupled from the managed switching element the L3 daemon of the host updates the respective local mapping table accordingly and also sends the updates e.g. by sending a special publish packet containing the updates to the map server so that the map server keeps the global mapping table updated with the change.

In some embodiments the L3 daemon running in each host that runs a managed edge switching element does not broadcast an ARP packet when the local mapping does not have an entry for a destination IP address to resolve. Instead the L3 daemon consults the map server to resolve the destination IP address into the destination MAC address. The map server resolves the destination IP address into a destination MAC address by looking up the global mapping table . In the case that the map server cannot resolve the IP address e.g. when the global mapping table does not have an entry for the IP address or the map server fails the L3 daemon will resort to broadcasting an ARP packet to other hosts that run managed edge switching elements. In some embodiments the map server is implemented in the same host or VM in which a second level managed switching element e.g. a pool node is implemented.

The process begins by determining at whether the process has a resolved address for the particular address received from the managed switching element. In some embodiments the process looks up a local mapping table that includes mappings of IP and MAC addresses to determine whether the process has a resolved address for the particular address.

When the process determines that the process has a resolved address the process proceeds to which will be described further below. Otherwise the process requests for and obtains a resolved address from the map server. The process then modifies at the local mapping table with the resolved address obtained from the mapping server. In some embodiments the process inserts a new mapping of the resolved address and the particular address into the local mapping table.

The process then sends the resolved address to the managed switching element. In some embodiments the process modifies the packet that has the particular address. In other embodiments the process modifies the flow template that the managed switching element had sent as a request for the resolved address. The process then ends.

The process begins by monitoring at a set of managed switching elements. In particular the process monitors for coupling and decoupling of machines to and from a managed switching element or any address change for the machines coupled to a managed switching element. In some embodiments the set of managed switching elements includes those managed switching elements that are running on the same host or virtual machine on which the L3 daemon is running.

Next the process determines at whether there has been such a change to a managed switching element that the process monitors. When the process determines at that there has not been a change the process loops back to to keep monitoring the set of managed switching elements. Otherwise the process modifies at the corresponding entries in the local mapping table. For instance when a VM migrates and gets coupled to one of the managed switching element in the set the process inserts a mapping of the IP address and the MAC address of the migrated VM into the local mapping table.

The process then sends the updated mapping to the map server so that the map server can update the global mapping table with the new and or modified mapping of the IP address and MAC address. The process then ends.

As described above the managed switching elements of some embodiments implement logical switches and logical routers based on flow tables supplied to the managed switching elements by the controller cluster one or more controller instances of some embodiments. In some embodiments the controller cluster generates these flow entries by performing table mapping operations based on the inputs or network events the controller cluster detects. Details of these controller clusters and their operations are described in U.S. patent application Ser. No. 13 177 533 and in the above incorporated filed U.S. patent application Ser. No. 13 589 077.

As mentioned in U.S. patent application Ser. No. 13 589 077 the network control system in some embodiments is a distributed control system that includes several controller instances that allow the system to accept logical datapath sets from users and to configure the switching elements to implement these logical datapath sets. In some embodiments one type of controller instance is a device e.g. a general purpose computer that executes one or more modules that transform the user input from a logical control plane to a logical forwarding plane and then transform the logical forwarding plane data to physical control plane data. These modules in some embodiments include a control module and a virtualization module. A control module allows a user to specify and populate logical datapath set while a virtualization module implements the specified logical datapath set by mapping the logical datapath set onto the physical switching infrastructure. In some embodiments the control and virtualization applications are two separate applications while in other embodiments they are part of the same application.

From the logical forwarding plane data for a particular logical datapath set the virtualization module of some embodiments generates universal physical control plane UPCP data that is generic for any managed switching element that implements the logical datapath set. In some embodiments this virtualization module is part of a controller instance that is a master controller for the particular logical datapath set. This controller is referred to as the logical controller.

In some embodiments the UPCP data is then converted to customized physical control plane CPCP data for each particular managed switching element by a controller instance that is a master physical controller instance for the particular managed switching element or by a chassis controller for the particular managed switching element as further described in U.S. patent application Ser. No. 13 589 077. When the chassis controller generates the CPCP data the chassis controller obtains the UPCP data from the virtualization module of the logical controller through the physical controller.

Irrespective of whether the physical controller or chassis controller generate the CPCP data the CPCP data for a particular managed switching element needs to be propagated to the managed switching element. In some embodiments the CPCP data is propagated through a network information base NIB data structure which in some embodiments is an object oriented data structure. Several examples of using the NIB data structure are described in U.S. patent application Ser. Nos. 13 177 529 and 13 177 533 which are incorporated herein by reference. As described in these applications the NIB data structure is also used in some embodiments to may serve as a communication medium between different controller instances and to store data regarding the logical datapath sets e.g. logical switching elements and or the managed switching elements that implement these logical datapath sets.

However other embodiments do not use the NIB data structure to propagate CPCP data from the physical controllers or chassis controllers to the managed switching elements to communicate between controller instances and to store data regarding the logical datapath sets and or managed switching elements. For instance in some embodiments the physical controllers and or chassis controllers communicate with the managed switching elements through OpenFlow entries and updates over the configuration protocol. Also in some embodiments the controller instances use one or more direct communication channels e.g. RPC calls to exchange data. In addition in some embodiments the controller instances e.g. the control and virtualization modules of these instance express the logical and or physical data in terms of records that are written into the relational database data structure. In some embodiments this relational database data structure are part of the input and output tables of a table mapping engine called nLog that is used to implement one or more modules of the controller instances.

As shown the logical controller includes a control application and a virtualization application . In some embodiments the control application is used to receive the logical control plane data and to convert this data to logical forwarding plane data that is then supplied to the virtualization application . The virtualization application generates universal physical control plane data from logical forwarding plane data.

In some embodiments some of the logical control plane data are converted from the inputs. In some embodiments the logical controller supports a set of API calls. The logical controller has an input translation application not shown that translates the set of API calls into LCP data. Using the API calls the user can configure logical switches and logical routers as if the user is configuring physical switching elements and routers.

The physical controllers and are the masters of the managed switching elements and respectively. The physical controller and of some embodiments receive the UPCP data from the logical controller and converts the UPCP data to CPCP data for the managed switching elements and respectively. The physical controller then sends the CPCP data for the managed switching element to the managed switching element . The physical controller sends the CPCP data for the managed switching element to the managed switching element . The CPCP data for the managed switching elements and are in the form of flow entries. The managed switching elements and then perform forwarding and routing the packets based on the flow entries. As described in in U.S. patent application Ser. No. 13 177 533 this conversion of LCP data to the LFP data and then to the CPCP data is performed by using an nLog engine.

Even though illustrates two physical controllers generating CPCP data from UPCP data for two different managed switching elements one of ordinary skill will realize that in other embodiment the physical controllers serve to simply relay the UPCP data to each switching element s chassis controller which in turn generates that switching element s CPCP data and pushes this data to it switching element.

The UI is an example interface through which the user can enter inputs and receive responses from a controller instance in order to manage the logical switches and routers. In some embodiments the UI is provided as a web application and thus can be opened up with a web browser. Alternatively or conjunctively the control application of some embodiments may allow the user to enter and receive inputs through a command line interface.

The left half of the figure illustrates that the user enters inputs to set up logical ports in logical switches and logical routers that are to be implemented by a set of managed switching elements of the network that the controller instance manages. In particular the user adds a logical port to a logical router LR by supplying at stage the port s identifier RP1 an IP address of 1.1.1.253 to associate with the port and a net mask 255.255.255.0. The user also adds a logical port to a logical switch LS1 by supplying at a port identifier SP1 and specifying that the port is to be connected to the logical port RP1 of the logical router. The user also adds another logical port to the logical router LR by supplying at stage the port s identifier RP2 an IP address of 1.1.2.253 to associate with the port and a net mask 255.255.255.0. The user also adds another logical port to the logical switch LS2 by supplying at a port identifier SP2 and specifying that the port is to be connected to the logical port RP2 of the logical router. The right half of the figure illustrates the ports added to the logical router and logical switches.

The control application as shown includes input translation input tables a rules engine output tables a exporter .

The input translation in some embodiments interacts with a management tool with which a user can view and or modify a logical network state. Different embodiments provide different management tools to the user. For instance the input translation in some embodiments provides a graphical tool such as the UI described above by reference to . Instead of or in conjunction with a graphical tool other embodiments may provide the user with a command line tool or any other type of management tool. The input translation receives inputs from the user through the management tool and processes the received inputs to create populate and or modify one or more input tables .

The input tables are similar to the input tables described in U.S. patent application Ser. No. 13 288 908 now issued as U.S. Pat. No. 9 043 452 which is incorporated herein by reference. An input table in some cases represents the state of the logical switches and the logical routers that the user is managing. For instance an input table is a table that stores IP addresses in classless inter domain routing CIDR format associated with logical ports of logical switches. The control application modifies input tables with inputs that the control application receives through the management tool or with any network events that the control application detects. After the control application modifies input tables the control application uses the rules engine to process the modified input tables.

The rules engine of different embodiments performs different combinations of database operations on different sets of input tables to populate and or modify different sets of output tables . For instance the rules engine modifies a table to associate a MAC address to a logical port of a logical router when the input table is changed to indicate that the logical port of the logical router is created. The output table includes flow entries that specify the actions for the managed switching elements that implement the logical switches and logical routers to perform on the network data that is being routed forwarded. In addition to the tables the rules engine may use other input tables constants tables and functions tables to facilitate the table mapping operation of the rules engine .

The output tables may also be used as input tables to the rules engine . That is a change in the output tables may trigger another table mapping operation to be performed by the rules engine . Therefore the entries in the tables may be resulted from performing table mapping operations and may also provide inputs to the rules engine for another set of table mapping operations. As such the input tables and the output tables are depicted in a single dotted box in this figure to indicate the tables are input and or output tables.

The table is for storing pairings of logical ports of logical routers and the associated MAC addresses. The table is a logical routing table for a logical router to use when routing the packets. In some embodiments the table will be sent to the managed switching element that implements the logical router. The table is for storing next hop identifiers and IP addresses for logical ports of logical routers. The table is for storing connections between logical ports of logical switches and logical ports of logical routers. The exporter publishes or sends the modified output tables in the output tables to a virtualization application .

The rules engine detects this update to the table and performs a set of table mapping operations to update the tables and . illustrates the result of this set of table mapping operations. Specifically this figure illustrates that the table has a new row which indicates that the logical port RP1 is now associated with a MAC address 01 01 01 01 01 01. This MAC address is generated by the rules engine while performing the table mapping operations using other tables or functions not shown .

The rules engine detects the update to the table and performs a set of table mapping operations to update the table . illustrates the result of this set of table mapping operations. Specifically this figure illustrates that the table has a new row which indicates that the IP address of the next hop for the logical port RP1 of the logical router is a given packet s destination IP address. 0 in this row means that the next hop s IP is the destination of the given packet that would be routed through RP1 of the logical router. 

The rules engine detects the updates to the table and performs a set of table mapping operations to update the table . illustrates the result of this set of table mapping operations. Specifically this figure illustrates that the table has a new row which indicates that the logical port SP1 is now associated with a MAC address 01 01 01 01 01 01 because SP1 and RP1 are now linked.

The rules engine detects the updates to the table and performs a set of table mapping operations to update the table . illustrates the result of this set of table mapping operations. Specifically this figure illustrates that the table has four new rows flow entries . The row is a flow entry indicating that packets whose destination MAC addresses is 01 01 01 01 01 01 are to be sent to the logical port SP 1 of the logical switch . The row is a flow entry indicating that any packet delivered to the logical port SP1 is to be sent to the logical port RP1. The row is a flow entry indicating that any packet delivered to the logical port RP1 is to be sent to the logical port SP1. The row is a flow entry indicating that a packet with an IP address that falls within the range of IP addresses specified by 1.1.1.253 24 should request for MAC address by asking an L3 daemon.

The new row indicates a logical port identified as RP2 is added and the IP addresses associated with this port is specified by the IP address 1.1.2.253 a prefix length 24 and the net mask 255.255.255.0. The new row which indicates that the logical port RP2 is now associated with a MAC address 01 01 01 01 01 02. The new row which indicates that the logical port SP2 is associated with a MAC address 01 01 01 01 01 02. The new row which is an entry in the routing table for the logical router . The row specifies that the next hop for the logical port RP2 has a unique identifier NH2. The row also includes a priority assigned to this row in the routing table.

The new row indicates that the IP address of the next hop for the logical port RP2 of the logical router is a given packet s destination IP address. The new row indicates that a logical port identified as SP2 of the logical switch is attached to the logical port RP2 of the logical router . Also the new row indicates that the logical port RP2 is attached to the logical port SP2.

The row is a flow entry indicating that packets whose destination MAC addresses is 01 01 01 01 01 02 are to be sent to the logical port SP2 of the logical switch . The row is a flow entry indicating that any packet delivered to the logical port SP2 is to be sent to the logical port RP2. The row is a flow entry indicating that any packet delivered to the logical port RP2 is to be sent to the logical port SP2. The row is a flow entry indicating that a packet with an IP address that falls within the range of IP addresses specified by 1.1.2.253 24 should request for MAC address by asking an L3 daemon.

These flow entries shown in are LFP data. This LFP data will be sent to the virtualization application which will generate UPCP data from the LFP data. Then the UPCP data will be sent to the physical controller which will customize the UPCP data for the managed switching element not shown in . Finally the physical controller will send the CPCP data to the managed switching element .

An example operation of the logical switches and the logical router and VMs 1 and 2 will now be described. This example assumes that a set of managed switching elements that implement the logical router and the logical switches and have all the flow entries and . This example also assumes that the logical data produced by the control application are converted to physical control plane data by the virtualization application and that the physical control plane data is received by the managed switching elements and converted into physical forwarding data.

When VM 1 intends to send a packet to VM 4 VM 1 first broadcasts an ARP request to resolve the logical router s MAC address. This ARP packet has a source IP address of VM 1 which is 1.1.1.10 in this example and a destination IP address of VM 4 which is 1.1.2.10 in this example. This broadcast packet has the broadcast MAC address ff ff ff ff ff ff as the destination MAC address and the packet s target protocol address is 1.1.1.253. This broadcast packet the ARP request is replicated to all ports of the logical switch including the logical port SP1. Then based on flow entry this packet is sent to RP1 of the logical router . The packet is then sent to an L3 daemon not shown according to the flow entry because the destination IP address 1.1.2.10 falls in the range of IP addresses specified by 1.1.2.253 24 i.e. because the target protocol address is 1.1.1.253 . The L3 daemon resolves the destination IP address to a MAC address 01 01 01 01 01 01 which is the MAC address of RP1. The L3 daemon sends the ARP response with this MAC address back to VM 1.

VM 1 then sends a packet to VM 4. This packet has VM 1 s MAC address as the source MAC address RP1 s MAC address 01 01 01 01 01 01 as a destination MAC address VM 1 s IP address 1.1.1.10 as the source IP address and VM 4 s IP address 1.1.2.10 as the destination IP address.

The logical switch then forwards this packet to SP1 according to the flow entry which indicates that a packet with the destination MAC address of 01 01 01 01 01 01 is to be sent to SP1. When the packet reaches SP1 the packet is then send to RP1 according to the flow entry which indicates that any packet delivered to SP1 to be sent to RP1.

This packet is then sent to the ingress ACL stage of the logical router which in this example allows the packet to go through RP1. Then the logical router routes the packet to the next hop NH2 according to the entry . This routing decision is then loaded to a register of the managed switching element that implements the logical router . This packet is then fed into the next hop lookup process which uses the next hop s ID NH2 to determine the next hop IP address and the port the packet should be sent to. In this example the next hop is determined based on the row which indicates that NH2 s address is the destination IP address of the packet and the port the packet should be sent to is RP2.

The packet then is fed into a MAC resolution process to resolve the destination IP address 1.1.2.10 to MAC address of VM 4. The L3 daemon resolves the MAC address and puts back a new flow entry e.g. by filling in a flow template with the resolved MAC address into the managed switching element that implements the logical router . According to this new flow the packet now has VM 4 s MAC address as the destination MAC address and the MAC address of RP2 01 01 01 01 01 02 of the logical router .

The packet then goes through the egress ACL stage of the logical router which in this example allows the packet to exit through RP2. The packet is then sent to SP2 according to the flow entry which indicates that any packet delivered to RP2 is to be sent to SP2. Then the L2 processing for the logical switch will send the packet to VM 4.

While all the LDPS processing is pushed to the managed edge switching elements only the interfaces to actual attached physical port integration address interoperability issues in some embodiments. These interfaces in some embodiments implement the standard L2 L3 interface for the host IP Ethernet stack. The interfaces between the logical switches and logical routers remain internal to the virtualization application and hence do not need to implement exactly the same protocols as today s routers to exchange information.

The virtualization application in some embodiments has the responsibility to respond to the ARP requests sent to the first hop router s IP address. Since the logical router s MAC IP address bindings are static this introduces no scaling issues. The last hop logical router in some embodiments does not have a similar strict requirement as long as the MAC and IP address es of the attached port are made known to the virtualization application it can publish them to the internal lookup service not exposed for the endpoints but only used by the logical pipeline execution. There is no absolute need to send ARP requests to the attached port.

Some embodiments implement the required L3 functionality as an external daemon running next to the Open vSwitch. In some embodiments the daemon is responsible for the following operations 

For generating ARP requests when integrating with external physical networks some embodiments assume that the packet can be dropped to the local IP stack by using the LOCAL output port of OpenFlow.

Mapping service itself is implemented in some embodiments by relying on the datapath functionality of the Open vSwitch daemons at the managed edge switching elements publish the MAC and IP address bindings by sending a special publish packet to the mapping service nodes which will then create flow entries using the flow templating. The query packets from the managed edge switching elements will be then responded to by these FIB entries which will send the packet to the special IN PORT after modifying the query packet enough to become a response packet.

Several embodiments described above and below provide network control systems that completely separate the logical forwarding space i.e. the logical control and forwarding planes from the physical forwarding space i.e. the physical control and forwarding planes . These control systems achieve such a separation by using a mapping engine to map the logical forwarding space data to the physical forwarding space data. By completely decoupling the logical space from the physical space the control systems of these embodiments allow the logical view of the logical forwarding elements to remain unchanged while changes are made to the physical forwarding space e.g. virtual machines are migrated physical switches or routers are added etc. .

More specifically the control system of some embodiments manages networks over which machines e.g. virtual machines belonging to several different users i.e. several different users in a private or public hosted environment with multiple hosted computers and managed forwarding elements that are shared by multiple different related or unrelated users may exchange data packets for separate LDP sets. That is machines belonging to a particular user may exchange data with other machines belonging to the same user over a LDPS for that user while machines belonging to a different user exchange data with each other over a different LDPS implemented on the same physical managed network. In some embodiments a LDPS also referred to as a logical forwarding element e.g. logical switch logical router or logical network in some cases is a logical construct that provides switching fabric to interconnect several logical ports to which a particular user s machines physical or virtual may attach.

In some embodiments the creation and use of such LDP sets and logical ports provides a logical service model that to an untrained eye may seem similar to the use of a virtual local area network VLAN . However various significant distinctions from the VLAN service model for segmenting a network exist. In the logical service model described herein the physical network can change without having any effect on the user s logical view of the network e.g. the addition of a managed switching element or the movement of a VM from one location to another does not affect the user s view of the logical forwarding element . One of ordinary skill in the art will recognize that all of the distinctions described below may not apply to a particular managed network. Some managed networks may include all of the features described in this section while other managed networks will include different subsets of these features.

In order for the managed forwarding elements within the managed network of some embodiments to identify the LDPS to which a packet belongs the network controller clusters automatedly generate flow entries for the physical managed forwarding elements according to user input defining the LDP sets. When packets from a machine on a particular LDPS are sent onto the managed network the managed forwarding elements use these flow entries to identify the logical context of the packet i.e. the LDPS to which the packet belongs as well as the logical port towards which the packet is headed and forward the packet according to the logical context.

In some embodiments a packet leaves its source machine and the network interface of its source machine without any sort of logical context ID. Instead the packet only contains the addresses of the source and destination machine e.g. MAC addresses IP addresses etc. . All of the logical context information is both added and removed at the managed forwarding elements of the network. When a first managed forwarding element receives a packet directly from a source machine the forwarding element uses information in the packet as well as the physical port at which it received the packet to identify the logical context of the packet and append this information to the packet. Similarly the last managed forwarding element before the destination machine removes the logical context before forwarding the packet to its destination. In addition the logical context appended to the packet may be modified by intermediate managed forwarding elements along the way in some embodiments. As such the end machines and the network interfaces of the end machines need not be aware of the logical network over which the packet is sent. As a result the end machines and their network interfaces do not need to be configured to adapt to the logical network. Instead the network controllers configure only the managed forwarding elements. In addition because the majority of the forwarding processing is performed at the edge forwarding elements the overall forwarding resources for the network will scale automatically as more machines are added because each physical edge forwarding element can only have so many machines attached .

In the logical context appended e.g. prepended to the packet some embodiments only include the logical egress port. That is the logical context that encapsulates the packet does not include an explicit user ID. Instead the logical context captures a logical forwarding decision made at the first hop i.e. a decision as to the destination logical port . From this the user ID i.e. the LDPS to which the packet belongs can be determined implicitly at later forwarding elements by examining the logical egress port as that logical egress port is part of a particular LDPS . This results in a flat context identifier meaning that the managed forwarding element does not have to slice the context ID to determine multiple pieces of information within the ID.

In some embodiments the egress port is a 32 bit ID. However the use of software forwarding elements for the managed forwarding elements that process the logical contexts in some embodiments enables the system to be modified at any time to change the size of the logical context e.g. to 64 bits or more whereas hardware forwarding elements tend to be more constrained to using a particular number of bits for a context identifier. In addition using a logical context identifier such as described herein results in an explicit separation between logical data i.e. the egress context ID and source destination address data i.e. MAC addresses . While the source and destination addresses are mapped to the logical ingress and egress ports the information is stored separately within the packet. Thus at managed switching elements within a network packets can be forwarded based entirely on the logical data i.e. the logical egress information that encapsulates the packet without any additional lookup over physical address information.

In some embodiments the packet processing within a managed forwarding element involves repeatedly sending packets to a dispatch port effectively resubmitting the packet back into the switching element. In some embodiments using software switching elements provides the ability to perform such resubmissions of packets. Whereas hardware forwarding elements generally involve a fixed pipeline due in part to the use of an ASIC to perform the processing software forwarding elements of some embodiments can extend a packet processing pipeline as long as necessary as there is not much of a delay from performing the resubmissions.

In addition some embodiments enable optimization of the multiple lookups for subsequent packets within a single set of related packets e.g. a single TCP UDP flow . When the first packet arrives the managed forwarding element performs all of the lookups and resubmits in order to fully process the packet. The forwarding element then caches the end result of the decision e.g. the addition of an egress context to the packet and the next hop forwarding decision out a particular port of the forwarding element over a particular tunnel along with a unique identifier for the packet that will be shared with all other related packets i.e. a unique identifier for the TCP UDP flow . Some embodiments push this cached result into the kernel of the forwarding element for additional optimization. For additional packets that share the unique identifier i.e. additional packets within the same flow the forwarding element can use the single cached lookup that specifies all of the actions to perform on the packet. Once the flow of packets is complete e.g. after a particular amount of time with no packets matching the identifier in some embodiments the forwarding element flushes the cache. This use of multiple lookups in some embodiments involves mapping packets from a physical space e.g. MAC addresses at physical ports into a logical space e.g. a logical forwarding decision to a logical port of a logical switch and then back into a physical space e.g. mapping the logical egress context to a physical outport of the switching element .

Such logical networks that use encapsulation to provide an explicit separation of physical and logical addresses provide significant advantages over other approaches to network virtualization such as VLANs. For example tagging techniques e.g. VLAN use a tag placed on the packet to segment forwarding tables to only apply rules associated with the tag to a packet. This only segments an existing address space rather than introducing a new space. As a result because the addresses are used for entities in both the virtual and physical realms they have to be exposed to the physical forwarding tables. As such the property of aggregation that comes from hierarchical address mapping cannot be exploited. In addition because no new address space is introduced with tagging all of the virtual contexts must use identical addressing models and the virtual address space is limited to being the same as the physical address space. A further shortcoming of tagging techniques is the inability to take advantage of mobility through address remapping.

The bus collectively represents all system peripheral and chipset buses that communicatively connect the numerous internal devices of the electronic system . For instance the bus communicatively connects the processing unit s with the read only memory the system memory and the permanent storage device .

From these various memory units the processing unit s retrieve instructions to execute and data to process in order to execute the processes of the invention. The processing unit s may be a single processor or a multi core processor in different embodiments.

The read only memory ROM stores static data and instructions that are needed by the processing unit s and other modules of the electronic system. The permanent storage device on the other hand is a read and write memory device. This device is a non volatile memory unit that stores instructions and data even when the electronic system is off. Some embodiments of the invention use a mass storage device such as a magnetic or optical disk and its corresponding disk drive as the permanent storage device .

Other embodiments use a removable storage device such as a floppy disk flash drive etc. as the permanent storage device. Like the permanent storage device the system memory is a read and write memory device. However unlike storage device the system memory is a volatile read and write memory such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments the invention s processes are stored in the system memory the permanent storage device and or the read only memory . From these various memory units the processing unit s retrieve instructions to execute and data to process in order to execute the processes of some embodiments.

The bus also connects to the input and output devices and . The input devices enable the user to communicate information and select commands to the electronic system. The input devices include alphanumeric keyboards and pointing devices also called cursor control devices . The output devices display images generated by the electronic system. The output devices include printers and display devices such as cathode ray tubes CRT or liquid crystal displays LCD . Some embodiments include devices such as a touchscreen that function as both input and output devices.

Finally as shown in bus also couples electronic system to a network through a network adapter not shown . In this manner the computer can be a part of a network of computers such as a local area network LAN a wide area network WAN or an Intranet or a network of networks such as the Internet. Any or all components of electronic system may be used in conjunction with the invention.

Some embodiments include electronic components such as microprocessors storage and memory that store computer program instructions in a machine readable or computer readable medium alternatively referred to as computer readable storage media machine readable media or machine readable storage media . Some examples of such computer readable media include RAM ROM read only compact discs CD ROM recordable compact discs CD R rewritable compact discs CD RW read only digital versatile discs e.g. DVD ROM dual layer DVD ROM a variety of recordable rewritable DVDs e.g. DVD RAM DVD RW DVD RW etc. flash memory e.g. SD cards mini SD cards micro SD cards etc. magnetic and or solid state hard drives read only and recordable Blu Ray discs ultra density optical discs any other optical or magnetic media and floppy disks. The computer readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code such as is produced by a compiler and files including higher level code that are executed by a computer an electronic component or a microprocessor using an interpreter.

While the above discussion primarily refers to microprocessor or multi core processors that execute software some embodiments are performed by one or more integrated circuits such as application specific integrated circuits ASICs or field programmable gate arrays FPGAs . In some embodiments such integrated circuits execute instructions that are stored on the circuit itself.

As used in this specification the terms computer server processor and memory all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification the terms display or displaying means displaying on an electronic device. As used in this specification the terms computer readable medium computer readable media and machine readable medium are entirely restricted to tangible physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals wired download signals and any other ephemeral signals.

While the invention has been described with reference to numerous specific details one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. In addition a number of the figures including conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations and different specific operations may be performed in different embodiments. Furthermore the process could be implemented using several sub processes or as part of a larger macro process. Thus one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details but rather is to be defined by the appended claims.

