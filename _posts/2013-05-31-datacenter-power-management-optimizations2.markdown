---

title: Datacenter power management optimizations
abstract: Methods and apparatus for datacenter power management optimization are disclosed. Metrics, including workload data, thermal measurements and the like are collected from numerous endpoints within a datacenter. System profiles of a plurality of servers, and application workload profiles for various workloads, are stored. Based on analysis of collected metrics, power optimization operations comprising either workload scheduling operations, power configuration change operations, or both, are initiated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09557792&OS=09557792&RS=09557792
owner: Amazon Technologies, Inc.
number: 09557792
owner_city: Reno
owner_country: US
publication_date: 20130531
---
More and more computing applications are being implemented using large data centers some of which may contain thousands of servers with different performance and power consumption characteristics. At any given data center time varying workloads from hundreds or thousands of clients may be executed and as a result the resource consumption levels may vary substantially over time. At some points in time a given subset of the computing resources housed a data center may be heavily used while at other times the same resources may be lightly used. Some provider networks may support numerous network accessible multi tenant services at dozens or hundreds of data centers distributed around the world where at least some of the services may rely on utilizing each other s resources making predictions about resource usage hard.

Power costs e.g. the costs of electrical power consumed by various computing devices and by the cooling infrastructure at the data center can comprise a substantial proportion of the operating expenses for data center maintenance. In addition to purely financial considerations data center operators may also wish to reduce power consumption as much as possible for environmental reasons while still providing the best possible service to the clients utilizing the data center. For at least some types of applications tradeoffs may be possible between power consumption at various computing devices which may correspond to the utilization levels of the devices and application performance.

Some data centers may house many different types of computing equipment and related infrastructure components and the equipment may typically change over time as may the cost of obtaining electrical power. The dynamic nature complexity and size of a data center s workload when combined with the complexity and size of the inventory may often complicate attempts to reduce or optimize power usage.

While embodiments are described herein by way of example for several embodiments and illustrative drawings those skilled in the art will recognize that embodiments are not limited to the embodiments or drawings described. It should be understood that the drawings and detailed description thereto are not intended to limit embodiments to the particular form disclosed but on the contrary the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope as defined by the appended claims. The headings used herein are for organizational purposes only and are not meant to be used to limit the scope of the description or the claims. As used throughout this application the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . Similarly the words include including and includes mean including but not limited to.

Various embodiments of methods and apparatus for implementing datacenter power management optimizations are described. The term datacenter as used herein refers to a facility whose primary purpose is to house computing devices and supporting infrastructure components. A typical large data center may for example comprise hundreds or thousands of computer servers storage servers and networking devices such as routers switches and the like redundant power sources such as various types of power generators communication connections air conditioning components as well as security safety devices. The various servers may be organized hierarchically in at least some datacenters e.g. the datacenter may include a number of rooms each room may include some number of racks and each rack may house some number of servers.

According to one embodiment a datacenter power optimization system may include one or more computing devices configured as a datacenter power manager DPM . The DPM may be configured to perform several different types of operations including for example monitoring and or collecting various types of metrics from a number of sources in the data center determining whether power optimization operations are feasible or advisable at a given point in time initiating the power optimization operations found and monitoring the impact of the optimizations. To help identify possible optimization operations a DPM database may be maintained in some embodiments to store datacenter inventory information location mappings for various computing devices of the datacenter e.g. indicating where a given device is physically located within the datacenter server and application workload profiles resource usage history power consumption history log records of previous optimization operations and the like.

In one embodiment the sources from which the DPM collects metrics may include a variety of network accessible endpoints e.g. implemented in software hardware or a combination of software and hardware located at various devices and infrastructure components of the datacenter. These sources may be termed power management endpoints PMEs herein. Several different types of PMEs may be instantiated in a given data center in such an embodiment including respective processes at various computer servers and or rack controllers PMEs that are associated with various components of the power distribution infrastructure such as PMEs at UPSs uninterruptible power supplies PDUs power distribution units generators and the like or PDUs in the cooling infrastructure. At least some types of PMEs may support programmatic control interfaces such as application programming interfaces that can be used by the DPM to initiate power configuration changes in one embodiment such as switching a particular power domain at a rack to a low power state from a normal power state or vice versa or switching between server power states e.g. using the Advanced Configuration and Power Interface or ACPI . PMEs may be configured to transmit metrics regarding temperature levels power usage application workload resource usage levels and the like to the DPM via one or more secure networking protocols in some embodiments e.g. in accordance with a predetermined schedule and or in response to requests from the DPM. Some PMEs may be configured to collect telemetry data from respective sets of sensors such as temperature sensors airflow sensors and the like. In addition to communicating with PMEs in some embodiments the DPM may communicate with management layers of various network accessible services being implemented at the data center e.g. in order to initiate workload changes or scheduling modifications intended to achieve power usage optimizations.

A DPM may periodically e.g. once every few seconds collect workload related and power related metrics from numerous PMEs distributed across the datacenter in some embodiments including server endpoints PDU endpoints rack endpoints and the like. The metrics may be aggregated together with previously collected metrics to determine workload trends thermal trends and or power usage trends at various components of the datacenter. The DPM may utilize the location mapping information stored in its database to determine the physical locations e.g. room numbers and or rack numbers of various servers and use server profiles and application workload profiles stored in the database to determine the power performance characteristics e.g. watts per operation of the various servers available. Using the combination of the information obtained from the database and the metrics received from the PMEs in some embodiments an optimization engine component of the DPM may be able at least at some points in time to identify one or more operations that could lead to short term and or long term reductions in power consumption at the datacenter. For example in some embodiments the DPM may be able to determine based on profile information from its database that transferring workload from one set of servers to another may enable a rack or a portion of a rack to be set to a low power or sleep state and or the level of cooling directed to that rack to be reduced which may reduce power consumed by the cooling infrastructure . Such changes could be made based on time window analysis in some embodiments. For example the DPM may detect a pattern in which a set of servers almost always has a high workload level during the time period 9 AM 5 PM but has a very low level of utilization during the time period 10 PM 4 AM. In this scenario it may be possible to run at least some servers at a low power state during the low utilization period e.g. by directing all the work requests during that period to a designated subset of servers that can be kept at normal power state. Similar power optimized workload redistributions may be initiated at any of several different granularities in some embodiments e.g. workloads may be migrated from one processing core to another within a single server that enables independent power state control for the different cores or from one server to another from one rack power domain to another rack power domain in the same rack from one rack to another or even from one data center to another.

In some embodiments the DPM may also be able to anticipate e.g. based on its knowledge of application workload patterns and or based on information from service managers of various services being implemented at the data center when increases in application workload levels are likely to occur. In such scenarios the DPM may be able to proactively bring up the power level of some set of servers that are currently in a low power state potentially leading to a reduction in the time taken to warm up to respond efficiently to client work requests associated with the applications.

In addition to operations that could reduce power consumption or that bring servers back to a high power state in anticipation of workloads in at least some embodiments the DPM may be able to proactively take actions to avoid or reduce the likelihood of maximum temperature limits being met at various points in the data center. For example if an analysis of PME metrics from a given server indicates that the probability of a thermal constraint e.g. a thermal design point or TDP constraint being violated at a given server exceeds a threshold and the DPM can determine a correlation between the workload level at the given server and the temperature the DPM may initiate the scheduling of workload away from the given server. Such proactive power optimization operations may prevent situations that could otherwise lead to server shutdown and corresponding disruptions of client applications. Proactive prevention of thermal constraint violations at various computing devices may also help to extend the lifetime of the devices in at least some embodiments.

According to one embodiment the DPM may be configured to classify servers and or other computing devices of the data center into categories based on the kinds of workloads that are most appropriate for the servers or devices e.g. from a performance per watt perspective. For example some types of servers with high end processing cores may be best suited for compute intensive tasks and may consume substantial amounts of power while other servers may consume less power while providing sufficient computing capacity for I O bound or memory bound workloads. In such embodiments the DPM may combine its knowledge of application workload characteristics with its knowledge of server characteristics to implement power efficient workload scheduling to achieve improved performance per watt characteristics e.g. those workloads that require high end processing may be directed to the high end servers while those workloads for which the performance capabilities of the low end servers are sufficient may be directed away from the high end servers. In contrast if a workload that is largely I O bound or memory bound is scheduled on a high end server the server may consume large amounts of power while waiting for I O operations to complete potentially resulting in a lower performance per watt metric than if the workload had been scheduled on lower end servers.

In at least one embodiment a set of hybrid servers may be installed at the data center whose power consumption characteristics may differ from those of the high end and low end servers. A given hybrid server may include both high end high power processing cores and low end lower power processing cores e.g. implementing the same instruction set as the high end cores in such an embodiment such that executable application instructions may be scheduled on either type of core depending on the compute versus I O needs or the compute versus memory needs of the application. In some embodiments shared memory modules may be accessed from either type of core in accordance with a cache coherent non uniform memory access ccNUMA architecture. In other embodiments the different types of processing elements at a hybrid server may be coupled using other techniques e.g. a loosely coupled architecture may be implemented in which more overhead is incurred when processes are migrated from one type of core to another. In scenarios in which the DPM s database does not contain profile information for a given application the workload of such an application may at least initially be scheduled at one or more hybrid servers. Components of the hybrid servers such as operating system components hardware or firmware components may initially schedule the execution of the instructions at one type of core e.g. the high end cores observe the behavior of the application e.g. how often the cores are waiting for I O or memory and then if appropriate schedule further execution at the other type of cores in one embodiment. Over time a profile of the application may be determined stored in the DPM database and used for subsequent workload associated with the application. Some applications may comprise different phases such as cycles of compute intensive phases followed by I O intensive phases in which case the hybrid servers may be considered optimal for scheduling the applications even after the characteristics of the applications are determined. In some embodiments an operating system component may be able to interact with lower level components at a hybrid server to implement fine grained power level transitions e.g. less power may be delivered to a core that is not being used heavily . Some types of applications may be scheduled initially on the hybrid servers for profile generation and scheduled on other classes of servers once their profiles are better understood.

Using the various types of optimizations including proactive operations to prevent overheating a DPM may be able to substantially improve power consumption per unit of application workload over time at a given datacenter in at least some embodiments while at the same time reducing outages that may result from thermal limits being exceeded. In some embodiments the optimization operations may be correlated with cost savings and patterns of optimization operations may be shared among DPMs of different data centers so that lessons learned over time about power efficiencies achievable at one data center can be used to optimize operations at other data centers as well. In one embodiment in which multiple datacenters are operated by the same business organization some of the optimizations may cross datacenter boundaries e.g. workloads or application instances may be migrated from one datacenter to another to achieve overall power consumption reductions.

A DPM database may be used to store various types of information usable to make power optimization decisions in the depicted embodiment. For example the database may include previously collected metrics from the different PMEs which may be useful for trend analysis inventory information listing the various types of computing and power related devices sensors and components at datacenter as well as location mappings indicating specifically where within the datacenter each device component or sensor is located e.g. at which rack a given server is installed . In at least some embodiments the DPM database may also be used to store at least two types of profile information server profiles indicating the performance and power consumption characteristics of different servers or server categories as well as application workload profiles characterizing the resource consumption patterns for different client applications that are executed at the servers . Server profiles may for example characterize different server models or brands based on their computation capabilities I O capabilities memory capabilities networking capabilities power consumption levels and the like. The profile information for a particular application may for example indicate the overall performance characteristics of the application e.g. whether in general the application is compute bound I O bound or memory bound whether there is a predictable time based variation in resource usage by the application e.g. whether a server at which the application runs is expected to have low utilization time windows and high utilization time windows in a predictable pattern the network bandwidth and latency requirements of the application and other similar data.

The DPM may be configured to try to achieve one or more power related goals in some embodiments such as an overall goal to reduce power consumption without affecting application performance by more than X a goal to reduce power related expenses by Y during the course of a year and or a goal to prevent more than P power related server shutdowns during a year. Based on some combination of the metrics received by the DPM from the PMEs and the information stored in its database the DPM may be able to identify opportunities for various types of power related optimizations and or opportunities for proactive preventive actions to avoid power related error or failure conditions in the depicted embodiment at various points in time. When it identifies such an opportunity the DPM may initiate one or more operations which may include for example workload scheduling changes e.g. directing incoming work requests to selected servers or migrating application instances or processes between different sets of servers to lower overall power consumption levels at the data center or to prevent overheating that may lead to shutdown and or power configuration changes e.g. setting the power state of a group of servers to deep sleep or other low power states . Arrows labeled e.g. B C D E F G and H indicate the direction in which commands or control signals to initiate various operations may be transmitted from the DPM in the depicted embodiment. The DPM may use one or more programmatic interfaces such as APIS supported by the various PMEs to transmit its requests or commands to initiate or perform various operations including for example various device type specific APIs or server supported APIs such as ACPI. It is noted that some PMEs may be configured primarily for metric collection and may not provide interfaces for initiating operations e.g. PME A in the depicted embodiment may be used primarily for verifying that the power sources are operating normally and may not be configured to accept any commands or instructions from the DPM . Other PMEs may be configured both for collecting and transmitting metrics to the DPM and receiving commands or instructions from the DPM while yet other PMEs may be configured primarily to accept commands from the DPM in some embodiments.

In some embodiments some of the operations initiated for power related optimizations by the DPM may involve the use of intermediaries such as service managers . A service manager may be responsible for distributing incoming workloads for applications related to a network accessible multi tenant service implemented at the datacenter in the depicted embodiment for migrating application instances or processes between servers and or for admission control decisions regarding incoming work requests e.g. to decide whether to accept or reject a work request from a client for a service . Accordingly if the DPM determines that the workload associated with a particular application should be directed to a selected server in some such embodiments the DPM may communicate the desired scheduling parameters to a service manager responsible for that application and the service manager may implement the requested workload distribution.

Metrics collectors may be responsible for gathering various types of measurements from PMEs in the depicted embodiment as indicated by arrow . In some embodiments a plurality of metrics collector modules may be implemented each responsible for obtaining metrics from a respective type of PME via a respective programmatic interface independently and asynchronously with respect to the other metrics collector modules. The parameters of metric collection may differ for different PMEs or even for different types of data collected from the same PME e.g. the rate at which workload data is collected from a particular server PME may differ from the rate at which thermal metrics are collected from that same server PME and both of those rates may differ from the rate at which airflow sensor data is collected from a cooling system s PME. Metrics collectors may store at least some of the collected metrics in the DPM database in some embodiments as indicated by the arrow labeled .

Optimization engine may be responsible for analyzing the metrics collected from the various PMEs together with the inventory location mappings server profiles and application workload profiles obtained from the DPM database as indicated by arrow and determining if and when various types of power optimization operations are to be attempted in view of the DPM s optimization goals. In some embodiments the optimization engine may utilize artificial intelligence techniques such as machine learning or pattern recognition techniques to identify the optimization opportunities and may be configured to build up a set of heuristics or rules for identifying the opportunities over time. In some embodiments the optimization engine may be configured to operate in cycles e.g. it may analyze the available data in a given cycle and if no optimization operations are found it may enter a dormant or sleep state for X seconds or minutes until the next cycle of analysis starts.

If the optimization engine determines that some set of optimization operations should be initiated workload command initiator s and or power configuration initiator s may be invoked. Workload command initiators may be responsible for issuing requests to modify application workload distribution to various servers e.g. either via communications C to some subset of PMEs or via communications A to service managers of the affected applications and services. Application workload distribution may include for example either directing work requests to selected servers migrating application processes or instances to different servers or both work request scheduling and application migration operations. Power configuration initiators may be responsible for changing power settings e.g. transitioning a rack or a power domain within a rack from a normal power state to a low power state or vice versa via communications B with the appropriate PMEs. In the depicted embodiment audit log records may be stored in the DPM database for some or all of the optimization operations initiated so that for example an analysis of the benefits achieved may be performed. In at least some embodiments audit log records may comprise historical records of power usage associated with various types of applications servers and the like and such historical records may be consulted by the DPM in some implementations to decide whether or when to implement a particular optimization. In various embodiments historical records may also be included in server profiles and or application profiles that are used for making optimization decisions. Such historical data may also be used by the DPM to improve its effectiveness over time in some embodiments e.g. using various types artificial intelligence techniques.

It is noted that in some embodiments some or all of the various components of the DPM shown in may be implemented on different computing devices e.g. the DPM functionality may be implemented in a distributed manner using a plurality of hardware and or software components. In other embodiments a single server may be responsible for the DPM functionality. Similarly the DPM database may also be implemented as a single standalone entity in some embodiments and as a distributed system in other embodiments. In some embodiments the DPM and or its database may be implemented in a high availability configuration e.g. with a plurality of servers and or storage devices set up such that single points of failure are eliminated.

In some embodiments the DPM may be configured to use trend analysis on the collected metrics and information about server and workload characteristics available in the DPM database to perform proactive operations that may help to reduce the chances of thermally induced application disruptions. illustrate an example of a proactive scheduling operation that may be initiated by a datacenter power manager according to at least some embodiments. In both the X axis represents time while the Y axis represents both temperature trends as well as workload level trends at a particular server . In the DPM has collected and performed trend analysis on workload information collected from time T through time T as indicated by the actual workload trend line . Similarly server temperature metrics collected from time T through time T indicate the trend illustrated by actual temperature trend line A. The DPM e.g. the optimization engine component of the DPM may determine that a strong correlation exists between the workload level and the temperature at the server. Server profile information about the server may indicate to the DPM that if the current temperature trends continue the maximum temperature thermal limit Tmax corresponding to the thermal design power TDP value for the server that the server can safely sustain may be reached at time Tk as indicated by the projected temperature trend line B.

Accordingly in the depicted embodiment the DPM may initiate a workload scheduling modification directing additional workload away from the server. For example the DPM may send a request for such workload redirection to one or more service managers responsible for assignment of incoming workload requests to servers or the DPM may send a command to a PME on the server to refuse to accept additional tasks. As a result the workload level at the server may stabilize as indicated by the workload trend line shown in . If the DPM s correlation analysis is accurate the temperature trend may also be modified as a result of the scheduling change as indicated by the actual temperature trend line C showing the measured temperature from time T through T. In this way the DPM may be able to prevent potential disruptions to client applications failures of client requests or even hardware replacement that may have resulted from a shutdown caused by the server exceeding its supported maximum temperature. Prevention or reduction of thermal stresses may also lead to an extension of the lifetime of the affected devices in some cases. Similar proactive operations may be performed using the DPM s optimization engine and database for other types of devices as well in some embodiments such as storage devices power distribution devices and the like. In some embodiments when the trends shown in are identified the DPM may initiate other operations in addition to the workload change described above e.g. the DPM may issue a command to a PME that can increase the cooling e.g. the airflow rate to the server.

In some embodiments the DPM may be able to generate substantial power savings by intelligently redistributing workloads and changing power settings accordingly. illustrate an example of a scheduling operation initiated by a datacenter power manager at a rack with a plurality of independently controlled power domains according to at least some embodiments. As shown in the power distribution components of a particular server rack may be organized as a collection of independently controllable power domains A B and C each with a respective domain PDU . Thus in the depicted embodiment power distribution to the servers A B C and D may be governed by domain PDU A with a corresponding PME A. Similarly power distribution to the servers E F G and H may be controlled by domain PDU B with PME B and power distribution to the servers I J K and L may be controlled by domain PDU C with PME C. Each of the domain PDUs A B and C may modify the power state of their respective power domain e.g. domain A B or C independently of the power state of the other domains in the depicted embodiment.

In for ease of presentation each of the depicted servers is shown executing respective amounts of workload of the same application or service expressed in units of operations per second ops sec . Metrics provided to the DPM by the server PMEs e.g. PMEs A B C D P F G H I J K and L indicate that as of a particular time or time period the respective workload at servers A B C D E F G H I J K and L is A ops sec B ops sec C ops sec D ops sec E ops sec F ops sec G ops sec H ops sec I ops sec J ops sec K ops sec and L ops sec. Furthermore the PMEs A B and C have indicated to the DPM that the power state of each of the power domains is NORMAL i.e. that the three power domains are each in a normal operating state with some amount of power being distributed to each of the servers based on the server s typical power consumption profile.

In accordance with a power use reduction goal and in view of the fact that power is independently controllable for each power domain the DPM may attempt to determine whether moving workload from some set of servers to a different set of servers can help to reduce the total power consumption in the three power domains shown in the depicted embodiment. For example the DPM may determine that several or all of the servers in power domain A and B are capable of handling more workload and that distributing the workload of the servers of power domain C among the servers of domains A and B may reduce the workload of power domain C sufficiently to allow domain C to be brought to a low power state without materially affecting the performance as perceived by the clients on whose behalf the operations are running and without substantial increases in the power consumption levels in domain A and B. In some embodiments the DPM may perform a multidimensional cost benefit analysis comparing the costs and benefits of transferring workload from power domain C to power domains A and B taking into account factors such as the total power consumed for the workload as a whole effects on client perceived performance overheads involved in the transfer risks of having to re transfer the workload again in the short term risks of encountering any of various system capability limits such as thermal limits likelihood that the overall workload is going to change substantially in the near future and so on.

In some datacenters several different types of servers may be available. In some scenarios it may be possible for the DPM to categorize the servers into groups with each group comprising servers with similar performance and or power consumption characteristics and to then initiate the assignment of application workloads to servers of the appropriate categories to achieve overall power usage reductions. illustrates example server profile categories that may be used by a DPM according to at least some embodiments. As shown servers are grouped into three categories in the depicted embodiment high end category A low end category B and a hybrid category C. Categories may also be referred to herein as capability categories . Information about several different characteristics of the servers of each category may be stored in the DPM database including processing elements available e.g. the number and specifications of general purpose or special purpose cores or processors power consumption levels application performance ratings and matching application types.

Servers in the high end capability category A are characterized as having N processing cores with clock speeds G1 GHz each C1 megabytes of cache memory and a symmetric multiprocessing architecture in the depicted embodiment. The typical power consumption level is P1 watts for category A and the servers of category A are best suited for compute intensive low latency low I O applications such as application A1 for which a performance level of X1 operations per second can be achieved. Servers in the low end capability category B are characterized as having two processors with clock speeds G2 GHz each C2 megabytes of cache memory and a symmetric multiprocessing architecture in the depicted embodiment. The typical power consumption level is P2 watts for low end category B and the servers of category B are best suited for low compute high I O applications such as application A2 for which a performance level of Y1 operations per second can be achieved.

The third capability category of servers hybrid servers C use a combination of high end processing cores and low end processing cores in a ccNUMA cache coherent non uniform memory architecture arrangement in the depicted embodiment. Thus each server of category C comprises N1 cores of clock speed G GHz and N2 cores of clock speed G2 GHz. The typical power consumption varies between P1 and P2 watts depending on the extent to which each type of cores is being used for example the hybrid server may make it possible to reduce power to cores of one type during time periods in which those cores are mostly idle. Similarly the performance capabilities of the hybrid servers may lie along the spectrum between the high end and low end servers. A range of between K1 and K2 operations per second of application A1 or a range of between L1 and L2 operations per second of application A2 is shown for the hybrid servers. As mentioned above hybrid servers may be appropriate for use for at least two types of application workloads applications with unknown or unpredictable resource consumption patterns or applications with distinct phases of compute intensive operations and I O intensive operations. Applications whose behavior i.e. in terms of compute versus I O or compute versus memory ratios is not well understood may be directed to hybrid servers and executed at either type of processing core based on the observed characteristics of the application until the resource consumption patterns of the application become better understood. If the resource consumption patterns indicate that the application can be characterized as compute intensive the application may be characterized as being best suited for high end servers of category A if the patterns indicate that the application can be characterized as low compute high I O the application may be designated as being suited for low end servers of category B. If the behavior and resource consumption patterns are unpredictable or comprise distinct phases with respect to compute intensive operations versus I O the application may be best suited for hybrid servers of category C. It is noted that any of several different types of hybrid server architectures may be implemented in different embodiments and that the scheduling of workloads to the different types of processing elements within a hybrid server may be performed at various levels and by various hardware and or software scheduling entities e.g. at the process level by operating system schedulers or at finer granularities such as blocks of executable instructions .

Using its knowledge of server performance and power profiles and its characterization of applications DPM may attempt to ensure that application workloads are directed to those servers to which the workload is best suited e.g. by issuing commands or directives to service managers . If the DPM s commands are implemented potential power wastage such as may occur if I O intensive workloads are directed to high end servers may be reduced or avoided in some embodiments. It is noted that in at least some embodiments a different number of server profile categories may be used than that shown in e.g. the DPM may categorize servers into more than three categories based on the types of hardware and software available at the data center .

The DPM may be configured to operate in cycles during each of which it performs some amount of analysis in an attempt to identify possible optimizations and initiates some or all of the feasible optimizations identified. As indicated in element during one of these cycles the DPM may collect the next set of metrics from the various PMEs in the datacenter e.g. via a secure encrypted protocol that defines metrics collection schedules for each of the metric types collected. The DPM may also be configured to track its own progress in achieving power related goals e.g. to determine whether sufficient optimization has already been achieved in a given time period. The collected metrics may be stored in a DPM database element e.g. together with location mappings for various devices in the datacenter s inventory server profiles application workload profiles and the like. The DPM e.g. using optimization engine may determine whether based on the analysis of collected metrics as well as database contents some set of power optimization operations is feasible or advisable.

If an appropriate power optimization operation is identified for implementation as detected in element the DPM may initiate the corresponding workload changes and or power configuration changes. Some power optimization operations may involve both workload modifications and power settings changes while other optimizations may involve just one of the two types of changes. If the optimization involves workload scheduling change s as detected in element the DPM may issue to corresponding scheduling command or requests to PMEs and or to service managers e.g. to redirect workload requests to a different set of servers element . If the optimization also involves power configuration changes as detected in element or involves only power configuration changes as also detected in element commands to make the appropriate changes such as changing power state at a power domain of a rack to low power or changing power state at a given processing core or CPU to low power may be issued to the appropriate PMEs element .

The DPM may perform the set of operations corresponding to elements for each identified optimization operation in the depicted embodiment. After all the optimization operations have been initiated the DPM may wait for the next set of metrics to be collected e.g. during its next cycle. If no optimization operations were found for implementation as detected in element the DPM may enter a dormant or sleep state until the next set of metrics arrive at which point the analysis and search for optimizations may be resumed.

If however information about the characteristics of the new workload is not available in the DPM database as also detected in element in the depicted embodiment the DPM may initiate the scheduling of the new work requests on some set of hybrid servers element similar to those described above with respect to . After the workload has been scheduled on the hybrid servers in some embodiments the DPM may monitor metrics collected from the hybrid servers to generate an application workload profile and store the profile in the DPM database for future use. The profile generated may indicate for example that the application A is compute intensive and is best suited to high end servers. Alternatively the profile generated may indicate that the application A is I O intensive or memory intensive and is therefore best suited to low end servers or that the application does not neatly fit into either the compute intensive or the I O intensive categories and so may be appropriate for hybrid servers at which power consumption may be optimized internally by switching instruction executions from one type of core to another.

If the trend analysis and or correlation computations indicate that a thermal limit such as a maximum ambient temperature is likely to be reached with some threshold probability P at the server unless some changes to workload or ambient conditions is made as detected in element the DPM may proactively initiate the redirection of additional workload to a different server element . In some embodiments other proactive actions may also or instead be taken such as increasing the cooling at the server and or transferring existing workload away from the server. If no danger of reaching a thermal limit is detected in operations corresponding to element the DPM may resume its collection of additional metrics and their analysis.

For any given set of source servers that are deemed to be underutilized the DPM may attempt to find a matching target servers to which the workload may be efficiently transferred or migrated for a net reduction in power consumption. Target servers may be identified based on server ownership e.g. workloads may only be transferred among servers that a given client owns or can be assigned in some embodiments their current utilization levels also represented in the metrics available to the DPM their profiles e.g. only servers that can perform the transferred operations with adequate performance may be selectable as targets and or the overhead associated with transferring the workload. If the DPM is able to find a target set of servers for workload consolidation element scheduling operations for the transfer of the workload may be initiated element e.g. application instances may be migrated by sending commands or requests via the appropriate programmatic interfaces to one or more service managers and or PMEs . After the workload is transferred the set of servers from which the workload was removed may be set to a low power state element . If no appropriate target set of servers is found in operations corresponding to element the DPM may resume searching for candidate source and target server sets for workload consolidation in the depicted embodiment. It is noted that at least in some embodiments the source and target sets of servers may overlap e.g. it may be possible to reduce overall power consumption by moving workload from one source server S to another source server S. In one embodiment DPMs at multiple data centers may collaborate to optimize overall power consumption for example some applications or workload may be migrated from one data center to another i.e. for a given source set of servers at data center DC a target set of servers may be found at a different data center DC.

The techniques described above of implementing various techniques for datacenter level power optimization based on metrics collected from various components and devices of the datacenter may be useful in a variety of different scenarios. For example such techniques may be especially beneficial in environments in which datacenters contain heterogeneous collections of servers e.g. due to inventory accumulated over the course of several years and in which the workloads or applications supported vary substantially in their compute needs and I O or memory needs. As power costs rise and or as the environmental impact of high power utilization levels increases these approaches may become even more valuable. As more control becomes possible over power consumption e.g. as server architectures that support selective programmable lowering of power to different components become more popular and as more intelligence is built in to the power distribution and cooling infrastructures in data centers the techniques described herein are likely to result in increased cost savings.

In at least some embodiments a server that implements a portion or all of one or more of the technologies described herein including the techniques to implement the various components of the DPM and or PMEs may include a general purpose computer system that includes or is configured to access one or more computer accessible media. illustrates such a general purpose computing device . In the illustrated embodiment computing device includes one or more processors coupled to a system memory via an input output I O interface . Computing device further includes a network interface coupled to I O interface .

In various embodiments computing device may be a uniprocessor system including one processor or a multiprocessor system including several processors e.g. two four eight or another suitable number . Processors may be any suitable processors capable of executing instructions. For example in various embodiments processors may be general purpose or embedded processors implementing any of a variety of instruction set architectures ISAs such as the x86 PowerPC SPARC or MIPS ISAs or any other suitable ISA. In multiprocessor systems each of processors may commonly but not necessarily implement the same ISA.

System memory may be configured to store instructions and data accessible by processor s . In various embodiments system memory may be implemented using any suitable memory technology such as static random access memory SRAM synchronous dynamic RAM SDRAM nonvolatile Flash type memory or any other type of memory. In the illustrated embodiment program instructions and data implementing one or more desired functions such as those methods techniques and data described above are shown stored within system memory as code and data .

In one embodiment I O interface may be configured to coordinate I O traffic between processor system memory and any peripheral devices in the device including network interface or other peripheral interfaces. In some embodiments I O interface may perform any necessary protocol timing or other data transformations to convert data signals from one component e.g. system memory into a format suitable for use by another component e.g. processor . In some embodiments I O interface may include support for devices attached through various types of peripheral buses such as a variant of the Peripheral Component Interconnect PCI bus standard or the Universal Serial Bus USB standard for example. In some embodiments the function of I O interface may be split into two or more separate components such as a north bridge and a south bridge for example. Also in some embodiments some or all of the functionality of I O interface such as an interface to system memory may be incorporated directly into processor .

Network interface may be configured to allow data to be exchanged between computing device and other devices attached to a network or networks such as other computer systems or devices as illustrated in through for example. In various embodiments network interface may support communication via any suitable wired or wireless general data networks such as types of Ethernet network for example. Additionally network interface may support communication via telecommunications telephony networks such as analog voice networks or digital fiber communications networks via storage area networks such as Fibre Channel SANs or via any other suitable type of network and or protocol.

In some embodiments system memory may be one embodiment of a computer accessible medium configured to store program instructions and data as described above for through for implementing embodiments of the corresponding methods and apparatus. However in other embodiments program instructions and or data may be received sent or stored upon different types of computer accessible media. Generally speaking a computer accessible medium may include non transitory storage media or memory media such as magnetic or optical media e.g. disk or DVD CD coupled to computing device via I O interface . A non transitory computer accessible storage medium may also include any volatile or non volatile media such as RAM e.g. SDRAM DDR SDRAM RDRAM SRAM etc. ROM etc. that may be included in some embodiments of computing device as system memory or another type of memory. Further a computer accessible medium may include transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as a network and or a wireless link such as may be implemented via network interface . Portions or all of multiple computing devices such as that illustrated in may be used to implement the described functionality in various embodiments for example software components running on a variety of different devices and servers may collaborate to provide the functionality. In some embodiments portions of the described functionality may be implemented using storage devices network devices or special purpose computer systems in addition to or instead of being implemented using general purpose computer systems. The term computing device as used herein refers to at least all these types of devices and is not limited to these types of devices.

Various embodiments may further include receiving sending or storing instructions and or data implemented in accordance with the foregoing description upon a computer accessible medium. Generally speaking a computer accessible medium may include storage media or memory media such as magnetic or optical media e.g. disk or DVD CD ROM volatile or non volatile media such as RAM e.g. SDRAM DDR RDRAM SRAM etc. ROM etc. as well as transmission media or signals such as electrical electromagnetic or digital signals conveyed via a communication medium such as network and or a wireless link.

The various methods as illustrated in the Figures and described herein represent exemplary embodiments of methods. The methods may be implemented in software hardware or a combination thereof. The order of method may be changed and various elements may be added reordered combined omitted modified etc.

Various modifications and changes may be made as would be obvious to a person skilled in the art having the benefit of this disclosure. It is intended to embrace all such modifications and changes and accordingly the above description to be regarded in an illustrative rather than a restrictive sense.

