---

title: Using shared virtual memory resources for performing memory-mapping
abstract: Functionality is described herein for memory-mapping an information unit (such as a file) into virtual memory by associating shared virtual memory resources with the information unit. The functionality then allows processes (or other entities) to interact with the information unit via the shared virtual memory resources, as opposed to duplicating separate private instances of the virtual memory resources for each process that requests access to the information unit. The functionality also uses a single level of address translation to convert virtual addresses to corresponding physical addresses. In one implementation, the information unit is stored on a bulk-erase type block storage device, such as a flash storage device; here, the single level of address translation incorporates any address mappings identified by wear-leveling and/or garbage collection processing, eliminating the need for the storage device to perform separate and independent address mappings.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09612973&OS=09612973&RS=09612973
owner: Microsoft Technology Licensing, LLC
number: 09612973
owner_city: Redmond
owner_country: US
publication_date: 20131109
---
In a technique referred to as memory mapping a computer system maps some portion of a file or other information item into virtual memory. The computer system then accesses the file via the virtual memory. More specifically each application process which interacts with a memory mapped file will create its own private instance of virtual memory resources for use in interacting with the file. For instance each process will create its own copy of page tables for use in interacting with the file. A process will destroy its private instance of the resources when it is finished using them.

Consider the particular case in which the computer system uses DRAM in conjunction with a flash storage device or other bulk erase type block storage device to interact with a file. In that case the computer system first consults the page tables associated with a process to determine whether a specified virtual address is currently resident in main memory. If not the computer system may use a file system to map the specified address to a location in secondary storage. A flash translation layer provided by the flash storage device next converts the address provided by the file system to the actual physical location at which the desired data item is stored in the flash storage device. Hence the computer system performs three levels of address translation when it encounters a page fault.

Functionality is described herein for memory mapping an information item such as a file into virtual memory by associating shared virtual memory resources with the information unit. The functionality then allows processes or other entities to interact with the information unit via the shared virtual memory resources as opposed to duplicating separate private instances of the virtual memory resources for each process that requests access to the information unit. In view of this behavior the shared virtual memory resources may be considered as system wide resources for use in interacting with the information unit rather than process specific resources.

According to one implementation the shared virtual memory resources include one or more self contained memory SCoMe units each associated with a particular information unit e.g. a file . Each SCoMe unit identifies one or more SCoMe virtual memory regions. Each SCoMe unit further provides a leaf page table for each SCoMe virtual memory region. The functionality uses the leaf page table to map a virtual address within an associated virtual memory region to a corresponding physical address.

According to another aspect each entry in a leaf page table of a SCoMe unit specifies a physical address. Control information provided by the leaf page table indicates whether the physical address corresponds to a location in main memory e.g. DRAM or an extended memory device e.g. a flash storage device . Hence the functionality can perform a single level of address translation to map a virtual address specified by a process to an actual physical location at which a desired data item is stored in a physical storage device.

According to another aspect the functionality may integrate wear leveling and garbage collection processes and or other management process es into the virtual memory resources. As a result of this feature the above referenced single level of address translation may also take into account wear leveling and or garbage collection considerations.

As a whole the functionality offers a more efficient manner of memory mapping an information unit to virtual memory compared to traditional methods. The improved efficiency ensues in part from a reduction in the number of page tables that are used in memory mapping an information unit coupled with a more streamlined manner of converting virtual addresses to physical addresses. For instance the functionality can allow a computer system to integrate the functions performed by a virtual memory mechanism a file system and a flash translation layer into a single virtual memory address translation mechanism.

The above approach can be manifested in various types of systems components methods computer readable storage media data structures graphical user interface presentations articles of manufacture and so on.

This Summary is provided to introduce a selection of concepts in a simplified form these concepts are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

The same numbers are used throughout the disclosure and figures to reference like components and features. Series numbers refer to features originally found in series numbers refer to features originally found in series numbers refer to features originally found in and so on.

This disclosure is organized as follows. Section A describes illustrative functionality by which processes or other entities can interact with memory mapped information units e.g. files via shared virtual memory resources. Section B sets forth illustrative methods which explain the operation of the functionality of Section A. Section C describes illustrative computing functionality that can be used to implement any aspect of the features described in Sections A and B.

As a preliminary matter some of the figures describe concepts in the context of one or more structural components variously referred to as functionality modules features elements etc. The various components shown in the figures can be implemented in any manner by any physical and tangible mechanisms for instance by software running on computer equipment hardware e.g. chip implemented logic functionality etc. and or any combination thereof. In one case the illustrated separation of various components in the figures into distinct units may reflect the use of corresponding distinct physical and tangible components in an actual implementation. Alternatively or in addition any single component illustrated in the figures may be implemented by plural actual physical components. Alternatively or in addition the depiction of any two or more separate components in the figures may reflect different functions performed by a single actual physical component. to be described in turn provides additional details regarding one illustrative physical implementation of the functions shown in the figures.

Other figures describe the concepts in flowchart form. In this form certain operations are described as constituting distinct blocks performed in a certain order. Such implementations are illustrative and non limiting. Certain blocks described herein can be grouped together and performed in a single operation certain blocks can be broken apart into plural component blocks and certain blocks can be performed in an order that differs from that which is illustrated herein including a parallel manner of performing the blocks . The blocks shown in the flowcharts can be implemented in any manner by any physical and tangible mechanisms for instance by software running on computer equipment hardware e.g. chip implemented logic functionality etc. and or any combination thereof.

As to terminology the phrase configured to encompasses any way that any kind of physical and tangible functionality can be constructed to perform an identified operation. The functionality can be configured to perform an operation using for instance software running on computer equipment hardware e.g. chip implemented logic functionality etc. and or any combination thereof.

The term logic encompasses any physical and tangible functionality for performing a task. For instance each operation illustrated in the flowcharts corresponds to a logic component for performing that operation. An operation can be performed using for instance software running on computer equipment hardware e.g. chip implemented logic functionality etc. and or any combination thereof. When implemented by computing equipment a logic component represents an electrical component that is a physical part of the computer system however implemented.

The following explanation may identify one or more features as optional. This type of statement is not to be interpreted as an exhaustive indication of features that may be considered optional that is other features can be considered as optional although not expressly identified in the text. Finally the terms exemplary or illustrative refer to one implementation among potentially many implementations.

In contrast in a traditional approach the first and second entities would create two separate instances of virtual memory resources. The first entity would then interact with the information units using its private version of the virtual memory resources while the second entity would interact with the information units using its private version of the virtual memory resources. More concretely stated each entity would create and utilize a separate copy of a set of page tables by which the entity translates virtual addresses to physical addresses. In this traditional approach then each separate instance of virtual memory resources may be considered as owned by its hosting entity. The computer system destroys each private version of the virtual memory resources once its hosting entity is finished using it.

The term entity as used herein is intended to have broad connotation. It may refer to a process that uses the virtual memory resources such as a process provided by an application or some other functional module within a computer system. Or an entity may correspond to a virtual machine guest etc. Nevertheless to facilitate and simplify explanation the ensuing description makes reference to the particular case in which entities correspond to processes.

Each process may also access private virtual memory resources. For example the first process may interact with first private virtual memory resources while the second process may interact with second private virtual memory resources . Each process may utilize its private virtual memory resources to interact with non memory mapped data items. Further each process may utilize portions of its private virtual memory resources to link to the shared virtual memory resources as will be clarified below.

The strategy shown in may have a number of technical advantages over the traditional approach. First the computer system may use fewer virtual memory resources compared to the traditional approach since it allows all processes to use the same virtual memory resources when interacting with the information units . More concretely stated the computer system may produce a reduced number of page tables compared to the traditional approach particularly in the scenario in which an information unit contains a large number of information items. Second the computer system may leverage the virtual memory resources to simplify the manner in which it translates virtual addresses to physical address essentially using the virtual memory resources to perform all address mappings in the computer system including address mapping traditionally associated with wear leveling and or garbage collection processes. Traditional approaches have used multiple mechanisms within the computer system to perform this translation all with their respective mapping schemes. The following description will clarify the nature of both of these potential benefits.

In addition the strategy shown in can continue to offer traditional benefits associated with virtual memory. For example a computer system can rely on the virtual memory resources to 1 reduce or eliminate the need by individual processes to separately manage their access to physical memory 2 provide isolation between processes 3 and give each process the illusion that it has access to more physical memory in DRAM than is actually available to it etc.

Similarly the second process process accesses physical memory via private virtual memory using one or more non shared page tables . The computer system uses the non shared page tables to convert virtual addresses that lie within the range of the private virtual memory to physical addresses corresponding to locations with the physical memory . The combination of the private virtual memory and the corresponding non shared page tables may be considered as private or non shared virtual memory resources that are accessible to the second process.

Both the first and second processes may also access physical memory via shared virtual memory using one or more shared page tables . The computer system uses the shared page tables to convert virtual addresses that lie within the range of the shared virtual memory to physical addresses corresponding to locations with the physical memory . In general the combination of the shared virtual memory and the corresponding shared page tables may be considered as shared virtual memory resources that are accessible to any process that runs on the computer system. In the context of the explanation of the physical memory may store any information unit e.g. any file or any portion of an information unit that has been memory mapped into the shared virtual memory .

A management system creates and maintains the shared virtual memory resources and governs the behavior of the shared memory resources. to be explained below provides additional illustrative details regarding one implementation of the management system .

Processes may interact with information units e.g. files as memory mapped resources by mapping these information units to the shared virtual memory . More specifically the computer system may define a plurality of self contained memory SCoMe units each of which mediates access to a particular information unit e.g. a particular file or some part thereof. Each SCoMe unit in turn is made up of one or more virtual memory regions. Each virtual memory region identifies a portion of the shared virtual memory . The virtual memory regions may be contiguous or non contiguous with respect to each other.

Further a SCoMe unit provides a leaf page table associated with each virtual memory region. Each leaf page table identifies a plurality of entries such as in one implementation 512 entries. Each entry in turn maps a particular virtual address that falls within the associated virtual memory region into a corresponding physical address. The physical address identifies the location at which a data item associated with the virtual address is physically located on a storage device.

For example the SCoMe shown in includes one or more virtual memory regions carved out from the shared virtual memory . SCoMe further includes corresponding leaf page tables . For instance a particular leaf page table provides entries for mapping virtual addresses to physical addresses for virtual addresses that falls within a particular virtual memory region . The SCoMe as a whole is associated with a particular information unit or portion thereof such as a file not shown as such the SCoMe provides a vehicle by which any process may interact with the information unit as a memory mapped entity. Each SCoMe unit is considered a system owned resource because it is not owned by any particular process.

Each virtual memory region may have a size that corresponds to some integral multiple of the virtual memory coverage of a leaf level page table. In one case for example the virtual memory region may have a size which is a multiple of 2 MB. The shared virtual memory can also designate a contiguous or non contiguous free region that contains shared virtual memory that is not currently assigned to any SCoMe unit. The management system can allocate chunks of virtual memory in units of 2 MB from this free region to any SCoMe unit e.g. to accommodate the scenario in which a process which interacts with the SCoMe unit dynamically requests additional virtual memory.

The information units themselves are physically stored on one or more physical storage devices . The physical storage devices include main memory which may be implemented as dynamic random access memory DRAM . The physical storage devices may also include one or more secondary storage devices which are treated as extended portions of main memory and therefore referred to herein as at least one extended memory device . For example one type of extended memory device may be implemented as a solid state drive SSD . The solid state drive may use any storage technology such as any technology which manages information in units of blocks in a log structured manner. These kinds of devices are referred to herein as bulk erase block storage devices examples of these devices include but are not limited to devices which use NAND based flash storage technology phase change memory PCM and so on. The physical storage devices may also include one or more other storage devices such as a hard disk drive. In general the computer system can more quickly and efficiently retrieve data items from main memory compared to other kinds of storage mechanisms.

The management system introduced in manages the shared virtual memory resources. Managing as the term is used here encompasses several functions including but not limited to creating maintaining and retiring the shared virtual memory resources handling access by application processes to the shared virtual memory resources and handling interaction with data items stored on the physical storage devices .

A SCoMe interaction module may serve as the main agent which mediates access to the shared virtual memory resources. For instance when the computer system boots up the SCoMe interaction module may define the shared virtual memory selected from the total swath of available virtual memory . Each active process may then reserve the shared virtual memory as part of its virtual memory. Each active process uses the shared virtual memory to interact with any memory mapped information unit that it opens in the course of its operation.

For example assume that a first process executes an instruction to open a file as a memory mapped resource. In response the SCoMe interaction module creates a corresponding SCoMe unit for use by the process in interacting with the file such as SCoMe shown in . The SCoMe interaction module also returns a handle identifier to the process for use by the process in subsequently interacting with the SCoMe unit. The process of creating a SCoMe unit involves selecting one or more virtual memory regions from the shared virtual memory and producing leaf page tables corresponding to those regions. Next assume that a second process executes an instruction to also open the same file as a memory mapped resource. The SCoMe interaction module associates the second process with the same SCoMe unit that has already been created without duplicating the SCoMe unit. The manner in which the SCoMe interaction module links the second process to the existing SCoMe unit will be described in greater detail below.

The SCoMe interaction module may store information which defines the shared virtual memory resources in a data store . For example the data store may store directory information which identifies the available SCoMe units. The directory information may use any data structure to represent the available SCoMe units such as a tree data structure. The SCoMe information may also identify the virtual memory regions and leaf page tables associated with each SCoMe unit.

As stated the SCoMe units constitute shared resources. The management system may employ other logic to produce non shared page tables in an on demand manner. For example a process may rely on the other logic to produce page tables so as to interact with other data items outside the scope of its interaction with memory mapped files. The process may also rely on the other logic to produce higher level pages which link to the leaf page tables in the manner described below .

Presume now that both the first and second process end or otherwise terminate their interaction with the memory mapped file. The SCoMe interaction module can address this situation in different ways corresponding to different respective implementations. In one case the SCoMe interaction module may retire the SCoMe unit corresponding to the now unused memory mapped file . Retirement may constitute deleting the information associated with the SCoMe unit such as its leaf page tables or moving the information from the main memory to a secondary storage device such as the extended memory device and or some other action. The SCoMe interaction module can also free up the portion of shared virtual memory that was previously used by the SCoMe unit being retired. The SCoMe interaction module can perform these clean up tasks at any juncture. In one case the SCoMe interaction module performs the above described cleanup tasks immediately once it is detected that no process is currently using a SCoMe unit. In another case the SCoMe interaction module performs the tasks when its workload falls below a prescribed threshold and or when the amount of available memory falls below a prescribed threshold.

In another case the SCoMe interaction module may delay removal of the unused SCoMe unit for a prescribed amount of time. The SCoMe interaction module delays removal based on the possibility that a process may soon again request interaction with this SCoMe unit and its corresponding memory mapped file. Once the delay period has expired the SCoMe interaction module can remove the SCoMe unit in any manner described above e.g. by removing the SCoMe unit immediately or when the SCoMe interaction module is idle and or when the amount of available memory falls below a prescribed threshold.

The SCoMe interaction module can create and manage the SCoMe units in different ways. In one approach the SCoMe interaction module may establish a management process. In operation when an application process opens an information unit for the first time as a memory mapped resource the SCoMe interaction module may assign a corresponding SCoMe unit to the management process. The SCoMe interaction module may then link the application process with the leaf page tables of the SCoMe unit without actually duplicating the SCoMe unit and its leaf page tables. In other words the management process serves as a kind of holding receptacle for holding all SCoMe units that have been created by application processes and not yet retired. The above approach represents one non limiting implementation among other possible implementations in another case for instance the SCoMe interaction module can associate each SCoMe unit with the application process which results in its creation that is the application process which first opens a corresponding information unit as a memory mapped resource.

Further the SCoMe interaction module can provide a management SCoMe unit to store all of the SCoMe information provided in the data store . That is the management SCoMe unit stores metadata which defines the other SCoMe units. The management SCoMe unit may otherwise have the structure described above being made up of one or more virtual memory regions and corresponding leaf page tables. The data items associated with the management SCoMe unit may be physically stored on any storage device or combination of storage devices such as the extended memory device .

An entity interaction module may allow the application processes to interact with memory mapped information units via the SCoMe units. The entity interaction module can perform this interaction via one or more different types of application programming interfaces APIs . A first set of APIs can allow an application to interact with a SCoMe unit in the manner in which a file system interacts with its files. For example the entity interaction module can use these APIs to open an information unit read from an information unit write to an information unit determine status information pertaining to an information unit and so on. But the entity interaction module works in a different way than a traditional file system because a traditional file system does not leverage virtual memory in the manner described above . For example the entity interaction module can perform reading and writing operations by copying data items between a user specified buffer and the virtual memory of an identified SCoMe.

With respect to the above described first type of API the entity interaction module may represent each SCoMe unit to each application process as a single continuous unit even though the virtual memory regions inside a SCoMe unit may not in fact be contiguous. In operation the entity interaction module can address the potential fragmentation of virtual memory within a SCoMe unit by translating a virtual address specified by an application process to an appropriate virtual address within the SCoMe unit being referenced. The entity interaction module can perform this operation using any per SCoMe translation scheme such as a sparse array translation mechanism. Since the virtual memory regions are relatively small in size e.g. 2 MB in one implementation the overhead of this translation mechanism is relatively small. Moreover the entity interaction module can produce this translation mechanism in an on demand fashion.

In a second type of API the entity interaction module includes logic which allows an application process to map a SCoMe unit to a single continuous virtual memory region that is separate from the default virtual memory location of the SCoMe unit. An application process may wish to employ this function so that it can interact with the SCoMe unit and its corresponding memory mapped information unit in a different manner from other application processes. The entity interaction module can produce the above result by associating the new virtual memory region with the leaf page tables of the default SCoMe unit. That is the entity interaction module links to the new virtual memory region with the existing page tables of the default SCoMe unit rather than duplicating those leaf page tables.

In a third type of API the entity interaction module can allow application processes to create persistent heaps using SCoMe units. The entity interaction module performs this task by dynamically allocating memory from a specified SCoMe unit. If the specified SCoMe unit does not have sufficient memory to satisfy such an allocation request it can request the SCoMe interaction module to assign more memory to the SCoMe unit. The SCoMe interaction module can allocate chunks of free virtual memory to SCoMe units in the above described granularity e.g. 2 MB in one non limiting case.

By virtue of the use of the third type of API a SCoMe unit that is used as a persistent heap may be mapped to the same virtual memory region across two or more application processes. This capability provides a way by which processes can share virtual memory without serializing the data items that they share.

The above described three types of APIs are cited by way of example not limitation. The entity interaction module can accommodate the use of yet other ways of interacting with SCoMe units and the information units that they represent.

A memory interaction module allows application processes to interact with the physical storage devices including the main memory and the extended memory device . As one function the memory interaction module converts virtual addresses identified by application processes to physical addresses. The physical addresses correspond to locations on a specified physical storage device. The memory interaction module can rely on hardware lookup logic provided by the computer system e.g. provided by the CPU of the computer system . The memory interaction module performs this operation using a set of page tables. With respect to a particular application process the set of page tables may encompass one or more process specific non shared page tables . If the application process is currently performing an operation that targets a SCoMe unit then the page tables will also include one or more leaf page tables of this SCoMe unit as maintained in the data store . to be described below provides additional information regarding one way in which the memory interaction module may use one or more page tables to convert a virtual address to a physical address.

A memory management module provided by the memory interaction module can also manage data items in the main memory . As stated above the main memory may correspond to DRAM. The memory management module can use any retention strategy such as a least recently used LRU strategy to identify data items in the main memory that are least needed by the current application processes at a current point in time and to move those data items to the extended memory device . The memory management module replaces the removed data items with other data items pulled from the extended memory device which are more urgently needed by the application processes at the current point in time. The memory management module can also synchronize dirty entries in the main memory with corresponding entries in the extended memory device using known techniques such that updates made to virtual memory are reflected in the persisted version of the data items.

In the following presume that the extended memory device represents a log structured storage device such as a bulk erase type block storage device or more particularly a flash storage device that uses any flash based storage technology to store data items such as NAND based storage technology. Each storage location in this type of storage device accommodates a limited number of writes after which the device can no longer be used. Further a flash storage device cannot write a data item to a storage location unless that storage location has been erased in advance. A typical flash storage device erases storage locations in units of blocks. A block is typically larger than a page of virtual memory.

A wear leveling module determines the locations at which data items are to be written in a flash storage device or other kind of bulk erase type block storage device . The wear leveling module attempts to distribute the use of storage locations in the flash storage device in a generally even manner. This approach reduces the possibility that a flash storage device will use up the write cycle capacity of some storage locations while other storage locations remain much less utilized thereby prematurely exhausting the usable lifetime of the flash storage device as a whole. The wear leveling module can use any wear leveling strategy to perform this task such as a dynamic wear leveling strategy a static wear leveling strategy and so on.

A garbage collection module also manages the locations at which data items are written and rewritten in a flash storage device so as to facilitate subsequent erasure of blocks of data items. The garbage collection module applies any garbage collection strategy to perform this task. Generally the garbage collection module attempts consolidate active data items within the blocks of flash storage device thereby freeing up unused blocks for erasure.

In one implementation the management system is implemented by an operating system which runs on the computer system . The wear leveling module and the garbage collection module can be implemented in different ways. In one implementation the computer system can implement the wear leveling module and or the garbage collection module as part of the operating system . For instance the management system itself can implement these modules. In another implementation the wear leveling module and or the garbage collection module may be implemented at least in part by control logic provided by the extended memory device which as said may represent a flash storage device. In any case the memory interaction module performs a single level of address translation that integrates any address mappings identified by the wear leveling process and or the garbage collection process.

More generally stated the extended memory device may rely on a management process to determine locations at which data items are to be written in the extended memory device . The management process takes into account the particular characteristics of the extended memory device and may include wear leveling and or garbage collection as described above but is not limited thereto. Any component or plural components within the management system may implement the management process. The memory interaction module performs a single level of address translation that takes into account any address mappings identified by the management process.

An illustrative virtual address may include entry identifying information that identifies a leaf page table as well as a particular entry e.g. entry in the leaf page table . In one case the entry identifying information provides an identifier which directly identifies a particular leaf page table . In another case the leaf page table corresponds to a leaf node in a hierarchal tree of higher level page tables such as higher level page tables . In this case the entry identifying information provides a set of identifiers which identify a sequence of one or more ancestor parent tables which lead to the leaf page table starting with a root node that points to a top level directory page table. Each entry in a non leaf page table points to a child page table.

In the context of computer system of a leaf page table that is associated with a virtual memory region of a SCoMe unit is a system wide feature. Any process can link to this leaf page table. A leaf page table that is not associated with a SCoMe unit is a private feature associated with the process itself and destroyed when it is no longer being used by the process. In either case according to one implementation the higher level page tables of a process may represent non shared resources of the process such as higher level page tables .

A process can link to a virtual memory region of a SCoMe unit by modifying an appropriate entry in a higher level page table to point to the leaf page table of that virtual memory region. For example assume that a higher level page table of a process serves as the parent page table of the leaf page table . An entry in the higher level page table can be set to point to the leaf page table . Another process which uses the same virtual memory region of the SCoMe unit can include an entry in a higher level page table which points to the same leaf page table .

The above implementation is set forth by way of illustration not limitation. In another implementation a SCoMe unit may encompass at least one higher level page table e.g. corresponding to the parent page table of the SCoMe unit s leaf page tables. In that case the leaf page tables and the higher level page tables are considered shared system wide resources. A process can link to two or more virtual memory regions in a SCoMe unit by linking to the shared parent page table in the SCoMe unit which in turn links to the desired virtual memory regions. In yet another case the computer system can specify linking information through other lookup mechanisms e.g. besides the use of higher level page tables.

The illustrative entry shown in specifies a physical address . The physical address in turn corresponds to a physical address within the main memory e.g. a DRAM or a physical address within the extended memory device e.g. a flash storage device . Among other roles control information in the entry specifies whether the physical address is associated with the main memory or the extended memory device .

The memory interaction module uses the identified physical address together with offset information specified in the virtual address to access a particular data item within a physical storage device. More specifically the physical address specifies a physical starting address within the physical storage device. The offset information identifies an offset location relative to the starting address.

Assume that a data item is currently resident in main memory . As noted above the physical address specifies the location of the data item in main memory . Although not depicted in the main memory can also store the physical address at which the same data item is persisted in the extended memory device providing an alternative physical address. Assume next that the memory interaction module removes the data item from the main memory such that it is now only resident in the extended memory device . The memory interaction module can then access the alternative physical address from main memory and insert it into the entry for the data item. More specifically the memory interaction module may save alternative address information only for data items that are currently resident in the main memory so the overhead of this address retention is relatively small.

From a more general standpoint note that the memory interaction module uses the page tables of the virtual memory resources to perform all address translation that is used to map a virtual address to a physical address where the virtual memory resources include the shared leaf tables in conjunction the linking non shared higher level tables. As noted above that single level of address translation encompasses any device specific address mappings identified by the wear leveling module the garbage collection module and or any other device specific management process. In contrast as explained above a traditional approach may resort to three tiers of address translation to perform this task. In a first level of the traditional process the virtual memory uses the page tables to convert the specified virtual address to a physical address providing that the desired data item is stored in the main memory. But if the desired data item is not in the main memory then the computer system may use a file system translation mechanism e.g. using a B tree mechanism or the like to convert the virtual address to a storage location on a secondary storage device. Providing that the secondary storage device is a flash storage device a controller of the flash memory device may use its translation layer to convert a logical address as specified by the file system to an actual final location in the flash storage device.

In comparison to the traditional approach the computer system described herein can more efficiently access a desired data item using fewer translation resources. In other words the computer system integrates the roles of a virtual memory file system and flash translation layer into the virtual memory resources. This is made possible in part through the stabilization of certain virtual memory resources making them system wide assets rather than fleeting process specific resources which are destroyed along with their hosting processes.

The leaf page table may internally specify permission information for each of its entries. Generally permission information governs the access rights of a process with respect to a data item or a group of data items in any manner e.g. by indicating that a particular data item or group of data items are read only data items read and write permitted data items no access data items and so on. In one implementation any process which links to the shared leaf page table is bound by the permission information specified in the leaf page table . This is because the leaf page table is a shared resource rather than a process specific resource.

False sharing occurs when the page table of a process allows it to access a SCoMe unit when such a process has not opened or cannot otherwise use the SCoMe unit. In one implementation the management system prevents false sharing in part by preventing SCoMe units from sharing leaf page tables. Further higher level page tables of a process are not shared. Further a higher level page table that directly links to a leaf page table only points to a single virtual memory region in a single SCoMe unit.

The computer system may provide other ways of specifying permission information. For example the first process can specify first permission information using its higher level page tables and the second process can specify second permission information using its higher level page tables . On this level the permission information can specify access rights with respect to all of the data items referenced by the leaf page table . If the access rights of a high level page table conflicts with the access rights specified in the child leaf page table then the higher level permission information may override the leaf level permission information. The first permission information may differ from the second permission information in any manner. Or the first permission information may be the same as the second permission information .

Assume that a second process also wishes to make use of the leaf page table in order to interact with a portion of a memory mapped file. But assume that the second process desires to assign custom permissions to only a subset of the data items represented by the leaf page table . Assume that a part of the leaf page table corresponds to that subset of data items. To accomplish that objective the second process can direct the management system to copy the part yielding a forked leaf page table . That is the forked leaf page table contains only the entries in the part . The management system then associates custom permission information with the forked leaf page table which differs from the original permission information in any manner. The second process uses one or more higher level page tables to link to the forked leaf page table and is therefore subject to the custom permission information rather than the original permission information . The second process may also interact with the original leaf page table with respect to other data items specified therein. The forked leaf page table is considered a system owned resource just as the original leaf page table rather than a process specific resource. As such one or more other processes can also link to the forked leaf page table as a shared resource.

A process B employs non shared virtual memory resources . In state 3 the process B process requests access to the memory mapped file via its associated SCoMe unit. In response the management system links process B to the shared virtual memory resources in the manner set forth above. Overall process A is currently utilizing virtual memory resources enclosed in the dashed line box while process B is utilizing virtual memory resources enclosed in the dashed line box . The boxes intersect to indicate that that parts of the virtual memory resources are shared by the two processes.

After storing the new data item the extended memory device informs the management system of the physical address at which it has stored the data item. The SCoMe interaction module can then insert that physical address in an appropriate entry in an appropriate leaf page table thereby updating the leaf page table so that it can subsequently map a virtual address to a correct physical address for the data item in question. To perform this task the extended memory device can preserve knowledge of the virtual memory address associated with the new data item . For example the extended memory device can store a virtual address associated with the new data item at any location within the device. The extended memory device may then report the virtual address along with the selected physical address to the management system . The management system uses the virtual address to locate the appropriate entry in the appropriate leaf page table for updating. As a result of this technique the leaf page table implicitly incorporates the outcome of the wear leveling process garbage collection process and or any other management process used by the flash storage device.

In another case the memory interaction module can write a set of data items in bulk mode to the extended memory device . The extended memory device may respond by sending back a vector to the management system that specifies plural physical addresses and plural corresponding virtual addresses.

In another scenario assume that the extended memory device already stores a data item at a first storage location in the extended memory device . It may move the data item to a second storage location in the extended memory device based on any consideration or combination of considerations. The extended memory device reports the new physical address to the management system in the same manner explained above e.g. by leveraging a stored virtual address associated with the moved data item to locate the appropriate entry in the appropriate leaf page table.

This section describes the operation of the computer system in flowchart form. Since the principles underlying the operation of the computer system have already been described in Section A certain operations will be addressed in summary fashion in this section.

More specifically block can entail creating a SCoMe unit associated with a file. Block may entail linking the second entity to the SCoMe unit without producing a new instance of the SCoMe unit. More concretely stated both the first and second entities link to and share the leaf page tables associated with the SCoMe unit rather than creating separate private instances of the leaf page tables.

The computing functionality can include one or more processing devices such as one or more central processing units CPUs and or one or more graphical processing units GPUs and so on.

The computing functionality can also include any storage resources for storing any kind of information such as code settings data etc. Without limitation for instance the storage resources may include any of RAM of any type s ROM of any type s bulk erase type block storage devices e.g. flash storage devices hard disks optical disks and so on. For example the storage resources can include the types of physical storage devices described above.

More generally any storage resource can use any technology for storing information. Further any storage resource may provide volatile or non volatile retention of information. Further any storage resource may represent a fixed or removal component of the computing functionality . The computing functionality may perform any of the functions described above when the processing devices carry out instructions stored in any storage resource or combination of storage resources. For example the computing functionality can perform the functions of the operating system including the management system when the processing devices carry out computer instructions that implement the functions of the operating system .

As to terminology any of the storage resources or any combination of the storage resources may be regarded as a computer readable medium. In many cases a computer readable medium represents some form of physical and tangible entity. The term computer readable medium also encompasses propagated signals e.g. transmitted or received via physical conduit and or air or other wireless medium etc. However the specific terms computer readable storage medium and computer readable medium device expressly exclude propagated signals per se while including all other forms of computer readable media.

The computing functionality also includes one or more drive mechanisms for interacting with any storage resource such as a hard disk drive mechanism an optical disk drive mechanism and so on.

The computing functionality also includes an input output module for receiving various inputs via input devices and for providing various outputs via output devices . Illustrative input devices include a keyboard device a mouse input device a touchscreen input device a digitizing pad one or more video cameras one or more depth cameras a free space gesture recognition mechanism one or more microphones a voice recognition mechanism any movement detection mechanisms e.g. accelerometers gyroscopes etc. and so on. One particular output mechanism may include a presentation device and an associated graphical user interface GUI . Other output devices include a printer a model generating mechanism a tactile output mechanism an archival mechanism for storing output information and so on. The computing functionality can also include one or more network interfaces for exchanging data with other devices via one or more communication conduits . One or more communication buses communicatively couple the above described components together.

The communication conduit s can be implemented in any manner e.g. by a local area network a wide area network e.g. the Internet point to point connections etc. or any combination thereof. The communication conduit s can include any combination of hardwired links wireless links routers gateway functionality name servers etc. governed by any protocol or combination of protocols.

Alternatively or in addition any of the functions described in the preceding sections can be performed at least in part by one or more hardware logic components. For example without limitation the computing functionality can be implemented using one or more of Field programmable Gate Arrays FPGAs Application specific Integrated Circuits ASICs Application specific Standard Products ASSPs System on a chip systems SOCs Complex Programmable Logic Devices CPLDs etc.

In closing the description may have described various concepts in the context of illustrative challenges or problems. This manner of explanation does not constitute a representation that others have appreciated and or articulated the challenges or problems in the manner specified herein. Further the claimed subject matter is not limited to implementations that solve any or all of the noted challenges problems.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

