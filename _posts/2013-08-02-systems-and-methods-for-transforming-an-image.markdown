---

title: Systems and methods for transforming an image
abstract: Systems, methods, and computer-readable media acquire an image captured with a mobile device. Motion sensor data of the mobile device at or near a time when the image was captured is acquired. An angle of rotation is computed based on the motion sensor data, and the image is transformed based on the angle of rotation. In another aspect, a user interface enables user control over image transformation. The user interface enables user control over rotating an image on a display at two or more granularities. A point of rotation may be user-defined. Rotated images may be scaled to fit within a viewing frame for displaying the transformed image.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09177362&OS=09177362&RS=09177362
owner: Facebook, Inc.
number: 09177362
owner_city: Menlo Park
owner_country: US
publication_date: 20130802
---
The present invention relates to the field of image processing. More particularly the present invention provides techniques for transforming images to account for tilt.

Today people have many options available to take pictures. In addition to traditional cameras dedicated exclusively to capturing images in film or digital form many mobile devices such as smartphones have the capability to take pictures. For example a mobile device may include a camera and image capturing software that enables the user to take pictures with the camera on the mobile device. These images may be stored to the mobile device and viewed at a later time. In addition some internet social networks provide users with the ability to share photos online. Members of these photo sharing social networks may take pictures with their mobile devices and upload the pictures to the photo sharing social network for others to see.

Images can be modified after they are captured. For example the mobile device or a system supporting the social network may provide filters or other types of editing tools to allow manual alteration of the image. Sometimes using these editing tools the owner of the image may choose to modify an image to provide aesthetic improvements. In other instances the editing tools may allow the owner of the image to modify other aspects of the image that are deemed undesirable.

In various aspects of the present disclosure systems methods and computer readable media are provided that acquire an image captured with a mobile device. Motion sensor data of the mobile device at or near a time when the image was captured is also acquired. An angle of rotation is computed based on the motion sensor data and the image is transformed based on the angle of rotation.

In an embodiment the transformation of the image includes rotating the image based on the computed angle of rotation. The transformation of the image may include scaling the image based on the dimensions of a viewing frame of a user interface of the mobile device.

In an embodiment the acquisition of motion sensor data includes sampling data from a gyroscope. The sampled data may be filtered by a low pass filter to reduce noise.

In an embodiment the acquisition of motion sensor data further includes sampling data from an accelerometer on the mobile device.

In an embodiment a user input element to allow a user to select automatic transformation of the image is provided and a selection for automatic transformation of the image is received. The transformation of the image is responsive to receipt of the selection.

In an embodiment the acquired image is displayed on a display of the mobile device before the image is transformed. The transformed image is displayed instead of the acquired image on the display after the image is transformed.

In an embodiment the angle of rotation is compared to a predetermined threshold angle. The transformed image is displayed when the angle of rotation does not exceed the predetermined threshold angle.

In an embodiment a user interface is displayed comprising a first control element enabling user controlled rotation of the transformed image at a first granularity. The first user input applied to the first control element is received and the transformed image is rotated at the first granularity based on the first user input.

In an embodiment a second control element enabling user controlled rotation of the transformed image at a second granularity in the user interface is displayed. A second user input applied to the second control element is received and the transformed image is rotated at the second granularity based on the second user input.

In an embodiment movement of the first control element and the second control element in a first direction from a reference position rotates the image in a first rotational direction and movement of the first control element and the second control element in a second direction from the reference position rotates the image in a second rotational direction.

In an embodiment a user interface enables user definition of a point of rotation about which to rotate the transformed image. A first user input applied to the user interface to define a point of rotation is received. A second user input applied to the user interface to define a direction and a granularity of rotation is received. The transformed image is rotated about the point of rotation according to the direction and the granularity of rotation based on the first user input and the second user input.

Many other features and embodiments of the invention will be apparent from the accompanying drawings and from the following detailed description.

The figures depict various embodiments of the present invention for purposes of illustration only wherein the figures use like reference numerals to identify like elements. One skilled in the art will readily recognize from the following discussion that alternative embodiments of the structures and methods illustrated in the figures may be employed without departing from the principles of the invention described herein.

Images captured by cameras including cameras built in mobile devices may suffer from certain shortcomings. The shortcoming may reflect irregularities in the image capture process and other factors. For example when the camera is tilted during the time of image capture the resulting captured image may reflect the tilt of the camera. The tilt may occur when the person controlling the camera positions the camera in a manner that does not conform to image alignment expectations. A tilted image may be aesthetically undesirable or unfaithful to the reality of the content depicted in the image.

A camera may be disposed within a mobile device. In an embodiment a tilt of the mobile device may be based on a deviation of an axis of the mobile device from a reference axis. When the longitudinal axis or latitudinal axis of the mobile device is not aligned i.e. parallel with a vertical reference axis or horizontal reference axis the mobile device may be considered to be tilted. A mobile device that is tilted during image capture may cause the captured image to appear tilted when displayed within a viewing frame of the mobile device.

It should be appreciated that while a tilt of the mobile device may be described in the present disclosure as based on a deviation of the longitudinal axis of the mobile device from a vertical reference axis a tilt of the mobile device also may be based on a deviation of the latitudinal axis of the mobile device from the vertical reference axis. For example when the latitudinal axis or longitudinal axis of the mobile device is not aligned i.e. parallel with a vertical reference axis or horizontal reference axis the mobile device may be considered to be tilted.

To address the shortcomings described above various embodiments of systems methods and computer readable media of the present disclosure may involve utilization of mobile device capabilities to remedy the occurrence of tilted images. Mobile devices may include a motion sensor that monitors a mobile device s movement and orientation. Motion sensors may for instance measure acceleration forces and rotational forces along the x y and z axes. Example motion sensors may include for example gyroscopes and accelerometers. Motion sensor data generated from one or more motion sensors may be used to determine a tilt or inclination of the mobile device relative to a reference axis.

Images captured with a mobile device as well as motion sensor data of the mobile device reflecting the position of the mobile device at or near a time when the image was captured may be acquired. An angle of rotation necessary to correct the tilted image may then be computed based on the motion sensor data and the image may be transformed based on the angle of rotation.

For example a mobile device may include a camera and image capture software that enables a user to take a picture with the camera on the mobile device. If the user takes the picture while holding the mobile device at a tilt then the captured image will be tilted accordingly when displayed within a viewing frame on the mobile device. Embodiments of the present disclosure enable the user to transform e.g. rotate the image to straighten the image or otherwise correct for the tilt of the mobile device and resulting image. In this way for instance a captured image may be straightened before storing the image on the mobile device or uploading it to a remote device such as a remote server of a social networking system that enables users to store and share photos online. In one embodiment the image capture software may be part of a mobile application for a social networking system which a user may download to a mobile device to enable the user to capture images and post them online to the social networking system. The tilt correction based on the motion sensor may be included as part of the mobile application for instance and enable a user to correct the tilt of a captured image. In one embodiment the tilt correction may occur automatically without user confirmation after the image is acquired. In another embodiment the tilt correction may require user initiation or confirmation before transforming the image.

The camera is disposed on the mobile device and enables a user to capture an image with the mobile device . The camera may for instance include a lens and image sensor that is used to generate image data representing an image e.g. a picture taken by the camera . For example the image sensor may include but is not limited to a charge coupled device CCD image sensor or a complementary metal oxide semiconductor CMOS sensor which captures light and converts it to electrical signals. Additional electronic circuitry may be included to convert the electrical signals from the image sensor to digital signals representing the image.

The image capture module operates in conjunction with the camera to enable the capture of an image. For example image capture module may control the operation of the camera during image capture. The image capture module may also for example generate a user interface or viewfinder to allow the user to take a picture using the camera on the mobile device .

The motion sensor is disposed on the mobile device and provides motion sensor data of the mobile device . The term motion sensor data is used broadly herein and may refer generally to data regarding movement position orientation speed etc. The motion sensor may include an accelerometer a gyroscope or any other type of tool or resource that can produce motion sensor data. For example the motion sensor may be based on acceleration forces e.g. from an accelerometer rotational forces along the three axes e.g. from a gyroscope a combination of acceleration forces and rotational forces etc. It should be appreciated that the motion sensor is not intended to be limited to a single motion sensor but may encompass more than one motion sensor e.g. an accelerometer and a gyroscope .

The image transformation module transforms a captured image by rotating the captured image and in some instances additionally scaling the image. In one embodiment the transformation is based on motion sensor data derived from the motion sensor . After acquiring the captured image and motion sensor data of the mobile device at or near a time when the mobile device captured the image the image transformation module determines an angle of rotation necessary to rotate the captured image to correct for the tilt of the mobile device when the image was captured. For example the image transformation module may use the application programming interface API for the operating system of the mobile device to receive and process the motion sensor data. For example a framework for the operating system of the mobile device may enable an application such as an application that implements embodiments of tilt correction of the present disclosure to receive motion sensor data from device hardware and process that data.

In an embodiment the image transformation module may specify a reference frame and the orientation or motion of the mobile device may then be compared to that reference frame by the motion sensor . The motion sensor data may vary in form in different embodiments e.g. vector raw acceleration value angular value relative to one or more reference axes etc. The tilt of the mobile device may be determined based on the motion sensor data and used to calculate the angle of rotation necessary to correct the tilted image. In an embodiment the image transformation module may specify a single axis of the reference frame and only the motion sensor data with respect to the single axis is used to determine the tilt of the mobile device and to transform tilted images.

In an embodiment the image transformation module provides control elements on a user interface to enable user control over image transformation. Based on the received user input applied through the control elements the image is rotated and scaled accordingly. In another embodiment the image transformation module may transform images based on motion sensor data as well as provide the control elements to enable user control over the transformation. For example the image transformation module may first transform the image based on motion sensor data and thereafter transform the image based on user input applied through the user interface or vice versa.

The display may be based on any variety of technologies such as liquid crystal display LCD organic light emitting diode OLED electronic paper etc. In one embodiment the display is a touchscreen display that enables user input to be received via the display.

The processing device is operably coupled to the camera the motion sensor and the display and operates in conjunction with the image capture module and the image transformation module to perform the various techniques described in the present disclosure. The term processing device is used broadly herein and may include one or more processors microprocessors microcontrollers etc. Additional details regarding example processing devices are described in .

It should be appreciated that one or more functional components may be combined into a single component. For example the image capture module and the image transformation module may combined into a single module. In some instances the image capture module and the image transformation module may be entirely or partially embodied in software and or hardware on the mobile device at the time of manufacturing. In some instances for example the image capture module and the image transformation module may be entirely or partially embodied in a software program e.g. a mobile application that is downloadable to the mobile device . Furthermore in some instances the image capture module and the image transformation module may work in conjunction with or be part of a social networking system that enables images to be shared online.

The sampler is configured to sample motion sensor data generated by the motion sensor e.g. gyroscope . For example the sampler may sample the motion sensor data at a predetermined sampling rate such as 30 samples per second. It should be appreciated that other sampling rates may be implemented.

The filter is configured to receive and filter the motion sensor data sampled from the sampler . For example in an embodiment the filter may be a low pass filter that filters out high frequency noise to generate a cleaner signal. For example in one embodiment a Kalman filter may be implemented. Averaging techniques or other smoothing algorithms may be implemented to smooth out and make the data signal more stable e.g. reducing noise in the motion sensor data.

The motion sensor data repository is configured to receive and store the motion sensor data from the sampler and the filter . The motion sensor data repository may include for example one or more volatile e.g. RAM SRAM etc. and non volatile memory e.g. NVRAM or Flash memory hard drive etc. which may store the motion sensor data for retrieval at a later time. In some instances the motion sensor data repository may be maintained in a more temporary form of memory such as a data buffer. For example the data buffer may be configured to hold a set amount of motion sensor data and when the mobile device enters into an image capture mode the sampler begins sampling the motion sensor data which may then be filtered by the filter and received by the data buffer .

The angle determination module is configured to receive the motion sensor data from the motion sensor data repository and to determine the tilt of the mobile device which in turn may determine the angle of rotation necessary to correct the image that is captured while the mobile device is tilted. For example the angle determination module may receive the motion sensor data from the motion sensor data repository and compute the tilt of the mobile device when the image was captured based on the motion sensor data providing the angle of rotation to correct the tilted image.

The timing module is used to determine the appropriate motion sensor data on which to base computation of the tilt. For example the timing module may be used to determine a set of motion sensor data that is used compute the tilt. In an embodiment the timing module identifies when the mobile device enters an image capture mode e.g. activates a photo capturing user interface for the user to take a picture and when the image is captured e.g. the user takes the picture . In such case for instance the angle determination module bases the angle determination on the entire set of motion sensor data that is sampled and filtered from the time the image capture mode is entered until the time of image capture. In an embodiment the angle determination module may base the angle determination on other time periods or the occurrences of other events.

In an embodiment the sampler may begin sampling the motion sensor data when the timing module indicates that the image capture mode has been entered. In an embodiment the sampler may already be sampling the motion sensor data and the timing module identifies the first sample of the set of motion sensor data as the sample corresponding to the time the image capture mode is entered.

In an embodiment the sampler may stop sampling the motion sensor data when the timing module indicates that an image has been captured. In an embodiment where the sampler continues sampling after an image has been captured the timing module may identify the last sample of the set e.g. the sample corresponding to the time the image was captured.

In another embodiment the timing module may identify the appropriate motion sensor data to use for the angle determination based on a predetermined time period or number of samples before i.e. near in time to the capture of an image. The angle determination module uses the corresponding set of motion sensor data within the predetermined time period to compute the tilt and to determine the angle of rotation to correct the tilted image. For example when a user captures an image the last 30 samples from the time of capture may be used in the angle determination. Similarly for example the last 2 seconds of sampled motion sensor data from the time of image capture may be used for the angle determination. It should be appreciated that the predetermined time and number of samples are exemplary and are not intended to be limiting. Other predetermined times and samples may be implemented in other embodiments. It should also be appreciated that a single sample may also be used to compute the angle of rotation e.g. the single sample corresponding to the time the image was captured. Furthermore it should be appreciated that other methods to determine the specific set of motion sensor data of the angle of rotation computation may be implemented in other embodiments.

The rotation module the scaling module and the framing module work in conjunction to rotate and scale the image within a viewing frame of a user interface that displays the captured and transformed images. The rotation module rotates the captured image based on the angle of rotation determined by the angle determination module . For example the rotation module may use the angle of rotation to create a rotation transformation e.g. in the form of a matrix for the operating system to use to rotate the image accordingly.

The scaling module may scale the image based on the angle of rotation determined to correct the tilt for the captured image. The scaling module may determine a scaling factor based on dimensions of a viewing frame that is displayed by the framing module . For example the image may be scaled to avoid the creation of empty spaces within the viewing frame when the image is rotated. In one embodiment the scaling module scales the rotated image by the minimum amount necessary to remove spaces in the viewing frame. In an embodiment the framing module may modify the size of the viewing frame based on the angle of rotation. For example if enlarging an image to remove empty space in the viewing frame would render the image unclear e.g. if the image has low resolution the framing module may shrink the viewing frame to remove the empty space otherwise created when the image is rotated.

In the embodiment shown the image transformation module also includes the user controlled rotation module . The user controlled rotation module provides a user interface that enables user control over the rotation of the image. In this way for example the user may correct a tilted image by manually rotating the image accordingly.

In an embodiment the user controlled rotation module provides a user interface that enables the user to control the rotation of the image according to more than one level of granularity. For example the user interface may include a first control element that enables user controlled rotation of an image at a first granularity and a second control element that enables user controlled rotation of the image at a second granularity. The second granularity may for instance rotate the image at a slower rate than the first granularity to enable more precise rotation by the user. In an embodiment the first control element and the second control element are simultaneously displayed as dials that may be moved relative to a reference position to rotate the image clockwise or counterclockwise respectively. Based on user input received for the first control element and the second control element the user controlled rotation module rotates the image based on the first granularity and second granularity respectively within the viewing frame provided by framing module .

While the embodiment shown includes both the tilt determination module and the user controlled rotation module it should be appreciated that in other embodiments either the tilt determination module or the user controlled rotation module may be implemented without the other.

At block motion sensor data is acquired for the mobile device at or near the time of capture. For example in an embodiment the image transformation module acquires motion sensor data that is based on the tilt of the mobile device during the time the image was captured. For instance when the user enters an image capture mode on the mobile device the sampler begins sampling the motion sensor data from the gyroscope or other motion sensor implemented on the mobile device . The sampled motion sensor data may then be filtered and stored e.g. by the filter and the motion sensor data repository . In an embodiment the motion sensor data is sampled filtered and stored until the user takes the picture. The amount of motion sensor data to be sampled filtered or stored as well as the time period over which the motion sensor data is to be sampled filtered or stored may be determined in various manners.

At block an angle of rotation is computed based on the motion sensor data. For example in an embodiment the angle determination module shown in determines an angle of rotation to correct the tilt of the image. The angle determination module receives the motion sensor data from the motion sensor data repository and determines the tilt of the mobile device. The tilt may determine the angle of rotation to rotate the image to correct for the tilt of the mobile device at the time the image was captured.

In one embodiment the angle of rotation is compared to a predetermined threshold before transforming the image. The predetermined threshold for instance may represent a threshold at which a tilt of the image is assumed to be intended by the user e.g. artistically or otherwise intentionally tilted. If the angle of rotation to correct a tilted image exceeds the predetermined threshold then the image is not rotated to correct for tilt. For example the threshold may be 45 degrees or any other predetermined threshold. In some instances the predetermined threshold may be selected by a social networking system that maintains the image a user who uploaded or will view the image or both. In some instances information is provided to the user to indicate that the image rotation necessary to correct for the tilt exceeds the threshold. In some instances user confirmation or verification is requested to determine if image transformation should be performed regardless of the angle of rotation being greater than the threshold.

At block the image is transformed based on the computed angle of rotation. For example the rotation module of generates a rotation transformation to rotate the captured image by an angle of rotation computed by the angle determination module . In an embodiment the captured image is also scaled based on the angle of rotation. For example the scaling module of determines the scaling factor necessary to scale the captured image to fill a viewing frame that is displayed on a user interface.

At block the transformed image is displayed. For example the framing module of may operate in conjunction with the rotation module scaling module and the image capture program to render the transformed image within a viewing frame of a user interface that is presented on the display of the mobile device. In some instances the captured image is displayed in the viewing frame and subsequently replaced by the transformed image.

In an embodiment the user may save or otherwise store the transformed image into memory on the mobile device or communicate the transformed image to a remote server for storage within a social networking system. In some instances the captured image may be saved on the mobile device and or communicated to a remote device. In some instances the motion sensor data and or the final computed angle of rotation to correct the tilted image may be saved with the captured image e.g. as metadata included with the image data.

In one embodiment for example the transformation of the image is performed on the same mobile device that acquires the image e.g. captures the image with the camera and provides the motion sensor data with the motion sensor . For example a user of a mobile device may take a picture using the mobile device e.g. smartphone and the mobile device may compute the angle of rotation necessary to correct for any tilt based on the motion sensor data of the mobile device when the image was captured.

In another embodiment a device other than the mobile device that captured the image may acquire the captured image and motion sensor data and then perform the tilt correction. For instance a desktop computer or other remote device e.g. server may be in wired or wireless communication with the mobile device and receive the captured image and motion sensor data from the mobile device that captures the image. The desktop computer or other remote device may then compute the angle of rotation and transform the image based on the motion sensor data. In some instances the desktop computer or other remote device may receive the captured image and motion sensor data via the internet or from a portable memory device such as flash memory CD ROM etc. In some instances the motion sensor data may be stored with the image data e.g. within metadata in an image file and stored on a device e.g. the mobile device which captured the image or other remote device .

A captured image including a house is displayed in the viewing frame . The captured image was taken while the mobile device was tilted and the image of the depicted house is accordingly shown tilted within the viewing frame . A longitudinal axis for the mobile device is shown as a dotted line. Axis represents a vertical reference axis at the time of image capture.

The captured image is displayed in the viewing frame such that the border A of the captured image aligns with the viewing frame . It should be appreciated that in other embodiments the border A may be larger or smaller than the viewing frame . Because the captured image of the house was taken while the mobile device was tilted the captured image is shown displayed at a tilt within viewing frame . As shown angle A represents the angle at which the captured image is tilted and accordingly the angle of rotation necessary to correct the tilt i.e. counterclockwise rotation by angle A .

In an embodiment the transformation of the captured image includes scaling the captured image in addition to rotating the captured image . In this way for example the captured image may be scaled appropriately to increase the size of the border to fill the spaces . illustrates the captured image after it has been rotated counterclockwise by angle A and scaled to fill the spaces created by rotation of the captured image . Border C represents the border A after being increased to fit the entire viewing frame within the border so that no spaces remain in the viewing frame . The scaling may be performed simultaneously with the rotation of the captured image or may be performed sequentially.

User interface is shown displayed on display of a mobile device. User interface includes viewing frame for displaying a captured image or a transformed image or both. The user interface also includes transformation control elements A B A B and C that provide user control over transformation of a captured image. A grid is displayed in the viewing frame to provide the user with vertical and horizontal references.

In a captured image is displayed in viewing frame . A user may use transformation control elements A B A B and C to control the transformation of captured image . For example in an embodiment user interface may be displayed on a touchscreen display with transformation control elements A B A B and C that are touch activated or other otherwise touch controlled.

Transformation control element provides user control over the transformation of the captured image . Transformation control element includes a first control element that provides user control over rotating the captured image at a first granularity and a second control element that provides user control of rotating the captured image at a second granularity. For example the control element with finer granularity provides the user with more fine tuned control over the rotation of the capture image . In the embodiment shown the captured image is rotated about the center point C of the viewing frame . Furthermore in an embodiment the captured image is additionally scaled to avoid spaces within the viewing frame without image content.

A distance or degree that the user slides or otherwise moves a first control element with a finer granularity may correlate to smaller degree of rotation than the same or similar distance of movement of a second control element with a larger granularity. For instance in the embodiment shown control elements and are embodied as dials that may be moved clockwise or counterclockwise from a default position along an arc by a user e.g. via touching the dial and sliding it clockwise or counterclockwise with control element having a finer granularity than control element . The default position is the center of the arc where the dials are vertical and corresponds to an initial position where the captured image is not rotated. As the user moves control element or in the clockwise direction the captured image is rotated clockwise at the respective granularity. Similarly if the user moves control element or in the counterclockwise direction the captured image is rotated counterclockwise at the respective granularity. In this way movement of control element by a certain number of degrees along the corresponding arc from the default position results in a smaller degree of rotation of the captured image than the similar degree of movement of control element along the corresponding arc. For example movement of control element by 90 degrees from the default position may rotate the captured image by 45 degrees while movement of control element by 90 degrees from the default position may rotate the captured image by 15 degrees. It should be appreciated that these values are exemplary and should not be construed as limiting. Furthermore in another embodiment control element may be set to have finer granularity control compared to control element .

User interface also includes transformation control elements A and B which are embodied as user controlled buttons displayed on the display . When a user clicks or otherwise activates control elements A and B then the captured image is rotated either clockwise or counterclockwise accordingly. For example clicking control element A may rotate the captured image counterclockwise a predetermined number of degrees and clicking or otherwise activating control element B may rotate the captured image clockwise a predetermined number of degrees.

User interface also includes transformation control elements A B and C which provides the user with control over the granularity of control element . For example when the user selects A full movement of control element across the dial e.g. 90 degrees results in a 45 degree rotation of the captured image. Similarly when the user selects B full movement of control element across the dial e.g. 90 degrees results in a 35 degree rotation of the captured image. Similarly when the user selects C full movement of control element across the dial e.g. 90 degrees results in a 15 degree rotation of the captured image in the respective direction. In an embodiment the overall granularity of control element is also changed respectively with selections of control elements A B and C in addition to the overall granularity of control element . For example in the example provided for control element the granularity of control element may be 15 degrees 10 degrees and 5 degrees for control elements A B and C respectively. It should be appreciated that the values are exemplary and should not be construed as limiting. Other values may also be implemented in other embodiments.

In another embodiment the control element may include another type of user motion or input apart from an arc type motion. For instance in an embodiment the movement of the control elements may be linear. illustrates an example control elements that are linear according to one embodiment. Control elements A and A are shown in the default position and may be moved linearly to the right or left by the user to enable rotation of the captured image in the clockwise or counterclockwise direction. Control element A for instance may rotate the captured image with a finer granularity than control element A.

In yet another embodiment the control elements may be implemented based on a user controlled parameter such as a single click or other input provided tithe control elements. For example illustrates example control elements B and B in the form of buttons which may be clicked by a user. Clicking the control elements B B may cause the capture image to rotate by a certain amount. The control element B may have finer granularity control correlating to a smaller degree of rotation than the control element B. For example single click of control element B may cause a 5 degree tilt and a single click of control element B may cause a 2 degree tilt. In certain instances the user may be able to hold down the control elements to continuously rotate the captured image. In such case the finer granularity control element rotates the captured image at a slower rate e.g. degrees of rotation per second than the larger granularity control element.

At block user input applied to a second control element is received. The second control element is also displayed on the user interface in addition to the first control element. The second control element enables the user to transform the captured image according to a second granularity which is different than the granularity of the first control element. At block the captured image is transformed based on user input received for the second control element and accordingly is displayed in the viewing frame as it is transformed. For example with the embodiment shown in rotation of the captured image may be displayed in the viewing frame as it is rotated at the second granularity based on the user input.

At block once the transformation is complete e.g. the user indicates completion of the transformation by pressing a corresponding button the transformed image is finalized e.g. saved or otherwise stored into memory of a device or communicated via the internet to a remote server for storage within a social networking system. In some instances the captured image may be saved on a device and communicated to a remote device.

It should be appreciated that the processes described in may be performed in combination. For example in an embodiment the user interface is provided and process is performed after the captured image is transformed according to process in . In this way the user may choose to perform an automatic image transformation to correct for tilt and thereafter be presented with user interface to enable further manual user controlled transformations. In an embodiment when the transformed image resulting from the process is displayed on the display along with user interface the control elements and may be positioned in a default position e.g. vertical orientation . In another embodiment when the transformed image resulting from the process is displayed on the display along with user interface the control elements and may be positioned to reflect the tilt angle of the captured image. In this way a user may return the image to its pre transformation state by moving the control elements and to the default position.

In the embodiments shown above the captured image was rotated about the center C of the viewing frame . In other embodiments the captured image may be rotated about a point of rotation that is not the center of the viewing frame. illustrates an example process for transforming a captured image about a point of rotation selected by a user according to an embodiment of the present disclosure. illustrates an example user interface displaying an image before the process in is performed according to an embodiment of the present disclosure. illustrates the example user interface of displaying an image that was transformed according to the process of according to an embodiment of the present disclosure. A and B are described together herein.

At block of process user interface is displayed on display of a device e.g. mobile device and a captured image is displayed in a viewing frame of the user interface . In the captured image shown in a house is shown off center from the center C of viewing frame .

At block user input is received for a user selection of a point of rotation. For example a user may touch a point P in the viewing frame to identify the point of rotation as represented by encircled reference number in . In an embodiment the point of rotation may be indicated on the user interface e.g. as a dot or other reference.

At block user input is received for indicating a granularity of rotation. For example in an embodiment the granularity of rotation is defined by the user sliding his finger away from the user defined point of rotation P to a granularity selection point as represented by encircled reference number in and B. The granularity of rotation is computed based on the distance that the finger is slid away from the point of rotation. For example in one embodiment a granularity selection point that is a larger distance from the point of rotation generates a finer granularity than the granularity generated from a granularity selection point that is a smaller distance from the point of rotation. In yet another embodiment a larger distance from the point of rotation generates a granularity that is less fine than the granularity generated from a smaller distance from the point of rotation. As the user slides his finger away from the point of rotation a line may be displayed in real time from the point of rotation to the position of the user s finger.

At block user input is received for indicating a direction and degree of rotation. For example in one embodiment the direction of rotation is defined by the user sliding his finger clockwise or counterclockwise with respect to the point of rotation as represented by encircled reference number in . The direction may be defined by the clockwise or counter clockwise direction that the finger is slid e.g. clockwise rotating the captured image clockwise and counterclockwise rotating the captured image counterclockwise. Furthermore the degree of rotation may be defined by how far the finger is slid to the clockwise or counterclockwise direction.

At block the captured image is rotated about the point of rotation based on the indicated granularity and direction of rotation. As shown in the user s finger has been slid counterclockwise to a point represented by the encircled reference number until the image was rotated to a depicted orientation.

In an embodiment the captured image is rotated on the display as the user s finger is slid. In the embodiment shown the line is displayed from the point of rotation to the granularity selection point and the length of the line remains fixed when rotated to the depicted position as the user slides his finger in the counterclockwise direction to encircled reference number .

In an embodiment the user is required to continuously maintain his finger on the display while selecting the point of rotation the granularity selection point and the direction and degree of rotation. If for example the user lifts his finger off the display then the user must repeat the selection of this information.

In one embodiment the captured image is scaled while being rotated to eliminate any spaces between the rotated image and the viewing frame as discussed in more detail herein.

The user device comprises one or more computing devices that can receive input from a user and transmit and receive data via the network . In one embodiment the user device is a conventional computer system executing for example a Microsoft Windows compatible operating system OS Apple OS X and or a Linux distribution. In another embodiment the user device can be a device having computer functionality such as a smart phone a tablet a personal digital assistant PDA a mobile telephone etc. The user device is configured to communicate via the network . The user device can execute an application for example a browser application that allows a user of the user device to interact with the social networking system . In another embodiment the user device interacts with the social networking system through an application programming interface API provided by the native operating system of the user device such as iOS and ANDROID. The user device is configured to communicate with the external system and the social networking system via the network which may comprise any combination of local area and or wide area networks using wired and or wireless communication systems.

In one embodiment the network uses standard communications technologies and protocols. Thus the network can include links using technologies such as Ethernet 702.11 worldwide interoperability for microwave access WiMAX 3G 4G CDMA GSM LTE digital subscriber line DSL etc. Similarly the networking protocols used on the network can include multiprotocol label switching MPLS transmission control protocol Internet protocol TCP IP User Datagram Protocol UDP hypertext transport protocol HTTP simple mail transfer protocol SMTP file transfer protocol FTP and the like. The data exchanged over the network can be represented using technologies and or formats including hypertext markup language HTML and extensible markup language XML . In addition all or some links can be encrypted using conventional encryption technologies such as secure sockets layer SSL transport layer security TLS and Internet Protocol security IPsec .

In one embodiment the user device may display content from the external system and or from the social networking system by processing a markup language document received from the external system and from the social networking system using a browser application . The markup language document identifies content and one or more instructions describing formatting or presentation of the content. By executing the instructions included in the markup language document the browser application displays the identified content using the format or presentation described by the markup language document . For example the markup language document includes instructions for generating and displaying a web page having multiple frames that include text and or image data retrieved from the external system and the social networking system . In various embodiments the markup language document comprises a data file including extensible markup language XML data extensible hypertext markup language XHTML data or other markup language data. Additionally the markup language document may include JavaScript Object Notation JSON data JSON with padding JSONP and JavaScript data to facilitate data interchange between the external system and the user device . The browser application on the user device may use a JavaScript compiler to decode the markup language document .

The markup language document may also include or link to applications or application frameworks such as FLASH or Unity applications the SilverLight application framework etc.

In one embodiment the user device also includes one or more cookies including data indicating whether a user of the user device is logged into the social networking system which may enable modification of the data communicated from the social networking system to the user device .

The external system includes one or more web servers that include one or more web pages which are communicated to the user device using the network . The external system is separate from the social networking system . For example the external system is associated with a first domain while the social networking system is associated with a separate social networking domain. Web pages included in the external system comprise markup language documents identifying content and including instructions specifying formatting or presentation of the identified content.

In an embodiment the mobile device may be implemented as the user device . For example the user device may be a mobile device that includes an image capture module and an image transformation module . In an embodiment the image capture module and the image transformation module may be implemented as the image capture module and the image transformation module respectively. The user device may also include other components not shown such as a motion sensor a camera a display and a processing device. The user device may perform the image transformations discussed herein.

The social networking system includes one or more computing devices for a social network including a plurality of users and providing users of the social network with the ability to communicate and interact with other users of the social network. In some instances the social network can be represented by a graph i.e. a data structure including edges and nodes. Other data structures can also be used to represent the social network including but not limited to databases objects classes meta elements files or any other data structure. The social networking system may be administered managed or controlled by an operator. The operator of the social networking system may be a human being an automated application or a series of applications for managing content regulating policies and collecting usage metrics within the social networking system . Any type of operator may be used.

Users may join the social networking system and then add connections to any number of other users of the social networking system to whom they desire to be connected. As used herein the term friend refers to any other user of the social networking system to whom a user has formed a connection association or relationship via the social networking system . For example in an embodiment if users in the social networking system are represented as nodes in the social graph the term friend can refer to an edge formed between and directly connecting two user nodes.

Connections may be added explicitly by a user or may be automatically created by the social networking system based on common characteristics of the users e.g. users who are alumni of the same educational institution . For example a first user specifically selects a particular other user to be a friend. Connections in the social networking system are usually in both directions but need not be so the terms user and friend depend on the frame of reference. Connections between users of the social networking system are usually bilateral two way or mutual but connections may also be unilateral or one way. For example if Bob and Joe are both users of the social networking system and connected to each other Bob and Joe are each other s connections. If on the other hand Bob wishes to connect to Joe to view data communicated to the social networking system by Joe but Joe does not wish to form a mutual connection a unilateral connection may be established. The connection between users may be a direct connection however some embodiments of the social networking system allow the connection to be indirect via one or more levels of connections or degrees of separation.

In addition to establishing and maintaining connections between users and allowing interactions between users the social networking system provides users with the ability to take actions on various types of items supported by the social networking system . These items may include groups or networks i.e. social networks of people entities and concepts to which users of the social networking system may belong events or calendar entries in which a user might be interested computer based applications that a user may use via the social networking system transactions that allow users to buy or sell items via services provided by or through the social networking system and interactions with advertisements that a user may perform on or off the social networking system . These are just a few examples of the items upon which a user may act on the social networking system and many others are possible. A user may interact with anything that is capable of being represented in the social networking system or in the external system separate from the social networking system or coupled to the social networking system via the network .

The social networking system is also capable of linking a variety of entities. For example the social networking system enables users to interact with each other as well as external systems or other entities through an API a web service or other communication channels. The social networking system generates and maintains the social graph comprising a plurality of nodes interconnected by a plurality of edges. Each node in the social graph may represent an entity that can act on another node and or that can be acted on by another node. The social graph may include various types of nodes. Examples of types of nodes include users non person entities content items web pages groups activities messages concepts and any other things that can be represented by an object in the social networking system . An edge between two nodes in the social graph may represent a particular kind of connection or association between the two nodes which may result from node relationships or from an action that was performed by one of the nodes on the other node. In some cases the edges between nodes can be weighted. The weight of an edge can represent an attribute associated with the edge such as a strength of the connection or association between nodes. Different types of edges can be provided with different weights. For example an edge created when one user likes another user may be given one weight while an edge created when a user befriends another user may be given a different weight.

As an example when a first user identifies a second user as a friend an edge in the social graph is generated connecting a node representing the first user and a second node representing the second user. As various nodes relate or interact with each other the social networking system modifies edges connecting the various nodes to reflect the relationships and interactions.

The social networking system also includes user generated content which enhances a user s interactions with the social networking system . User generated content may include anything a user can add upload send or post to the social networking system . For example a user communicates posts to the social networking system from a user device . Posts may include data such as status updates or other textual data location information images such as photos videos links music or other similar data and or media. Content may also be added to the social networking system by a third party. Content items are represented as objects in the social networking system . In this way users of the social networking system are encouraged to communicate with each other by posting text and content items of various types of media through various communication channels. Such communication increases the interaction of users with each other and increases the frequency with which users interact with the social networking system .

The social networking system includes a web server an API request server a user profile store a connection store an action logger an activity log an authorization server and an image importing module . In an embodiment of the invention the social networking system may include additional fewer or different components for various applications. Other components such as network interfaces security mechanisms load balancers failover servers management and network operations consoles and the like are not shown so as to not obscure the details of the system.

The user profile store maintains information about user accounts including biographic demographic and other types of descriptive information such as work experience educational history hobbies or preferences location and the like that has been declared by users or inferred by the social networking system . This information is stored in the user profile store such that each user is uniquely identified. The social networking system also stores data describing one or more connections between different users in the connection store . The connection information may indicate users who have similar or common work experience group memberships hobbies or educational history. Additionally the social networking system includes user defined connections between different users allowing users to specify their relationships with other users. For example user defined connections allow users to generate relationships with other users that parallel the users real life relationships such as friends co workers partners and so forth. Users may select from predefined types of connections or define their own connection types as needed. Connections with other nodes in the social networking system such as non person entities buckets cluster centers images interests pages external systems concepts and the like are also stored in the connection store .

The social networking system maintains data about objects with which a user may interact. To maintain this data the user profile store and the connection store store instances of the corresponding type of objects maintained by the social networking system . Each object type has information fields that are suitable for storing information appropriate to the type of object. For example the user profile store contains data structures with fields suitable for describing a user s account and information related to a user s account. When a new object of a particular type is created the social networking system initializes a new data structure of the corresponding type assigns a unique object identifier to it and begins to add data to the object as needed. This might occur for example when a user becomes a user of the social networking system the social networking system generates a new instance of a user profile in the user profile store assigns a unique identifier to the user account and begins to populate the fields of the user account with information provided by the user.

The connection store includes data structures suitable for describing a user s connections to other users connections to external systems or connections to other entities. The connection store may also associate a connection type with a user s connections which may be used in conjunction with the user s privacy setting to regulate access to information about the user. In an embodiment of the invention the user profile store and the connection store may be implemented as a federated database.

Data stored in the connection store the user profile store and the activity log enables the social networking system to generate the social graph that uses nodes to identify various objects and edges connecting nodes to identify relationships between different objects. For example if a first user establishes a connection with a second user in the social networking system user accounts of the first user and the second user from the user profile store may act as nodes in the social graph. The connection between the first user and the second user stored by the connection store is an edge between the nodes associated with the first user and the second user. Continuing this example the second user may then send the first user a message within the social networking system . The action of sending the message which may be stored is another edge between the two nodes in the social graph representing the first user and the second user. Additionally the message itself may be identified and included in the social graph as another node connected to the nodes representing the first user and the second user.

In another example a first user may tag a second user in an image that is maintained by the social networking system or alternatively in an image maintained by another system outside of the social networking system . The image may itself be represented as a node in the social networking system . This tagging action may create edges between the first user and the second user as well as create an edge between each of the users and the image which is also a node in the social graph. In yet another example if a user confirms attending an event the user and the event are nodes obtained from the user profile store where the attendance of the event is an edge between the nodes that may be retrieved from the activity log . By generating and maintaining the social graph the social networking system includes data describing many different types of objects and the interactions and connections among those objects providing a rich source of socially relevant information.

The web server links the social networking system to one or more user devices and or one or more external systems via the network . The web server serves web pages as well as other web related content such as Java JavaScript Flash XML and so forth. The web server may include a mail server or other messaging functionality for receiving and routing messages between the social networking system and one or more user devices . The messages can be instant messages queued messages e.g. email text and SMS messages or any other suitable messaging format.

The API request server allows one or more external systems and user devices to call access information from the social networking system by calling one or more API functions. The API request server may also allow external systems to send information to the social networking system by calling APIs. The external system in one embodiment sends an API request to the social networking system via the network and the API request server receives the API request. The API request server processes the request by calling an API associated with the API request to generate an appropriate response which the API request server communicates to the external system via the network . For example responsive to an API request the API request server collects data associated with a user such as the user s connections that have logged into the external system and communicates the collected data to the external system . In another embodiment the user device communicates with the social networking system via APIs in the same manner as external systems .

The action logger is capable of receiving communications from the web server about user actions on and or off the social networking system . The action logger populates the activity log with information about user actions enabling the social networking system to discover various actions taken by its users within the social networking system and outside of the social networking system . Any action that a particular user takes with respect to another node on the social networking system may be associated with each user s account through information maintained in the activity log or in a similar database or other data repository. Examples of actions taken by a user within the social networking system that are identified and stored may include for example adding a connection to another user sending a message to another user reading a message from another user viewing content associated with another user attending an event posted by another user posting an image attempting to post an image or other actions interacting with another user or another object. When a user takes an action within the social networking system the action is recorded in the activity log . In one embodiment the social networking system maintains the activity log as a database of entries. When an action is taken within the social networking system an entry for the action is added to the activity log . The activity log may be referred to as an action log.

Additionally user actions may be associated with concepts and actions that occur within an entity outside of the social networking system such as an external system that is separate from the social networking system . For example the action logger may receive data describing a user s interaction with an external system from the web server . In this example the external system reports a user s interaction according to structured actions and objects in the social graph.

Other examples of actions where a user interacts with an external system include a user expressing an interest in an external system or another entity a user posting a comment to the social networking system that discusses an external system or a web page within the external system a user posting to the social networking system a Uniform Resource Locator URL or other identifier associated with an external system a user attending an event associated with an external system or any other action by a user that is related to an external system . Thus the activity log may include actions describing interactions between a user of the social networking system and an external system that is separate from the social networking system .

The authorization server enforces one or more privacy settings of the users of the social networking system . A privacy setting of a user determines how particular information associated with a user can be shared. The privacy setting comprises the specification of particular information associated with a user and the specification of the entity or entities with whom the information can be shared. Examples of entities with which information can be shared may include other users applications external systems or any entity that can potentially access the information. The information that can be shared by a user comprises user account information such as profile photos phone numbers associated with the user user s connections actions taken by the user such as adding a connection changing user profile information and the like.

The privacy setting specification may be provided at different levels of granularity. For example the privacy setting may identify specific information to be shared with other users the privacy setting identifies a work phone number or a specific set of related information such as personal information including profile photo home phone number and status. Alternatively the privacy setting may apply to all the information associated with the user. The specification of the set of entities that can access particular information can also be specified at various levels of granularity. Various sets of entities with which information can be shared may include for example all friends of the user all friends of friends all applications or all external systems . One embodiment allows the specification of the set of entities to comprise an enumeration of entities. For example the user may provide a list of external systems that are allowed to access certain information. Another embodiment allows the specification to comprise a set of entities along with exceptions that are not allowed to access the information. For example a user may allow all external systems to access the user s work information but specify a list of external systems that are not allowed to access the work information. Certain embodiments call the list of exceptions that are not allowed to access certain information a block list . External systems belonging to a block list specified by a user are blocked from accessing the information specified in the privacy setting. Various combinations of granularity of specification of information and granularity of specification of entities with which information is shared are possible. For example all personal information may be shared with friends whereas all work information may be shared with friends of friends.

The authorization server contains logic to determine if certain information associated with a user can be accessed by a user s friends external systems and or other applications and entities. The external system may need authorization from the authorization server to access the user s more private and sensitive information such as the user s work phone number. Based on the user s privacy settings the authorization server determines if another user the external system an application or another entity is allowed to access information associated with the user including information about actions taken by the user.

The social networking system may include an image module . The image module may communicate with the user device to upload one or more images from the user device to the social networking system . For example the image module may receive a transformed image that has been captured by and transformed by the user device . In addition the image module may implement the functionality of the image transformation module . In an embodiment the image transformation techniques described herein may be performed by a suitable combination of the image module and the image transformation module . For example based on motion sensor data and other information provided by the mobile device as discussed herein the computation of the angle of rotation for a tilted image may be performed by the image module the image transformation module or both.

The foregoing processes and features can be implemented by a wide variety of machine and computer system architectures and in a wide variety of network and computing environments. illustrates an example of a computer system that may be used to implement one or more of the embodiments described herein in accordance with an embodiment of the invention. For example computer system may represent user device shown in and or mobile device .

The computer system includes sets of instructions for causing the computer system to perform the processes and features discussed herein. The computer system may be connected e.g. networked to other machines. In a networked deployment the computer system may operate in the capacity of a server machine or a client machine in a client server network environment or as a peer machine in a peer to peer or distributed network environment. In an embodiment of the invention the computer system may be the social networking system the user device and the external system or a component thereof. In an embodiment of the invention the computer system may be one server among many that constitutes all or part of the social networking system .

The computer system includes a processor a cache and one or more executable modules and drivers stored on a computer readable medium directed to the processes and features described herein. Additionally the computer system includes a high performance input output I O bus and a standard I O bus . A host bridge couples processor to high performance I O bus whereas I O bus bridge couples the two buses and to each other. A system memory and one or more network interfaces couple to high performance I O bus . The computer system may further include video memory and a display device coupled to the video memory not shown . Mass storage and I O ports couple to the standard I O bus . The computer system may optionally include a keyboard and pointing device a display device or other input output devices not shown coupled to the standard I O bus . Collectively these elements are intended to represent a broad category of computer hardware systems including but not limited to computer systems based on the x86 compatible processors manufactured by Intel Corporation of Santa Clara Calif. and the x86 compatible processors manufactured by Advanced Micro Devices AMD Inc. of Sunnyvale Calif. as well as any other suitable processor.

An operating system manages and controls the operation of the computer system including the input and output of data to and from software applications not shown . The operating system provides an interface between the software applications being executed on the system and the hardware components of the system. Any suitable operating system may be used such as the LINUX Operating System the Apple Macintosh Operating System available from Apple Computer Inc. of Cupertino Calif. UNIX operating systems Microsoft Windows operating systems BSD operating systems and the like. Other implementations are possible.

The elements of the computer system are described in greater detail below. In particular the network interface provides communication between the computer system and any of a wide range of networks such as an Ethernet e.g. IEEE 802.3 network a backplane etc. The mass storage provides permanent storage for the data and programming instructions to perform the above described processes and features implemented by the respective computing systems identified above whereas the system memory e.g. DRAM provides temporary storage for the data and programming instructions when executed by the processor . The I O ports may be one or more serial and or parallel communication ports that provide communication between additional peripheral devices which may be coupled to the computer system .

The computer system may include a variety of system architectures and various components of the computer system may be rearranged. For example the cache may be on chip with processor . Alternatively the cache and the processor may be packed together as a processor module with processor being referred to as the processor core . Furthermore certain embodiments of the invention may neither require nor include all of the above components. For example peripheral devices coupled to the standard I O bus may couple to the high performance I O bus . In addition in some embodiments only a single bus may exist with the components of the computer system being coupled to the single bus. Furthermore the computer system may include additional components such as additional processors storage devices or memories.

In general the processes and features described herein may be implemented as part of an operating system or a specific application component program object module or series of instructions referred to as programs . For example one or more programs may be used to execute specific processes described herein. The programs typically comprise one or more instructions in various memory and storage devices in the computer system that when read and executed by one or more processors cause the computer system to perform operations to execute the processes and features described herein. The processes and features described herein may be implemented in software firmware hardware e.g. an application specific integrated circuit or any combination thereof.

In one implementation the processes and features described herein are implemented as a series of executable modules run by the computer system individually or collectively in a distributed computing environment. The foregoing modules may be realized by hardware executable modules stored on a computer readable medium or machine readable medium or a combination of both. For example the modules may comprise a plurality or series of instructions to be executed by a processor in a hardware system such as the processor . Initially the series of instructions may be stored on a storage device such as the mass storage . However the series of instructions can be stored on any suitable computer readable storage medium. Furthermore the series of instructions need not be stored locally and could be received from a remote storage device such as a server on a network via the network interface . The instructions are copied from the storage device such as the mass storage into the system memory and then accessed and executed by the processor .

Examples of computer readable media include but are not limited to recordable type media such as volatile and non volatile memory devices solid state memories floppy and other removable disks hard disk drives magnetic media optical disks e.g. Compact Disk Read Only Memory CD ROMS Digital Versatile Disks DVDs other similar non transitory or transitory tangible or non tangible storage medium or any type of medium suitable for storing encoding or carrying a series of instructions for execution by the computer system to perform any one or more of the processes and features described herein.

For purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the description. It will be apparent however to one skilled in the art that embodiments of the disclosure can be practiced without these specific details. In some instances modules structures processes features and devices are shown in block diagram form in order to avoid obscuring the description. In other instances functional block diagrams and flow diagrams are shown to represent data and logic flows. The components of block diagrams and flow diagrams e.g. modules blocks structures devices features etc. may be variously combined separated removed reordered and replaced in a manner other than as expressly described and depicted herein.

Reference in this specification to one embodiment an embodiment other embodiments one series of embodiments or the like means that a particular feature design structure or characteristic described in connection with the embodiment is included in at least one embodiment of the disclosure. The appearances of for example the phrase in one embodiment or in an embodiment in various places in the specification are not necessarily all referring to the same embodiment nor are separate or alternative embodiments mutually exclusive of other embodiments. Moreover whether or not there is express reference to an embodiment or the like various features are described which may be variously combined and included in some embodiments but also variously omitted in other embodiments. Similarly various features are described that may be preferences or requirements for some embodiments but not other embodiments.

The language used herein has been principally selected for readability and instructional purposes and it may not have been selected to delineate or circumscribe the inventive subject matter. It is therefore intended that the scope of the invention be limited not by this detailed description but rather by any claims that issue on an application based hereon. Accordingly the disclosure of the embodiments of the invention is intended to be illustrative but not limiting of the scope of the invention which is set forth in the following claims.

