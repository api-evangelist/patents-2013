---

title: Fine-grained access control for synchronized data stores
abstract: A remote distributed data store may be configured to process data updates received through invocation of a common API with reference to a common schema. Local data stores may also be configured to process updates using a common API and schema. Data for multiple users may be stored in a common collection of items maintained by a remote distributed data store. User identity may be verified through a public identity service. User identity and access permissions may be associated with items stored in a remote distributed data store.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09330271&OS=09330271&RS=09330271
owner: Amazon Technologies, Inc.
number: 09330271
owner_city: Reno
owner_country: US
publication_date: 20131015
---
A service provider may maintain a remote distributed data store on behalf of its customers. This may involve operating a distributed database management system that employs techniques such as horizontal partitioning and replication or other methods of scaling the system to meet customer demand. Application of these techniques may cause the distributed data store to behave differently than an otherwise comparable local non distributed data store. This may be the case even when a distributed data store and a local data store utilize the same data model. For example a distributed data store and a local data store might each utilize a key value data model but nevertheless behave in a different manner due to partitioning replication and other factors. These differences may interfere with utilizing a local database in conjunction with a remote distributed data store.

Distributed data stores may exhibit different characteristics than local data stores. In order to provide data store and retrieval services to a potentially large number of users a provider of a hosted data service may employ distributed computing techniques not commonly applied to on premises data stores which may be referred to as local data stores. In some cases providers of a hosted data service may utilize different data models than those commonly used with local data stores. For example a provider may utilize a key value paradigm rather than the relational model. Application of these and other techniques may result in a distributed data store that behaves differently than a local non distributed data store.

These and other differences may interfere with scenarios that involve testing applications against a local data store prior to deployment or utilizing a common code base against a local or a remote data store. In addition in some cases there may be a need to synchronize between a local data store and a remote distributed data store.

A provider may host a data store in a datacenter and provide access to the data store as a service to various entities typically clients of the provider. The data store may be exposed to clients through a web service a web application a remote procedure call and so forth. These mechanisms and others may be referred to herein as services. In some embodiments a data store may provide an integrated front end that exposes one or more of the services to end users of the entities or customers. Through the services the end users may make requests that include various operations and queries to be performed on the data store through the use of application programming interface API calls to the service. A request may comprise for example an invocation of an API on behalf of a client as well as an invocation of an operation on a data store on behalf of a client. An invocation of an API may be based on a schema which may generally be described as a definition of the structure or organization of data maintained in a data store. An API may be described as a standard for interacting with the service. An invocation of an API may be described as asking some implementation of the API to perform the requested function.

The provider of a remote data store service may also require payment from a client in exchange for the use of the capacity. However the profitability of the endeavor may depend on a customer paying an amount that is proportional to the capacity consumed on its behalf. A limit on capacity consumption may be imposed on a client and enforced through various techniques such as throttling queuing and so forth. When usage exceeds the amount provisioned to the customer requests for services on behalf of a customer may be rejected or suspended.

Providers of these services may employ a distributed data store. A distributed data store may maintain data organized by collections of items. The items may each comprise a set of name value pairs a set of values identified by column names or undefined values. For convenience collections of items may be referred to as tables. In various embodiments individual items in a collection may not conform to any particular schema and each item may accordingly contain a different number of values each of which may have a different logical meaning and type. Values that have the same logical meaning and type such as those identified by the same name or column may for convenience be referred to as columns. Other embodiments of a data store may enforce particular structural requirements such as row and column format uniqueness constraints primary and foreign key relationships and the like. Examples of distributed data stores include key value databases relational databases non structured query language NoSQL databases document oriented databases object oriented databases and so forth.

The items in a collection may be identified by primary key values. Each primary key value taken as a whole uniquely identifies an item stored in the collection. To access an item a client of the distributed data store may issue a request that includes a primary key value that corresponds to that item. Embodiments may also support access using secondary keys which may not necessarily correspond to precisely one item in a table.

Primary keys may be defined when the collection is created. A description of a primary key may be stored with other aspects of the collection s schema which may include any required columns constraints on values and so forth. For the primary key schema may include a description of the length and value of a primary key. For example a primary key might be defined as a 256 bit binary value or as a variable length string of characters.

Primary keys may also be used in a distributed data store in conjunction with partitioning. In order to support large volumes of data and high workload demands distributed data stores may support partitioning the data in a table over a number of computing nodes. Various forms of partitioning may be supported. In horizontal partitioning each computing node may maintain a subset of the collection of items. In vertical partitioning data may be divided based on columns or fields so that each computing node may contain a subset of the columns defined on the table. Distributed data stores may host each horizontal or vertical partition on a separate computing node.

While a table can be split into multiple horizontal partitions each horizontal partition may be replicated between computing nodes so that the same item is stored on more than one computing node or more generally the same horizontal partition may be hosted on more than one computing node. This may improve the availability of the system because if one of the computing nodes becomes unavailable another computing node having the replicated data may be able to step in and take its place. Replication may improve the scalability of the system by allowing load to be shared among multiple computing nodes

Consistency between replicated partitions may be maintained using a technique that involves quorum or consensus between the replicated partitions. Embodiments may require quorum only among currently active computing nodes which may improve availability because it does not require all of the computing nodes to be online.

In some embodiments quorum may involve determining that a minimum number of computing nodes participate in a read or write operation. For read operations at least the minimum number of computing nodes must respond to a request to read an item. Because data is not necessarily replicated immediately it may be the case that two given computing nodes will have different values for the same item. If so some embodiments may return each version of the data along with information descriptive of the version. For write operations quorum may involve the minimum number of computing nodes acknowledging the success of a write operation. For example if three computing nodes share replicated data a write operation might be required of two of the three computing nodes. Embodiments may impose different quorum requirements based on the type of operation involved. For example write operations may involve a higher threshold number of computing nodes in order to achieve quorum.

A distributed data store may support a wide variety of operations. Non limiting examples include put operations which involve storing or updating items and read operations which involve retrieving values corresponding to an item. Both operations may supply primary key values for use by the distributed data store in identifying the item. Another example of an operation that may be supported by some embodiments is a range query. A range query involves returning a span of items conforming to some set of fixed criteria. The various operations on data may be made more efficient through the use of index structures which may be over the primary key. In addition to a primary key based index a secondary index may be used. Unlike primary key values secondary key values are not necessarily unique. In other words a single secondary key value may refer to one or more entries in a secondary index.

Distributed data stores may reflect a data consistency pattern that can be described as eventual consistency. This term refers to updated data that is not immediately available in updated form even after an update has been committed. For example an update might be committed in a distributed data store with three replication partners. An update might be committed and also stored in two of the three replication partners but not a third. If a query were to rely on the third replication partner the results would reflect a previous version of the data rather than the currently committed version of the data. However because the update has been committed the update will eventually be applied to the third replication partner. Accordingly the data may be said to be eventually consistent. A similar pattern may be seen with index structures. Some indexes may not be immediately updated to reflect an update to a data store. If so results of a query that relies on that index may correspond to a version of the data prior to the update. However eventually the index will be updated and will return consistent results.

In contrast to a distributed data store a local data store may not exhibit the various behaviors described above in regards to distributed data stores. For example a local data store may typically operate only on a single computing node without any replication partners and therefore not exhibit eventual consistency behaviors related to replication. In addition because a local data store is not hosted by a provider a local data store is not typically subjected to limits on capacity utilization. These and other differences in behavior may make testing applications against a local data store less effective than testing applications against a distributed data store. In various embodiments a local data store may be located on the same device as one or more applications that utilize the data store. In other embodiments a local data store may be accessible through a network connected to a computing device on which one or more applications operate. In some embodiments a local data store is one operated or administered by a client of a hosting provider and a remote data store is one operated and administered primarily be a hosting provider.

Another issue involving the use of local data stores is transferring and synchronizing data. Various scenarios include generating an initial data set on a local data store during testing and transferring the data set to a distributed data store when an associated application is deployed. Another scenario includes a common code base operating on a local device against a local data store when there is no network connectivity with a remote distributed data store. When network connectivity is restored data maintained in the local and remote data stores may be synchronized.

In various cases and embodiments local data stores may employ different data models than the remote data store. Various non limiting examples include relational databases document oriented databases JAVASCRIPT object notation JSON stores XML databases object oriented databases in memory data structures and so forth. Embodiments may therefore map between items in a remote distributed data store and items in the local data store. This may involve identifying item values in an item collection and mapping to entities in the local data store schema such as columns object fields and so forth. Similar mappings may be performed when the local and remote data stores employ the same data model. In other cases the data model and schemas employed may be the same or sufficiently similar so that mapping can be performed on a one to one basis or directly.

Various processes operating on a computing device such as computing device or computing device may store information on a local data store using APIs that support key value data stores. The term API may comprise any of various methods of interacting with a key value database such as functions methods remote procedure calls protocols and so forth. Key value data stores may be described as having key value semantics which includes APIs compatible with the use of a key value store as well as various behaviors consistent with the use of a key value data store. For example support for key value semantics may involve providing APIs and behavior allowing a set of values to be associated with a key.

Embodiments may provide support for the use of key value semantics using a variety of approaches. One example is depicted by local key value data store which may natively implement a key value database. In other words local key value data store may be designed and implemented primarily as a key value store as opposed to a relational database or some other form of data store.

In other embodiments a local data store may comprise a relational database object oriented database in memory or on disk data structure and so forth. For example local data store could be implemented as a relational database or as a NoSQL database that has different key value semantics than is desired. Embodiments may employ a component such as key value data store emulation to allow processes operating on computing device to access data using the desired key value semantics. Emulation of key value semantics may comprise mapping between schemas translating query statements converting data types and adapting to API differences.

Data may be exchanged between a local data store such as local data stores and and a distributed key value data store hosted at one or more remote locations . Exchange of data between local data stores such as local data stores and and a remote distributed key value data store may comprise various operations such as bulk import bulk export replication and synchronization. Bulk imports and exports involve transmission of blocks of data from a local data store to a distributed data store or from a distributed data store to a local data store. Replication may involve data transfers from one data store to another so that all or part of the source data store may be mirrored on the target data store. Synchronization involves mirroring and also involves ensuring that updates made to one data store are propagated or at least reconciled with other data stores.

Distributed key value data store may comprise a plurality of horizontal partitions such as horizontal partitions and . Each horizontal partition may be replicated. For example horizontal partition might comprise three key value data stores and which generally contain the same set of replicated data. One of these say key value data store might be considered a master copy which initially receives updates and propagates changes to its replication partners in this case key value data stores and . A quorum system may be employed for the purposes of committing changes and or maintaining serializability. A second horizontal partition may comprise key value data stores and and may replicate data in a manner similar to horizontal partition . Each replication partner may be hosted at the same location or a different location. There may accordingly be a variable amount of lag or delay in propagating updates to all of the replication partners. In addition the use of quorum and other techniques related to using distributed computing techniques may introduce variations in the behavior of a distributed key value data store as compared to a local key value data store.

Applications operating on computing device may access data using key value API independently of the semantics of the underlying local data store . Embodiments may configure key value API to be compatible not only with applications running on a local device such as computing device but also on a remote device such as an application running in an off premises data center and connected to a distributed key value store. For example an application might be written and tested on a local device using a local data store and then deployed to a data center. At the same time data from the local data store might also be exported to a distributed key value data store. The application would then be able to commence operation at the data center using the data originally kept locally.

In order to allow such an application to be tested before deployment embodiments may provide various emulation capabilities. A wide variety of behaviors might be emulated including and in addition to those depicted in . Embodiments may perform emulation to allow for a single code base to be used on local devices and off premises computing facilities to avoid vendor lock in to improve testing procedures and so on. Embodiments may detect various characteristics of the environment in which the code runs to determine whether the applicable data store is a local data store or a distributed data store.

Error injection comprises a module that introduces simulated errors in a controllable fashion to simulate testing. Embodiments may for example randomly inject errors that might normally occur only in a distributed key value store. This includes errors related to replication quorum authentication throttling provisioned throughput and so on. Embodiments might also simulate node failover events that may occur when a replicated data store or the computing node on which it operates experiences an error that causes it to cease functioning.

Distributed databases may have behaviors related to eventual consistency. Embodiments may therefore emulate this behavior as depicted by module . In one embodiment read requests are modified to sometimes return pre modification values in order to simulate an effect of eventual consistency. In a distributed data store an update applied to one node might take some time to propagate from one data store to another data store acting as a replication partner. In addition there may be a delay in updating data structures such as indexes. Accordingly returning a pre modification value may simulate an effect of using a replication partner as the source of the data. The pattern of returning old and new values may be based partly on a simulated pattern of accessing replication partners for workload distribution such as with a round robin scheme. For example in a three node replication scheme the emulation could return a new value 33.3 of the time and an old value 66.6 percent of the time. To simulate the effect of delayed index updates queries may return results that would be accurate under the old data but inaccurate after considering the effect of the new data.

In order to support returning old i.e. pre update values or to simulate the behavior of old indexes embodiments may for example extend the schema of the local data store or utilize a new data store in order to retain pre update values.

Provisioned throughput emulation may comprise maintaining counts or metrics pertaining to use of the data store. A hosted remote distributed data store may impose a fee structure based upon an amount of data transferred to or from the data store. In various embodiments this may be a fixed total amount an amount per unit time and so on. When the amount of data transferred exceeds a limit throttling may result. This may imply a delay in processing operations extra charges accrued or a throttling error. Emulation of provisioned throughput restrictions may allow testing and predictions related to provisioned throughput issues. The cost of hosting a service for example might be estimated using a local data store prior to deployment to a remote distributed data store. Embodiments might also provide recommendations as to the level of throughput that should be provisioned. This may for example be based on running tests against a local data store to estimate an amount of provisioned throughput that may be needed at the remote distributed data store.

Latency emulation refers to simulating the effects of latency that might be encountered while running against a distributed system. Embodiments may delay sending responses by an appropriate amount of time. The amount of delay may depend on various factors. These may include delays based on the geographic distribution of data centers hosting replicated data stores. For example referring to key value data store and key value data store might be located in separate data centers located on different continents. Depending on where the data centers are located varying amounts of delay are to be expected. In addition to or instead of geography network topology may be used. This may include for example the number of hops required to communicate between a computing device and a key value data store.

Query latency may also be emulated. This may be based partly on horizontal partitioning of data. For example in data in a collection might be divided between horizontal partitions and . Querying and assembling result sets may take longer when data is hosted on multiple partitions as compared to when data is hosted within a single partition. Embodiments may therefore simulate additional latency when satisfying a query would if performed on a distributed key value data store cross horizontal partition boundaries.

Local store adapter may provide adaptations for simulating the semantics of a key value store when local data store does not provide native support. Embodiments may utilize a variety of approaches to simulate key value semantics. For example if local data store is a relational database the following techniques might be employed alone or in some combination. Indexes corresponding to the key in a key value collection may be created if not already present to provide more efficient access to values based on the key. A view may be created in which two or more tables are joined based on the key. The same join may be issued on an ad hoc basis rather than creating a view. Statements issued to the key value API may be translated from for example PUT and GET statements to INSERT and SELECT statements. When forming statements projections may be based on values associated with a key.

In some embodiments a number of local key value stores each operating on a computing device may be synchronized with a central remote key value data store. Moreover data from each device may be stored within a single collection maintained by the remote key value data store. depicts an embodiment of a system which synchronizes data between multiple computing devices and a single item collection hosted by a distributed key value store.

In item collection may be a collection of items accessible and storable by a key value. For illustrative purposes item collection may contain a subset of items associated with user one and a subset of items associated with user two . In this context the term item may refer to keys and their associated values. For security reasons subset of items should be accessible to user one but not to user two and subset of items should be accessible to user two but not user one . Embodiments may impose these restrictions when transferring data to or from item collection . For example if computing device is operated by user one then data added to local key value store should not be allowed to affect any data contained in subset of items during synchronization. Similarly if computing device is operated by user two changes to subset of items should for security reasons be propagated to local data store but not local data store .

Various hosted applications may also add data to item collection . Some of this data may be associated with a particular user such as user one and thereafter embodiments may prevent such data from being accessible to users other than user one . Other data may be common to all users. Embodiments may therefore make common data accessible to all users.

Embodiments may utilize federated identity services to verify the identity of users and control access to data in a common item collection. Federated identity refers to linking a user s identity across multiple independent domains. To do so one or more public identity services such as identity service and identity service may be utilized. The identity services may be available through a public network such as the Internet. These services allow for a user to be authenticated with a single sign on and for exchange of associated credentials across multiple security domains such as those that might exist between computing device and remote locations . A user of computing device might perform authentication through an identity service for example prior to communication with the remote distributed data store. The remote distributed data store upon receiving a verified identity of the user originating from identity service may authenticate the user even when the remote distributed data store has no preexisting knowledge of that user s identity. The remote distributed data store may maintain an association between the verified identity and items owned by the corresponding user within a consolidated collection containing items owned by multiple users.

Operations and depict applying an update to a local data store. These operations may be performed on a local device while operations and which relate to updating data in a remote data store are processed. Accordingly when synchronization is initiated at operation there may be new altered or deleted data on the local data store as well as the remote data store.

Operation depicts initiating an update of a local data store by invoking a local version of a common API. The term common API refers to an API that may be employed on a local device in conjunction with a local data store or in a hosted environment against a remote data store. Note that the term common API does not necessarily imply that instructions and or circuitry implementing common APIs are the same. However methods of interaction are the same between different versions of a common API.

Operation depicts mapping from a common key value schema to a schema employed by a local data store. Interactions with a common API may be partially expressed by reference to a common schema. Embodiments may therefore support inserting updating and deleting data on a local device or in a hosted environment through commands that refer to data using the same key value oriented schema. However the local data store may not necessarily be a key value data store. In some embodiments the local data store may be a key value data store but utilize a different schema. In either case a translation step may be performed in which references to the common schema are translated to references to the local schema. This operation may include transformations of commands such as key value commands such as PUTS and GETS to relational commands such as INSERT or SELECT. It may also include mapping from an item collection referenced by a common schema to one or more tables in a relational database. Embodiments may employ joins projections and so forth to cause INSERT or SELECT commands to apply to a set of values corresponding to those in the collection. Embodiments may also employ various techniques such as multi valued columns or sparse column support. These techniques may be employed by various embodiments in conjunction with mapping variable numbers of values in a key value schema to a generally fixed number of columns in the relational model.

In some embodiments operations and may be omitted or altered to the extent that the local data store is accessed independently of a common API or common schema. Data may be downloaded from the remote distributed data store to the local data store or uploaded from the local data store to the remote distributed data store. The upload or download may be performed in conjunction with mapping between the schemas of the local and remote data stores. In some embodiments a parallel data store may also be operated locally. The parallel data store may more closely reflect the schema of the remote distributed data store than the primary local data store. Replication and schema mapping may be performed locally.

Operation depicts applying the update to a local data store. This may comprise executing commands formulated by the mappings described above regarding operation . By applying the update the local data store may contain new or modified data or data may have been deleted. During the synchronization process initiated at operation these changes may be propagated to and or reconciled with the remote data store.

The remote data store may also have updated data. Operation depicts initiating an update of the remote data store by invoking a hosted version of the common API. This may occur for example when a hosted application invokes a version of the common API. A hosted application may comprise various forms of code or instructions that are executed on computing facilities remote to the local device such as those provided by a remote data center. In some cases and embodiments an update may be initiated through a web service remote procedure call and so forth.

At operation the update may be applied to the remote data store. In various embodiments the remote data store comprises a distributed key value database. When the common API is based on a key value paradigm little or no translation is required. However in some cases the common API may utilize different schema or semantics than those used by the remote data store. In such cases embodiments may map between the schema or semantics of the common API and those of the remote data store.

Operation depicts initiating the synchronization process. In general terms synchronizing may involve applying changes made to the local store to the remote store and applying changes made to the remote store to the local store. A reconciliation process may be employed to resolve conflicting changes. Data may be exchanged using a variety of approaches including log based methods or a mark and sweep approach. In the log based method entries describing incremental changes to data are exchanged. In the mark and sweep approach changed items are marked accordingly and sent in full during reconciliation. Embodiments may employ various other approaches and optimizations.

Operation depicts mapping between schemas of the local and remote distributed data store. Embodiments may map from the schema of the remote distributed data store to that of the local data store or from the schema of the local data store to the schema of the remote distributed data store. As used herein the term mapping may apply to mappings in either direction. Embodiments may for example perform a bulk transfer of data from a remote distributed data store to a local data store. The data may be mapped to the schema of the local data store and stored locally. Embodiments may also perform bulk uploads from the local data store to the remote distributed data store in which case the data may be mapped from the schema of the local data store to the schema of the remote distributed data store.

Operation depicts restricting the scope of the synchronization based at least in part on user identity. As described herein user identity may be verified through the use of public federated identity services or various other approaches. Data from multiple users may be stored within the same collection at the remote store. Embodiments may restrict the synchronization process to apply only to those items in the remote data store that are associated with the user.

Operation depicts transferring data from a remote data store to a local data store. As indicated above the scope of the transfer may be limited based on user identity. If so embodiments may limit the transfer to include only those items that are owned by the user or for which the user has read privileges. Data transferred to the local device may be applied to the local store possibly subject to a reconciliation process if there are conflicting updates.

Operation depicts transferring data from the local data store to the remote data store. The data may then be applied to the remote data store restricted by the scope of the operation as determined based on user identity. Embodiments may prevent updates from being applied to data that is not owned by a user or items for which the user does not have write privileges. When data is inserted an association between the user and the data may be maintained in the remote data store to indicate that the user has access rights to the data.

Operation depicts applying provisioned throughput restrictions to the synchronization process. For example some embodiments may restrict throughput on the remote data store to a provisioned amount. This may be based for example on a level of service purchased by the user. These limits may be exceeded during synchronization. Embodiments may employ a number of approaches in regards to provisioned throughput restrictions. In one embodiment the amount of data requiring transfer is determined and used to break the synchronization process into segments that are small enough to have a low likelihood of encountering a provisioned throughput restriction. In another embodiment synchronization is paused should a provisioned throughput restriction be encountered. In yet another embodiment synchronization is performed continuously or in frequent increments so that each phase is unlikely to trigger a throughput restriction.

Embodiments may perform one directional transfers of data. For example embodiments may perform a bulk upload of data from a local data store to a remote data store or from a remote data store to a local data store. Embodiments may request and receive information from the remote data store or other source that is indicative of provisioned throughput restrictions. The information may indicate the rate of throughput allowed a burst rate allowed usage pattern information and so forth. Based on the information embodiments may schedule bulk transfers to minimize the possibility of provisioned throughput being exceeded. Embodiments may also apply throttling to restrict transfers to using a specific amount of throughput per unit of time.

Operation depicts configuring a local data store to act as a gateway for data to be transferred to a remote data store from a remote data store or both. Embodiments may act as a data gateway by transmitting data in the desired direction or directions continuously or at periodic intervals. Operations through depict aspects of updating a local data store and transferring the data to a remote data store. Operations through depict aspects of updating a remote data store and transferring the data to a local data store.

Operation depicts initiating an update of a local data store by invoking a local version of a common API and schema. Embodiments may employ a common API to allow applications running locally or remotely to initiate updates using common API semantics. Common schema acts to improve transferability of data between data stores. In some embodiments commonality of API and or schema is not required.

At operation the update may be applied to a local data store. Some embodiments may perform mapping from the common schema to the schema employed by the local data store. In other embodiments the common schema and the schema employed by the local data store may be identical or sufficient. In some embodiments the schema employed by the local data store may be a subset of the schema employed by the remote data store or of a common schema not fully implemented by any single data store.

Embodiments may execute a command to update the local data store where the command may reflect any schema mapping that has occurred. The command may also be based on a mapping from key value semantics to relational semantics if the local data store is a relational database.

Operation depicts controlling the rate of data transfers from the local data store to the remote data store. Embodiments may employ a variety of factors to determine when to transfer data or to determine how much data to transfer. The scheduling frequency and amount of data transfer may be based on factors such as a level of provisioned throughput an amount of data in the local data store that has changed a minimum interval a maximum interval a predefined frequency and so forth.

Embodiments may perform data coalescing and or de duplication of data prior to or contemporaneously with synchronization. This may for example involve eliminating redundant updates to the same item so that the amount of data transmitted may be reduced. These operations may be performed by the local data store and or the remote distributed data store.

Various filters may be applied to data on the local data store and or the remote distributed data store. The filters may be employed to select or to restrict data to be synchronized or otherwise transferred. Examples of filters include queries that identify subsets of data in a table or other collection of items data associated with a user group or role recent data and so forth. In some cases and embodiments filters for the remote distributed data store may be supplied by an administrator of the remote distributed data store and are not overridden by user or customer supplied filters. In other cases users or customers may supply filters to be applied to the remote data store upon synchronization or other transfers of data.

When a transfer is initiated data maintained in the local data store may mapped as depicted by operation from the schema of the local data store to the common schema. Some embodiments may map directly to the schema of the remote data store if it differs from that of the common schema. Once mapped the data may be transferred to the remote data store as depicted by operation . In some embodiments the data may be transferred as a series of commands that may be parsed and executed at the remote location using the common API. Other embodiments may encode or compress the data.

The amount of data transferred may be limited by a number of factors. Embodiments may for example send only the data that has been affected by a change made since the previous transmission. The volume of data may be limited to an amount that would not exceed provisioned throughput restrictions or that is unlikely to cause provisioned throughput restrictions to be exceeded.

Operations through depicts aspects of updating a remote data store and transferring the updated data to one or more local data stores. This may be done instead of or in addition to operations through . In one embodiment a remote distributed data store transmits updates to a plurality of local data stores.

Applications running in a hosted environment may initiate an update of the data maintained in a remote distributed data store as depicted by operation . This may be done through an application s use of a common API and schema. In some embodiments an application that runs on a local computing device using a local data store may be made operative in a hosted environment without changing its use of the common API or schema. Similarly an application that runs in a hosted environment against a remote distributed data store may be made to operate on a local computing device using a local data store.

Operation depicts applying the update to a remote distributed data store. In some cases and embodiments translation between the common schema and that of the remote distributed data store may be performed. In other cases the common API schema and that of the remote distributed data store are the same. Embodiments may apply the update once any necessary translation is performed. Updates to the remote distributed data store may be subject to access control. Data transferred from the local data store may be supplied with a verified identity of a user which may be based on the user validating his credentials via a public identity service. Information indicative of the verified identity of the user may be supplied with the data and used to control access to subcollections in a collection of the remote distributed data store. These operations may be performed even if the remote distributed data store has not previously seen or validated the credentials of the user.

As depicted by operation the rate or period for transmitting data to local data stores may be controlled based on factors such as provisioned throughput restrictions and the amount of data that has changed subsequent to the previous transmission.

Operation depicts mapping from the schema of the remote distributed data store to the common schema. In some embodiments the schema of the remote distributed data store and the common schema will be the same or the schema of the remote distributed data store will be a subset of the common schema. In such cases the mapping operation may be omitted. Subsequent to mapping the data may be transferred from the remote distributed data store to one or more local data stores as depicted by operation . The data sent may be restricted based on a verified identity of the user and any applicable rights associated with the items to be transferred to the local store. For example when an item is stored in a collection of the remote distributed data store it may be associated with a verified user identity. At operation some embodiments may restrict data transferred to those items that are associated with the user of the local data store.

Data transfer operations may be restricted by applying fine grained permissions to updates to a remote distributed data store. depicts an embodiment for applying fine grained permissions to data transfer operations between a local data store and a remote distributed data store. Although depicted as a sequence of operations those of ordinary skill in the art will appreciate that the depicted order should not be construed as limiting the scope of the present disclosure and that at least some of the depicted operations may be altered omitted reordered or performed in parallel.

As depicted by operation a subset of data in a remote distributed data store may be associated with a first user group or role. The subset may be a subcollection of a common collection of data of the remote distributed data store. Other subsets of data in the common collection may be associated with other users groups or roles. For example a collection of data in the remote distributed data store might be employed to store information pertaining to multiple users only a subset of which is accessible to the first user. The association may imply some level of control over or rights to access the data. The rights may be affirmative such as the right to read or modify the data. The rights may also be negative for example indicating that the first user group or role is not permitted to write to the data.

Operation depicts receiving a request to synchronize data between a local data store and a common collection of data maintained by a remote distributed data store or otherwise transfer data into the remote distributed data store. The request may be received for example by a hosted application operating remotely and connected to the remote distributed data store. The request may originate from a local computing device under the control of the first user. The local computing device may contain a local data store in which one or more collections are to be synchronized with one or more collections in the remote distributed data store. The local collections to be synchronized may be placed within corresponding collections at the remote distributed data store. The corresponding collections at the remote distributed data store may be shared with other users.

In some embodiments a single computing device may maintain multiple data stores each associated with a different user or multiple users may share a single local data store. If so a subset of data from a local collection might be synchronized with a remote distributed data store rather than the entire local data store or an entire collection in a remote data store. A verified identity of the current user may be submitted with the data to be synchronized so that the correct user may be identified.

The received request may include identification information such as credentials associated with the first user group or role. At operation the credentials or other identifying information may be verified. Embodiments may employ a domain server or database associated with the same domain as the remote distributed data store. Other embodiments may employ outside federated identity services to verify the identity of the user in a manner that is compatible with single sign on across multiple independent domains.

At operation embodiments may transmit data from the remote distributed data store to the local data store associated with the first user where the data is associated with the first user group or role. For example the first user might have read privileges for the data in question and therefore be permitted read access. Operation may also be described as transmitting data from the subcollection of data associated with the first user group or role.

At operation the remote distributed data store may receive and process updates from the local data store associated with the first user group or role. The updates may be applied to data in the remote distributed data store when the data corresponds to a subcollection associated with the user group or role. The updates may also be applied if the user group or role has been granted write privileges to the data.

Embodiments may receive a request to synchronize data from the common collection in the remote distributed data store with data in a local data store associated with a second user group or role as depicted by operation . During synchronization embodiments may exclude data associated with the first user group or role when transmitting data to the local data store or when accepting updates from the local data store as depicted by operation .

A local data store may possess a schema that is not identical to that of a remote distributed data store. This may be due to use of different storage paradigms such as a local data store that uses a relational data store and a remote distributed data store that uses a key value paradigm. It may also be a result of the use of different forms of normalization data types constraints and so forth. Embodiments may simulate the schema of a remote distributed data store through various means for example by utilizing views joins projections and so forth. Operation depicts simulating the schema of a remote distributed data store.

Operation depicts simulating replication delays. In various embodiments of a distributed data store updates to one data store may not be immediately applied to all replicas of the data source. The update is said to be eventually consistent because once the update has become durable it is guaranteed to be eventually propagated to all of the replicas. An application that accesses eventually consistent data may encounter errors caused by this delay. The delay may be simulated to enable a test application to discover such errors while still operating locally. Similarly operation depicts simulating delays in applying updates to index structures. In a remote distributed database system some indexes may not be updated immediately. Queries that rely on these indexes may therefore produce inconsistent results which may also result in errors. Embodiments may also simulate this behavior.

Operation depicts simulating provisioned throughput rules. Incoming or outgoing data for the local data store may be measured to determine a throughput rate. Embodiments may trigger throttling errors if the measured throughput exceeds the provisioned amount. The provisioned amount may be set to the same amount as a corresponding remote distributed data store. As depicted by operation some embodiments may be employed to evaluate alternative provisioned throughput scenarios. For example a local data store could measure throughput usage and recommend that the user purchase an amount of provisioned throughput that is optimal for the measured uses. Embodiments may also provide cost estimates based on the amount of usage. The estimates may be used as a measure of how much provisioned throughput should be purchased in order to operate on the remote distributed data store prior to deployment.

Various throttling effects may also be simulated as depicted by operation . This may include throttling based on exceeding provisioned throughput or other causes. Latency may also be simulated as depicted by operation . Embodiments may emulate expected latency based on a variety of factors including expected characteristics of a remote distributed data store corresponding to or associated with the local data store. For example an application might be tested locally with a local data store. Upon deployment both the application and data store might be switched to a hosted environment in which a remote distributed data store is employed rather than a single local data store. Characteristics of the remote distributed data store such as the number of horizontal partitions the number of replication partners and so forth may be used as a basis for emulating behaviors such as latency.

As depicted by operation errors might also be simulated. Factors such as a load randomness and so forth may be utilized to determine how often and what type of errors should be generated. Injection of errors allows for improvement testing prior to deployment.

Embodiments may support deployment from local to a remote hosted computing environment as depicted by operation . Deployment may comprise exporting data from the local data store to the remote distributed data store. In some embodiments deployment may involve synchronizing changes to schema made on the local data store. For example a schema could be developed and tested on the local data store and then deployed to the remote distributed data store. In some cases and embodiments deployment may involve creating a new schema in the remote distributed data store such that the new schema is similar or identical to schema on the local data store.

Embodiments may receive information about various characteristics of a distributed data store. The information may describe horizontal partitioning schemes replication schemes indexing schemes information about the volume and distribution of data and so on. Embodiments may utilize the information to emulate distributed data store behaviors such as request latency.

Embodiments of the present disclosure may be employed in conjunction with many types of database management systems DBMSs . A DBMS is a software and hardware system for maintaining an organized collection of data on which storage and retrieval operations may be performed. In a DBMS data is typically organized by associations between key values and additional data. The nature of the associations may be based on real world relationships that exist in the collection of data or it may be arbitrary. Various operations may be performed by a DBMS including data definition queries updates and administration. Some DBMSs provide for interaction with the database using query languages such as structured query language SQL while others use APIs containing operations such as put and get and so forth. Interaction with the database may also be based on various protocols or standards such as hypertext markup language HTML and extended markup language XML . A DBMS may comprise various architectural components such as a storage engine that acts to store data on one or more storage devices such as solid state drives.

Communication with processes executing on the computing nodes and operating within data center may be provided via gateway and router . Numerous other network configurations may also be employed. Although not explicitly depicted in various authentication mechanisms web service layers business objects or other intermediate layers may be provided to mediate communication with the processes executing on computing nodes and . Some of these intermediate layers may themselves comprise processes executing on one or more of the computing nodes. Computing nodes and and processes executing thereon may also communicate with each other via router . Alternatively separate communication paths may be employed. In some embodiments data center may be configured to communicate with additional data centers such that the computing nodes and processes executing thereon may communicate with computing nodes and processes operating within other data centers.

Computing node is depicted as residing on physical hardware comprising one or more processors one or more memories and one or more storage devices . Processes on computing node may execute in conjunction with an operating system or alternatively may execute as a bare metal process that directly interacts with physical resources such as processors memories or storage devices .

Computing nodes and are depicted as operating on virtual machine host which may provide shared access to various physical resources such as physical processors memory and storage devices. Any number of virtualization mechanisms might be employed to host the computing nodes.

The various computing nodes depicted in may be configured to host web services database management systems business objects monitoring and diagnostic facilities and so forth. A computing node may refer to various types of computing resources such as personal computers servers clustered computing devices and so forth. A computing node may for example refer to various computing devices such as cell phones smartphones tables embedded device and so on. When implemented in hardware form computing nodes are generally associated with one or more memories configured to store computer readable instructions and one or more processors configured to read and execute the instructions. A hardware based computing node may also comprise one or more storage devices network interfaces communications buses user interface devices and so forth. Computing nodes also encompass virtualized computing resources such as virtual machines implemented with or without a hypervisor virtualized bare metal environments and so forth. A virtualization based computing node may have virtualized access to hardware resources as well as non virtualized access. The computing node may be configured to execute an operating system as well as one or more application programs. In some embodiments a computing node might also comprise bare metal application programs.

Each of the processes methods and algorithms described in the preceding sections may be embodied in and fully or partially automated by code modules executed by one or more computers or computer processors. The code modules may be stored on any type of non transitory computer readable medium or computer storage device such as hard drives solid state memory optical disc and or the like. The processes and algorithms may be implemented partially or wholly in application specific circuitry. The results of the disclosed processes and process steps may be stored persistently or otherwise in any type of non transitory computer storage such as e.g. volatile or non volatile storage.

The various features and processes described above may be used independently of one another or may be combined in various ways. All possible combinations and sub combinations are intended to fall within the scope of this disclosure. In addition certain methods or process blocks may be omitted in some implementations. The methods and processes described herein are also not limited to any particular sequence and the blocks or states relating thereto can be performed in other sequences that are appropriate. For example described blocks or states may be performed in an order other than that specifically disclosed or multiple blocks or states may be combined in a single block or state. The example blocks or states may be performed in serial in parallel or in some other manner. Blocks or states may be added to or removed from the disclosed example embodiments. The example systems and components described herein may be configured differently than described. For example elements may be added to removed from or rearranged compared to the disclosed example embodiments.

It will also be appreciated that various items are illustrated as being stored in memory or on storage while being used and that these items or portions thereof may be transferred between memory and other storage devices for purposes of memory management and data integrity. Alternatively in other embodiments some or all of the software modules and or systems may execute in memory on another device and communicate with the illustrated computing systems via inter computer communication. Furthermore in some embodiments some or all of the systems and or modules may be implemented or provided in other ways such as at least partially in firmware and or hardware including but not limited to one or more application specific integrated circuits ASICs standard integrated circuits controllers e.g. by executing appropriate instructions and including microcontrollers and or embedded controllers field programmable gate arrays FPGAs complex programmable logic devices CPLDs etc. Some or all of the modules systems and data structures may also be stored e.g. as software instructions or structured data on a computer readable medium such as a hard disk a memory a network or a portable media article to be read by an appropriate device or via an appropriate connection. The systems modules and data structures may also be transmitted as generated data signals e.g. as part of a carrier wave or other analog or digital propagated signal on a variety of computer readable transmission media including wireless based and wired cable based media and may take a variety of forms e.g. as part of a single or multiplexed analog signal or as multiple discrete digital packets or frames . Such computer program products may also take other forms in other embodiments. Accordingly the present invention may be practiced with other computer system configurations.

Conditional language used herein such as among others can could might may e.g. and the like unless specifically stated otherwise or otherwise understood within the context as used is generally intended to convey that certain embodiments include while other embodiments do not include certain features elements and or steps. Thus such conditional language is not generally intended to imply that features elements and or steps are in any way required for one or more embodiments or that one or more embodiments necessarily include logic for deciding with or without author input or prompting whether these features elements and or steps are included or are to be performed in any particular embodiment. The terms comprising including having and the like are synonymous and are used inclusively in an open ended fashion and do not exclude additional elements features acts operations and so forth. Also the term or is used in its inclusive sense and not in its exclusive sense so that when used for example to connect a list of elements the term or means one some or all of the elements in the list.

While certain example embodiments have been described these embodiments have been presented by way of example only and are not intended to limit the scope of the inventions disclosed herein. Thus nothing in the foregoing description is intended to imply that any particular feature characteristic step module or block is necessary or indispensable. Indeed the novel methods and systems described herein may be embodied in a variety of other forms furthermore various omissions substitutions and changes in the form of the methods and systems described herein may be made without departing from the spirit of the inventions disclosed herein. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of certain of the inventions disclosed herein.

