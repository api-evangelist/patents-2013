---

title: Hierarchical hash tables for SIMT processing and a method of establishing hierarchical hash tables
abstract: A graphical processing unit having an implementation of a hierarchical hash table thereon, a method of establishing a hierarchical hash table in a graphics processing unit and GPU computing system are disclosed herein. In one embodiment, the graphics processing unit includes: (1) a plurality of parallel processors, wherein each of the plurality of parallel processors includes parallel processing cores, a shared memory coupled to each of the parallel processing cores, and registers, wherein each one of the registers is uniquely associated with one of the parallel processing cores and (2) a controller configured to employ at least one of the registers to establish a hierarchical hash table for a key-value pair of a thread processing on one of the parallel processing cores.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09600852&OS=09600852&RS=09600852
owner: Nvidia Corporation
number: 09600852
owner_city: Santa Clara
owner_country: US
publication_date: 20130510
---
This application is directed in general to hash tables and more specifically to hash tables for single instruction multiple threads SIMT .

A graphics processing unit computing system includes a combination of a Graphics Processing Unit GPU and a Central Processing Unit CPU . As such efficiently operating a graphics pipeline includes distributing a workload appropriately between the CPU and the GPU according to the differences between the two architectures.

One difference between a GPU and a CPU is in how the two architectures address the issue of accessing off chip memory. This can be an expensive operation and include hundreds of clock cycles of latency. In CPUs many transistors are allocated to on chip caches in order to reduce the overall latency caused by off chip memory accesses.

In contrast GPUs devote more transistors to arithmetic units than to memory cache thereby achieving much higher arithmetic throughput than CPUs. However this GPU architecture results in higher latency for off chip memory access compared to CPUs.

In one aspect the present disclosure provides a graphical processing unit having an implementation of a hierarchical hash table thereon. In one embodiment the graphics processing unit includes 1 a plurality of parallel processors wherein each of the plurality of parallel processors includes parallel processing cores a shared memory coupled to each of the parallel processing cores and registers wherein each one of the registers is uniquely associated with one of the parallel processing cores and 2 a controller configured to employ at least one of the registers to establish a hierarchical hash table for a key value pair of a thread processing on one of the parallel processing cores.

In another aspect the present disclosure provides a method of establishing a hierarchical hash table in a graphics processing unit having parallel processors. In one embodiment the method includes 1 receiving a key and a value of a key value pair of a thread processing on a parallel processor and selecting a register of a parallel processing core of the parallel processor to insert the value.

In yet another aspect the disclosure provides a GPU computing system including 1 a central processing unit including a processor and associated with a memory and 2 a graphics processing unit having a hierarchical hash table the graphics processing unit including 2A a plurality of parallel processors wherein each of the plurality of parallel processors includes parallel processing cores a shared memory coupled to each of the parallel processing cores and registers wherein each one of the registers is uniquely associated with one of the parallel processing cores and 2B a controller configured to employ the registers to store values of key value pairs associated with threads executing on the parallel processing cores.

A GPU includes a plurality of multiprocessors referred to herein as parallel processors. To increase arithmetic density each GPU parallel processor has a single instruction unit for multiple arithmetic units. The threads running on a parallel processor are partitioned into groups in which all threads execute the same instruction simultaneously. This execution model is referred to as Single Instruction Multiple Threads SIMT . For example the CUDA architecture from Nvidia Corporation of Santa Clara Calif. has groups that are called warps wherein each warp has 32 threads.

Each thread needs to maintain a small hash table with implicitly synchronized threads independently inserting and querying entries. A difficulty however is that with a large number of threads the amount of near on chip memory is severely limited resulting in difficulty achieving high performance on hierarchical memory architectures like on GPUs.

This disclosure provides a scheme to implement parallel hash tables shared by implicitly synchronized threads. The parallel hash tables referred to herein as hierarchical hash tables employ registers and shared memory of parallel processors to store key value pairs. Hardware instructions can be used to insert values in registers of different threads and to indicate when the registers are full. Being able to employ register files for hash tables advantageously reduces latency associated with memory accesses since the registers are the fastest memory pool i.e. closest memory pool with a lower latency compared to a shared memory and a global memory associated with a parallel processor. Additionally employing the registers takes advantage of the largest on chip memory pool of a parallel processor the registers.

In one embodiment the hierarchical hash tables are generated based on an algorithm that employs hierarchical memory to store keys and values of a hash table. The keys and values are designed to be stored in the three levels of memory associated with a parallel processor registers or another type of non dynamically addressable memory shared memory and global memory. In embodiments disclosed herein a protocol is implemented to emulate dynamic addressing of the registers. Though employing the registers has the disadvantage of requiring more steps for insertion of values it offers the advantage of processing larger hash tables on chip i.e. on the parallel processor.

In embodiments disclosed herein the hierarchical hash tables use shared memory to store keys registers to store values and global GPU memory to store remaining keys and values. A remaining key or value is a key or value that cannot be stored in a register or a shared memory.

Before describing various embodiments of the novel method and mechanism a GPU computing system within which the mechanism may be embodied or the method carried out will be described.

As shown the system data bus connects the CPU the input devices the system memory and the graphics processing subsystem . In alternate embodiments the system memory may connect directly to the CPU . The CPU receives user input from the input devices executes programming instructions stored in the system memory operates on data stored in the system memory sends instructions and or data i.e. work or tasks to complete to a graphics processing unit to complete and configures needed portions of the graphics processing system for the GPU to complete the work. The system memory typically includes dynamic random access memory DRAM used to store programming instructions and data for processing by the CPU and the graphics processing subsystem . The GPU receives the transmitted work from the CPU and processes the work. In one embodiment the GPU completes the work in order to render and display graphics images on the display devices . In other embodiments the graphics processing subsystem can be used for non graphics processing. For example the GPU can be used to implement the logic of the function library to address various problems. Parallel processors to of the GPU are employed for processing the work. As used herein n is an integer value. With respect to the GPU n indicates the number of parallel processors. As used throughout the disclosure n can vary depending on the particular type of GPU.

As also shown the system memory includes an application program an application programming interface API and a graphics processing unit GPU driver . The application program generates calls to the API in order to produce a desired set of results such as a sequence of graphics images.

The graphics processing subsystem includes the GPU an on chip GPU memory an on chip GPU data bus a GPU local memory and a GPU data bus . The GPU is configured to communicate with the on chip GPU memory via the on chip GPU data bus and with the GPU local memory via the GPU data bus . As noted above the GPU can receive instructions from the CPU process the instructions in order to render graphics data and images or perform non graphics operations and store these images or resulting data in the GPU local memory . Subsequently the GPU may display certain graphic images or data stored in the GPU local memory on the display devices .

The GPU includes a GPU controller the parallel processors to and a global memory . The controller is configured to assist in operating the GPU . The controller includes the necessary logic via hardware software or a combination thereof to direct operations of the parallel processors to . Each of the parallel processors to also includes its own control units registers execution pipelines and caches. In some embodiments the parallel processors to are configured as a graphics pipeline or as a portion of a graphics pipeline.

Each of parallel processors to are coupled to the global memory . The GPU may be provided with any amount of on chip GPU memory and GPU local memory including none and may use on chip GPU memory GPU local memory and system memory in any combination for memory operations. The CPU can allocate portions of these memories for the GPU to execute work.

Additionally each one of the parallel processors to include a shared memory and registers that are not shown in . The shared memory the registers and the global memory provide a hierarchical memory structure for each of the processing cores to . Accessing the registers and the shared memory reduces latency compared to accessing the other above noted memories of the graphics processing system . provides more detail of a single parallel processor having a shared memory and registers wherein the hierarchical hash tables disclosed herein can be advantageously implemented.

The GPU is used to implement a hierarchical hash table employing the hierarchical memory structure associated with parallel processors to . The GPU is configured to employ the registers of the parallel processors to to store values of key value pairs associated with threads executing on parallel processing cores of the parallel processors to . Additionally the GPU is configured to employ the shared memory to store keys of the key value pairs. As such the GPU employs the registers and shared memory of the parallel processors to to establish a hierarchical hash table that associates keys to values for executing threads. The GPU is also configured to employ the global memory for the hierarchical hash table when the registers and the shared memory are full. By employing the registers and shared memory for a hash table memory accesses to the global memory are minimized and latency is reduced.

The on chip GPU memory is configured to include GPU programming code and on chip buffers . The GPU programming may be transmitted from the GPU driver to the on chip GPU memory via the system data bus .

The GPU local memory typically includes less expensive off chip dynamic random access memory DRAM and is also used to store data and programming used by the GPU . As shown the GPU local memory includes a frame buffer . The global memory can be a programming abstraction. In one embodiment the global memory can be physically located on the on chip memory . In other embodiments the global memory is physically located on the local memory . In yet other embodiments the global memory can be physically located on both the on chip memory and the local memory .

The display devices are one or more output devices capable of emitting a visual image corresponding to an input data signal. For example a display device may be built using a cathode ray tube CRT monitor a liquid crystal display or any other suitable display system. The input data signals to the display devices are typically generated by scanning out the contents of one or more frames of image data that is stored in the frame buffer .

The functions library is a database that stores a series of operating instructions that represent an algorithm for performing a particular function. In one embodiment the functions library can be an Algebraic Multigrid AMG Solver. As such the hierarchical hash tables disclosed herein can be used within the AMG solver for Sparse Matrix Matrix Product SpMM or more specifically to compute the product of sparse matrices in Compressed Sparse Row CSR format.

While SpMM multiplication is an important component of AMG methods other important operations can be reduced to an operation similar in structure to SpMM where the most efficient implementation uses a per warp on chip hash table to compute segmented reductions without sorting . Examples of SpMM like operations that benefit from the disclosed hierarchical hash table include computing the 2 ring of each vertex in a graph computing the sparsity pattern for ILU1 factorization and computing the graph structure resulting from vertex aggregation. Additionally a 2 ring interpolation method which uses the same framework as the SpMM multiplication can employ the hierarchical hash table. The ILU1 can be used as part of solvers. The hierarchical hash tables therefore can be used to accelerate a range of operations that are important for a variety of graph and sparse linear algebra applications.

Having described a GPU computing system including a GPU with parallel processors in which hierarchical hash tables and method for generating hierarchical hash tables may be embodied or carried out embodiments of a parallel processor and a method will be described.

The parallel processor includes processing cores to register to a shared memory and connecting circuitry . Additionally one skilled in the art will understand that the parallel processor can include additional components that are not illustrated but are typically included in a parallel processor such as thread schedulers and special function units.

The processing cores are configured to perform parallel processing of a single instruction i.e. SIMT processing. Each one of the registers to is specifically associated with a particular one of the processing cores to . As such each of the registers to provide a quick access for threads executing on the processing cores to . In the illustrated embodiment the registers to are data registers. Typically the registers to are organized into a register file.

The shared memory is coupled to each one of the processing cores and provides additional storage space therefor. The shared memory is user managed and provides higher latency when accessing compared to the registers to

For the hierarchical hash table the registers and the shared memory are not used as a cache to fasten key value lookups but rather as a buffer to store elements during the life of a group of threads e.g. a warp .

The connecting circuitry couples the processing cores to to the shared memory . The connecting circuitry and couple the registers and to their respective processing cores and . One skilled in the art will understand that additional connecting circuitry would be used to connect corresponding registers and processing cores to each other. Each of the connecting circuitry and are conventional connectors typically employed in graphic processing units.

Threads executing on parallel processing cores such as processing cores and of have keys and values to insert. Threads insert their keys and values in parallel. For simplicity of explanation the method assumes that no two threads share the same key and is directed to inserting keys and values for a single thread executing on a processing core of a parallel processor. The generalization to cases where two threads share the same key can easily be done by someone skilled in the art. One skilled in the art will also understand that the method can be executed in parallel to insert keys and values of threads executing in parallel on parallel processing cores of a parallel processor.

The method proceeds in two main steps. First the method attempts to insert keys in shared memory e.g. shared memory of and values in registers e.g. registers to of . If the insertion fails i.e. some executing threads fail to insert their respective pair the method attempts to insert the key or value in a global memory e.g. global memory of . Different hashing functions are used to insert keys and values in the hierarchical hash table. However in contrast to creating conventional hash tables the method is more complex because some values are not stored in dynamically addressable memory. Instead some values are stored in or at least attempted to be stored in registers associated with executing threads. This satisfies a need to achieve a good occupancy and deliver high performance since hash tables stored solely in a shared memory consume a significant portion of that memory. When keys are inserted in shared memory and values in registers less shared memory is used and a higher occupancy is achieved. The method begins in a step .

In a step keys and values of key value pairs of threads executing on a parallel processor are ready to be stored. The key value pairs are produced by the executing threads.

In a step the threads select a slot in shared memory for inserting their keys. If several threads compete for the same slot no more than one thread can be satisfied.

In a step threads which have found slots in shared memory for their keys apply for registers to store their values. A thread may apply for a register which does not belong to the core which is executing that thread.

In a step the values are inserted in the selected registers or the existing values in those registers are updated. One skilled in the art may decide to apply a different policy with respect to inserting or updating the values in registers.

In a step a global memory buffer associated with the parallel processor is used to store the keys and the values that cannot be stored in the shared memory and register. In some applications global memory is not needed to store keys and values. The method ends in a step .

In one embodiment the method can employ different hashing functions to insert keys and values in the hash table. The hash functions can be conventional hash functions wherein the parameters are varied for each function. The following represents an algorithm that can be employed to insert keys and values in a hash table.

In one embodiment the function to insert keys and values in shared memory and registers can be implemented by the following algorithm.

The above code fragment hash idx . . . takes the key applies the hash function and returns an integer memory location. If the hash function generates an integer memory location that is greater than the number of buffer slots then a modulus of the integer memory location can be taken to fit within the buffer slots. In step if m smem . . . it is determined if the slot in shared memory has a key already stored. If so the value in the corresponding register has to be updated. If the key is not in the shared memory and the slot is empty then an attempt to write the key in that slot in shared memory is made. In one embodiment the emptiness of a slot can be represented with a sentinel value for e.g. 1 if no valid key is negative.

When a slot is empty several threads may attempt to write their keys at that slot. Only one wins. Each thread determines if it is the winner. If so then the corresponding register is updated with the value or the value is inserted in the register. If a loser then the same operation is rerun with a different hash function wherein a limited number of hash functions exist.

After a number of hash functions have been used all of the registers are loaded with the values. A hardware command can be used to indicate that the threads are filled. In one embodiment the hardware command VOTE can be used to determine if the registers are full. In VOTE each of the threads indicates via a binary vote if their registers are full. Since there is a limited amount of memory provided by the registers and the shared memory global memory is used to store remaining values and keys.

To be able to address registers as if they were shared memory the method employs a protocol which consists of two steps. That protocol allocates a buffer in the shared memory where candidate threads can be registered. In a first step writer threads which have a value to store will write their identifier in the shared registration buffer. In a second step threads owning the registers that are to be modified will collect values from the writers and update the registers.

In that protocol the registration buffer contains a slot per register to be addressed. In one embodiment a slot will typically use less memory space than the value stored in register. The protocol determines if a slot in the shared buffer contains the identifier of a thread. If so the thread owning the register associated with the slot can collect the value from that other thread and store that value in its corresponding register.

In one embodiment the following code can be used to implement the first step of the protocol. Writer threads which have a value to insert can register for insertion.

When all writer threads are registered threads turn themselves into storage threads to perform the second step.

If a slot of the shared memory contains a valid thread identifier the identifier of a writer thread the storage thread collects the value from the writer and updates its storage. In one embodiment the following code can be used to collect values and update the registers.

In that code fragment my slots r contains the identifier of the candidate thread which wants to write in register r if any. If there is no candidate a sentinel identifier can be used e.g. warpSize.

A hardware instruction such as SHFL can be used to distribute the candidate values to different threads. SHFL is a hardware command that allows threads to read registers from other threads.

The above apparatuses and schemes provide a method of operating a graphics processing unit that includes receiving an operating instruction partitioning the operating instruction into multiple threads for processing by parallel processors wherein each of the multiple threads includes a key value pair and employing registers and shared memory of the parallel processors to establish hash tables for the multiple operating threads.

A portion of the above described apparatuses systems or methods may be embodied in or performed by various processors including parallel processors or computers wherein the computers are programmed or store executable programs of sequences of software instructions to perform one or more of the steps of the methods. The software instructions of such programs may represent algorithms and be encoded in machine executable form on non transitory digital data storage media e.g. magnetic or optical disks random access memory RAM magnetic hard disks flash memories and or read only memory ROM to enable various types of digital data processors or computers to perform one multiple or all of the steps of one or more of the above described methods or functions of the apparatuses described herein.

Portions of disclosed embodiments may relate to computer storage products with a non transitory computer readable medium that have program code thereon for performing various computer implemented operations that embody a part of an apparatus system or carry out the steps of a method set forth herein. Non transitory used herein refers to all computer readable media except for transitory propagating signals. Examples of non transitory computer readable media include but are not limited to magnetic media such as hard disks floppy disks and magnetic tape optical media such as CD ROM disks magneto optical media such as floptical disks and hardware devices that are specially configured to store and execute program code such as ROM and RAM devices. Examples of program code include both machine code such as produced by a compiler and files containing higher level code that may be executed by the computer using an interpreter.

Those skilled in the art to which this application relates will appreciate that other and further additions deletions substitutions and modifications may be made to the described embodiments.

